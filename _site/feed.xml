<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="https://arthurpesah.me/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arthurpesah.me/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-01-31T16:01:17+01:00</updated><id>https://arthurpesah.me/feed.xml</id><title type="html">Arthur Pesah</title><subtitle>Research blog
</subtitle><author><name>Arthur Pesah</name><email>arthur.pesah@gmail.com</email></author><entry><title type="html">Quantum error correction with the stabilizer formalism — Part I</title><link href="https://arthurpesah.me/blog/2023-01-31-stabilizer-formalism-1/" rel="alternate" type="text/html" title="Quantum error correction with the stabilizer formalism — Part I" /><published>2023-01-31T00:00:00+01:00</published><updated>2023-01-31T00:00:00+01:00</updated><id>https://arthurpesah.me/blog/stabilizer-formalism-1</id><content type="html" xml:base="https://arthurpesah.me/blog/2023-01-31-stabilizer-formalism-1/">&lt;p&gt;Now that you know &lt;a href=&quot;/blog/2022-05-21-classical-error-correction/&quot;&gt;all you need to know about classical error correction&lt;/a&gt;, the time has finally come to learn how to correct those damn errors that keep sabotaging your quantum computer! The key tool, introduced by Daniel Gottesman in &lt;a href=&quot;https://thesis.library.caltech.edu/2900/2/THESIS.pdf&quot;&gt;his landmark 1997 PhD thesis&lt;/a&gt; is the stabilizer formalism.
The same way most classical codes fall into the linear code category, almost all the quantum codes you will encounter can be classified as stabilizer codes. And for a good reason: stabilizer codes are simply the quantum generalization of linear codes!
Your beloved parity checks will turn into stabilizers, a set of commuting measurements controlling the parity of your qubits in different bases.
Parity-check matrices and Tanner graphs will get slightly bigger and more constrained. But apart from that, if you’re more or less comfortable with the notions discussed in the last post, going quantum shouldn’t give you too much trouble.&lt;/p&gt;

&lt;p&gt;So, what’s the plan? To make the content of this post a bit more digestible, I’ve decided to divide it into two parts.
In the first part, we will start by motivating the need for the stabilizer formalism, using the quantum repetition code and Shor’s code as examples. We will then be ready to formally define stabilizer codes and one of its most important families, the CSS codes.
To illustrate our construction, we will end the post by introducing a simple code that really exemplifies the stabilizer and CSS construction: the Steane code. Finally, in case you need it, I’ve put some reminders on the manipulation of Pauli operators in the appendix.&lt;/p&gt;

&lt;p&gt;In the second part of this series, we will look more deeply into the properties of stabilizer codes. In particular, we will introduce the notion of logical operator, see how to generalize parity-check matrices and Tanner graphs to the quantum setting, and study some properties of stabilizers states and the Clifford group.&lt;/p&gt;

&lt;p&gt;This two-part series should give you all you need to start exploring the quantum error correction literature. And with all those tools in our hand, we will finally be ready to study the most popular never beaten quantum code: the surface code! So hang in there, have a good read, and I assure you the journey will be worth it!&lt;/p&gt;

&lt;h2 id=&quot;motivation-a-new-lens-on-the-quantum-repetition-code&quot;&gt;Motivation: a new lens on the quantum repetition code&lt;/h2&gt;

&lt;p&gt;To understand the challenges in building quantum codes, let’s look back at our good ol’ quantum repetition code (introduced in the &lt;a href=&quot;/blog/2022-01-25-intro-qec-1/&quot;&gt;first post&lt;/a&gt; of this series), with our new error correction knowledge.  As a reminder, the quantum repetition code is defined as the encoding of a single-qubit logical state &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \psi \rangle_L = a \vert 0 \rangle_L + b \vert 1 \rangle_L&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \psi \rangle_L = a \vert 0 \rangle_L + b \vert 1 \rangle_L&lt;/script&gt; as a 3-qubit physical state:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
\vert \psi \rangle = a \vert 000 \rangle + b \vert 111 \rangle
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\vert \psi \rangle = a \vert 000 \rangle + b \vert 111 \rangle
\end{aligned}&lt;/script&gt;

&lt;p&gt;We define the &lt;strong&gt;codespace&lt;/strong&gt; of our code as the space of all the states that can be written as above (for any &lt;code class=&quot;MathJax_Preview&quot;&gt;a,b&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a,b&lt;/script&gt;).
Let’s see how errors affect the codespace.
As we saw in the first post, quantum noise comes into two flavours: &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; errors, also known as bit flips, and &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors, also known as phase flips.
Let’s focus on &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; errors first.
If an &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; error occurs on the first qubit, the state is transformed into&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
\vert \widetilde{\psi} \rangle = X_1 \vert \psi \rangle = a \vert 100 \rangle + b \vert 011 \rangle
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\vert \widetilde{\psi} \rangle = X_1 \vert \psi \rangle = a \vert 100 \rangle + b \vert 011 \rangle
\end{aligned}&lt;/script&gt;

&lt;p&gt;In the classical repetition code, errors such as this one could be detected by reading the message and seeing that it is neither &lt;code class=&quot;MathJax_Preview&quot;&gt;000&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;000&lt;/script&gt; or &lt;code class=&quot;MathJax_Preview&quot;&gt;111&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;111&lt;/script&gt;, or in other words, it is not a codeword.
Decoding would then consist of taking a majority vote on the three bits.&lt;/p&gt;

&lt;p&gt;However, in the quantum case, “reading the message” would collapse the state and ruin any further computation we might want to do on this state.
To make error detection work, let’s remember that there is another technique to detect errors on linear codes (including the repetition code), that we saw in the last post: we can look at the parity checks of the code! For the classical repetition codes, those parity checks measured the parity of all the pairs of bits. If some of the checks had an odd parity, it meant that an error had occurred, and depending on which pairs had a violated parity-check equation, we could then decode any single bit-flip error.&lt;/p&gt;

&lt;p&gt;And that’s where the magic comes in: those parity checks can be measured quantumly without collapsing the state!
First, you might wonder what “parity” means for our state, given that we have a superposition of two computational basis elements in the codespace.
However, it’s easy to verify that it is actually well-defined. Consider the state &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \psi \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \psi \rangle&lt;/script&gt; subjected to any number of bit-flip errors. Choose one of the two elements of the superposition. Look at the parity of a pair of qubits. This parity will be the same if you had chosen the other element of the superposition. This is due to the fact that bit flips are always applied simultaneously on both parts of the superposition.&lt;/p&gt;

&lt;p&gt;Now, how do we measure this parity? We can simply measure the observable &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_i Z_j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_i Z_j&lt;/script&gt;.
Indeed, &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \psi \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \psi \rangle&lt;/script&gt; is an eigenstate of &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_i Z_j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_i Z_j&lt;/script&gt;, and remains so when bit-flip errors are applied to the state.
For instance, &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_1 Z_2 \vert \psi \rangle = \vert \psi \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_1 Z_2 \vert \psi \rangle = \vert \psi \rangle&lt;/script&gt;, meaning that measuring &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_1 Z_2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_1 Z_2&lt;/script&gt; on the error-free state will give &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt;.
On the other hand, &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_1 Z_2 \vert \widetilde{\psi} \rangle = Z_1 Z_2 X_1 \vert \psi \rangle = - X_1 Z_1 Z_2 \vert \psi \rangle = -\vert \widetilde{\psi} \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_1 Z_2 \vert \widetilde{\psi} \rangle = Z_1 Z_2 X_1 \vert \psi \rangle = - X_1 Z_1 Z_2 \vert \psi \rangle = -\vert \widetilde{\psi} \rangle&lt;/script&gt;, so measuring this operator when a bit-flip has occurred on the first qubit gives &lt;code class=&quot;MathJax_Preview&quot;&gt;-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt; (if this manipulation of Pauli operators is not straightforward to you, feel free to read the &lt;a href=&quot;2023-01-21-stabilizer-formalism-1/#appendix-handling-pauli-operators-with-ease&quot;&gt;Appendix&lt;/a&gt; on Pauli operators and come back).
Checking those calculations on your own, and for other examples of bit-flip errors, should convince you that measuring &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_i Z_j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_i Z_j&lt;/script&gt; gives you exactly the parity between the qubits &lt;code class=&quot;MathJax_Preview&quot;&gt;i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;. In general, the result of measuring those operators is exactly like the syndrome we introduced in the previous post: we get &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; when a parity-check equation is satisfied, and &lt;code class=&quot;MathJax_Preview&quot;&gt;-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt; when it is not. As an example of how to measure those parity checks, the circuit to measure &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_1 Z_2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_1 Z_2&lt;/script&gt; is given below:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/stabilizer-formalism-1/repetition-code-circuit.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, what have we done so far? We have found a set of operators &lt;code class=&quot;MathJax_Preview&quot;&gt;\{Z_i Z_j\}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\{Z_i Z_j\}&lt;/script&gt; such that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Our error-free state is a simultaneous &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; eigenstate of all those operators&lt;/li&gt;
  &lt;li&gt;Single and two-qubit &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; errors anti-commute with some of them, allowing the detection and correction of &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; errors.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will soon call those operators “stabilizers”, and study their general properties.
But first—you might have been wondering all this time—what about &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors?
As it happens, they are actually undetectable by our code.
For instance, if a &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; error were to occur on the first qubit of the state &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \psi \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \psi \rangle&lt;/script&gt;, we would get the state&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
\vert \widetilde{\psi} \rangle = Z_1 \vert \psi \rangle = a \vert 000 \rangle - b \vert 111 \rangle
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\vert \widetilde{\psi} \rangle = Z_1 \vert \psi \rangle = a \vert 000 \rangle - b \vert 111 \rangle
\end{aligned}&lt;/script&gt;

&lt;p&gt;This is still in the codespace of our code! Therefore, we have no way to detect this error:
&lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors are &lt;strong&gt;logical errors&lt;/strong&gt;.
We define the &lt;strong&gt;distance&lt;/strong&gt; of a code as the smallest Pauli error that maps the codespace to itself,
or in other words, the smallest logical error. Since a single &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; error in the quantum repetition code
is undetectable, it means that the distance of the code is &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;.
Denoting &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; the number of physical qubits of a code, &lt;code class=&quot;MathJax_Preview&quot;&gt;k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; the number of logical qubits it encodes,
and &lt;code class=&quot;MathJax_Preview&quot;&gt;d&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; its distance, we say that the repetition code is an &lt;code class=&quot;MathJax_Preview&quot;&gt;[[n,k,d]]=[[n,1,1]]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[[n,k,d]]=[[n,1,1]]&lt;/script&gt;-code.
Note the use of double brackets, a common convention used to distinguish quantum from classical codes.&lt;/p&gt;

&lt;p&gt;So what do we do from there? Taking inspiration from the quantum repetition code, let’s try to build
our first code that can detect both &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors: Shor’s code.&lt;/p&gt;

&lt;h2 id=&quot;our-first-truly-quantum-code-shors-code&quot;&gt;Our first truly quantum code: Shor’s code&lt;/h2&gt;

&lt;p&gt;To understand the idea behind Shor’s code, let’s first see how we could design a repetition code that only protects information against &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors, instead of &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; errors. Can you see what modification of the repetition code would be required?&lt;/p&gt;

&lt;p&gt;The idea is to simply use a different basis for the codewords. Indeed, replacing &lt;code class=&quot;MathJax_Preview&quot;&gt;\{\vert 000\rangle, \vert 111\rangle\}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\{\vert 000\rangle, \vert 111\rangle\}&lt;/script&gt; by &lt;code class=&quot;MathJax_Preview&quot;&gt;\{\vert +++\rangle, \vert ---\rangle\}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\{\vert +++\rangle, \vert ---\rangle\}&lt;/script&gt;, and the parity-check measurements &lt;code class=&quot;MathJax_Preview&quot;&gt;\{Z_i Z_j\}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\{Z_i Z_j\}&lt;/script&gt; by &lt;code class=&quot;MathJax_Preview&quot;&gt;\{X_i X_j\}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\{X_i X_j\}&lt;/script&gt;, we obtain a code that can correct any single-qubit &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; error.&lt;/p&gt;

&lt;p&gt;The trick found by Peter Shor is to combine those two codes using a process called &lt;strong&gt;concatenation&lt;/strong&gt;. It consists of encoding the logical qubits of one code using a second code. In our case, we can encode the logical qubits of the &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt;-basis repetition code into the &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;-basis repetition code. This defines the 9-qubit Shor’s code, made of the following codewords&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
\vert 0 \rangle_{L_2} = \vert + \rangle_{L_1} \otimes \vert + \rangle_{L_1} \otimes \vert + \rangle_{L_1} = \frac{1}{2^{3/2}} \left(\vert 000 \rangle + \vert 111 \rangle \right) \left(\vert 000 \rangle + \vert 111 \rangle \right) \left(\vert 000 \rangle + \vert 111 \rangle \right) \\
\vert 1 \rangle_{L_2} = \vert - \rangle_{L_1} \otimes \vert - \rangle_{L_1} \otimes \vert - \rangle_{L_1} = \frac{1}{2^{3/2}} \left(\vert 000 \rangle - \vert 111 \rangle \right) \left(\vert 000 \rangle - \vert 111 \rangle \right) \left(\vert 000 \rangle - \vert 111 \rangle \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\vert 0 \rangle_{L_2} = \vert + \rangle_{L_1} \otimes \vert + \rangle_{L_1} \otimes \vert + \rangle_{L_1} = \frac{1}{2^{3/2}} \left(\vert 000 \rangle + \vert 111 \rangle \right) \left(\vert 000 \rangle + \vert 111 \rangle \right) \left(\vert 000 \rangle + \vert 111 \rangle \right) \\
\vert 1 \rangle_{L_2} = \vert - \rangle_{L_1} \otimes \vert - \rangle_{L_1} \otimes \vert - \rangle_{L_1} = \frac{1}{2^{3/2}} \left(\vert 000 \rangle - \vert 111 \rangle \right) \left(\vert 000 \rangle - \vert 111 \rangle \right) \left(\vert 000 \rangle - \vert 111 \rangle \right)
\end{aligned}&lt;/script&gt;

&lt;p&gt;where &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \cdot \rangle_{L_1}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \cdot \rangle_{L_1}&lt;/script&gt; refers to logical qubits after the first encoding, for which &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 0 \rangle_{L_1}=\vert 000 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 0 \rangle_{L_1}=\vert 000 \rangle&lt;/script&gt;, and &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \cdot \rangle_{L_2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \cdot \rangle_{L_2}&lt;/script&gt; refers to logical qubits after the second encoding, for which &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 0 \rangle_{L_2}=\vert +++ \rangle_{L_1}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 0 \rangle_{L_2}=\vert +++ \rangle_{L_1}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now, if an &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; error occurs on any of the 9 qubits, we will be able to correct it by measuring &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_i Z_{j}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_i Z_{j}&lt;/script&gt; on all the qubits &lt;code class=&quot;MathJax_Preview&quot;&gt;i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; of the same block. Indeed, you can check that in the absence of error, we have&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
Z_i Z_j \vert \psi \rangle_{L_2} = \vert \psi \rangle_{L_2}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
Z_i Z_j \vert \psi \rangle_{L_2} = \vert \psi \rangle_{L_2}
\end{aligned}&lt;/script&gt;

&lt;p&gt;with &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \psi \rangle_{L_2} = a \vert 0 \rangle_{L_2} + b \vert 1 \rangle_{L_2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \psi \rangle_{L_2} = a \vert 0 \rangle_{L_2} + b \vert 1 \rangle_{L_2}&lt;/script&gt;.
This means that measuring those operators will give the result &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;On the other hand, let’s say we have an &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; error on the fourth qubit of the logical zero state. It would result in the state:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
X_4 \vert 0 \rangle_{L_2} = \frac{1}{2^{3/2}} \left(\vert 000 \rangle + \vert 111 \rangle \right) \left(\vert 100 \rangle + \vert 011 \rangle \right) \left(\vert 000 \rangle + \vert 111 \rangle \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
X_4 \vert 0 \rangle_{L_2} = \frac{1}{2^{3/2}} \left(\vert 000 \rangle + \vert 111 \rangle \right) \left(\vert 100 \rangle + \vert 011 \rangle \right) \left(\vert 000 \rangle + \vert 111 \rangle \right)
\end{aligned}&lt;/script&gt;

&lt;p&gt;The odd parity between qubit 4 and qubits 5 and 6, detected by measuring &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_4 Z_5&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_4 Z_5&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_5 Z_6&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_5 Z_6&lt;/script&gt;, tells us that the &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; error occurred on qubit &lt;code class=&quot;MathJax_Preview&quot;&gt;4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;4&lt;/script&gt;. We can therefore correct this error, and more generally any other single-qubit &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; error, by analyzing the result of the &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_i Z_j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_i Z_j&lt;/script&gt; measurements.&lt;/p&gt;

&lt;p&gt;Let’s now focus our attention to &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors. Let’s remember that for the Z-basis repetition code, the operator &lt;code class=&quot;MathJax_Preview&quot;&gt;\overline{X}=XXX&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\overline{X}=XXX&lt;/script&gt; is a logical Pauli &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; operator, turning &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 0 \rangle_{L_1}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 0 \rangle_{L_1}&lt;/script&gt; into &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 1 \rangle_{L_1}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 1 \rangle_{L_1}&lt;/script&gt;. Let’s write &lt;code class=&quot;MathJax_Preview&quot;&gt;\overline{X_1}=X_1 X_2 X_3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\overline{X_1}=X_1 X_2 X_3&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;\overline{X_2}=X_4 X_5 X_6&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\overline{X_2}=X_4 X_5 X_6&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\overline{X_3}=X_7 X_8 X_9&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\overline{X_3}=X_7 X_8 X_9&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;To detect &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors, we can measure the parity &lt;code class=&quot;MathJax_Preview&quot;&gt;\overline{X}_i \overline{X}_{j}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\overline{X}_i \overline{X}_{j}&lt;/script&gt; for all &lt;code class=&quot;MathJax_Preview&quot;&gt;i,j \in \{1,2,3\}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i,j \in \{1,2,3\}&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;i \neq j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i \neq j&lt;/script&gt;. Indeed, you can verify explicitly that we have&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
\overline{X_i} \overline{X_j} \vert \psi \rangle_{L_2} = \vert \psi \rangle_{L_2}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\overline{X_i} \overline{X_j} \vert \psi \rangle_{L_2} = \vert \psi \rangle_{L_2}
\end{aligned}&lt;/script&gt;

&lt;p&gt;for all &lt;code class=&quot;MathJax_Preview&quot;&gt;i,j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i,j&lt;/script&gt;. Moreover, if a &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; error occurs, for example on the fourth qubit of the logical zero state, we would get:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
Z_4 \vert 0 \rangle_{L_2} = \vert +-+ \rangle_{L_1} = \frac{1}{2^{3/2}} \left(\vert 000 \rangle + \vert 111 \rangle \right) \left(\vert 000 \rangle - \vert 111 \rangle \right) \left(\vert 000 \rangle + \vert 111 \rangle \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
Z_4 \vert 0 \rangle_{L_2} = \vert +-+ \rangle_{L_1} = \frac{1}{2^{3/2}} \left(\vert 000 \rangle + \vert 111 \rangle \right) \left(\vert 000 \rangle - \vert 111 \rangle \right) \left(\vert 000 \rangle + \vert 111 \rangle \right)
\end{aligned}&lt;/script&gt;

&lt;p&gt;The odd parity between blocks &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2&lt;/script&gt; and blocks &lt;code class=&quot;MathJax_Preview&quot;&gt;2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt; can be detected using the operator &lt;code class=&quot;MathJax_Preview&quot;&gt;\overline{X_1} \overline{X_2} = X_1 X_2 X_3 X_4 X_5 X_6&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\overline{X_1} \overline{X_2} = X_1 X_2 X_3 X_4 X_5 X_6&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\overline{X_2} \overline{X_3} = X_4 X_5 X_6 X_7 X_8 X_9&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\overline{X_2} \overline{X_3} = X_4 X_5 X_6 X_7 X_8 X_9&lt;/script&gt;. You can check that explicitly by applying those operators to &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_4 \vert 0 \rangle_{L_2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_4 \vert 0 \rangle_{L_2}&lt;/script&gt; and showing for instance that&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
\overline{X_1} \overline{X_2} Z_4 \vert 0 \rangle_{L_2} = - Z_4 \vert 0 \rangle_{L_2}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\overline{X_1} \overline{X_2} Z_4 \vert 0 \rangle_{L_2} = - Z_4 \vert 0 \rangle_{L_2}
\end{aligned}&lt;/script&gt;

&lt;p&gt;meaning that the result of measuring &lt;code class=&quot;MathJax_Preview&quot;&gt;\overline{X_1} \overline{X_2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\overline{X_1} \overline{X_2}&lt;/script&gt; will be &lt;code class=&quot;MathJax_Preview&quot;&gt;-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt;. Similarly, the result of measuring &lt;code class=&quot;MathJax_Preview&quot;&gt;\overline{X_2} \overline{X_3}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\overline{X_2} \overline{X_3}&lt;/script&gt; will be &lt;code class=&quot;MathJax_Preview&quot;&gt;-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt;. However, note that we would have obtained the exact same measurement result if the &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; error had occurred on qubit &lt;code class=&quot;MathJax_Preview&quot;&gt;5&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;5&lt;/script&gt; or qubit &lt;code class=&quot;MathJax_Preview&quot;&gt;6&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;6&lt;/script&gt;. That’s our first example of &lt;strong&gt;error degeneracy&lt;/strong&gt;: different errors causing the same syndrome. How do we decide where to apply our correction then? The answer is that it doesn’t matter: we can choose either of the three middle qubits. To see why, let’s look at what happens if we apply &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_3&lt;/script&gt; to a state affected by the error &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_4&lt;/script&gt;:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
Z_3 Z_4 \vert \psi \rangle_{L_2} = \vert \psi \rangle_{L_2}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
Z_3 Z_4 \vert \psi \rangle_{L_2} = \vert \psi \rangle_{L_2}
\end{aligned}&lt;/script&gt;

&lt;p&gt;It gets us back to the original state! The reason is that any state in the codespace is a +1 eigenstate of &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_3 Z_4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_3 Z_4&lt;/script&gt; (as we saw when looking at &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; error detection). The same phenomenon would have happened if we had applied &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_6&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_6&lt;/script&gt;, and therefore we can correct the &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_4&lt;/script&gt; error by applying a &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; operator on any of the three middle qubits. More generally, any single-qubit &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; error can be corrected by analyzing the result of the &lt;code class=&quot;MathJax_Preview&quot;&gt;\overline{X}_i \overline{X}_{j}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\overline{X}_i \overline{X}_{j}&lt;/script&gt; measurements. This trick for dealing with error degeneracies is preponderant in quantum error correction, and we will see the most general version of it when looking at decoding stabilizer codes (in the next post).&lt;/p&gt;

&lt;p&gt;So our code is able to detect and correct any single-qubit error. But what about errors of higher weight? Or in other words, what is the distance of Shor’s code (i.e. the smallest undetectable errors)? Since this is a perfect exercise to see if you’ve understood this code, I leave this question as an exercise!&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise&lt;/strong&gt;: show that Shor’s code has a distance of &lt;code class=&quot;MathJax_Preview&quot;&gt;d=3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d=3&lt;/script&gt;. &lt;br /&gt;
&lt;em&gt;(&lt;strong&gt;Hint&lt;/strong&gt;: find a weight-3 error, made of either &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; or &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; elements, that preserves the codespace)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In summary, we have found a &lt;code class=&quot;MathJax_Preview&quot;&gt;[[9,1,3]]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[[9,1,3]]&lt;/script&gt; quantum code that can detect both &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors by measuring a set of operators &lt;code class=&quot;MathJax_Preview&quot;&gt;\{ Z_i Z_j \}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\{ Z_i Z_j \}&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\{\overline{X_i} \overline{X_j} \}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\{\overline{X_i} \overline{X_j} \}&lt;/script&gt;. This means that error-free states (i.e. codewords) are a common &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; eigenstate of those operators, while states subjected to weight-1 and weight-2 errors are &lt;code class=&quot;MathJax_Preview&quot;&gt;-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt; eigenstates of some of those operators, allowing us to detect those errors. With this example in mind, we are now finally ready to delve into the stabilizer formalism!&lt;/p&gt;

&lt;h2 id=&quot;stabilizer-formalism-first-definitions&quot;&gt;Stabilizer formalism: first definitions&lt;/h2&gt;

&lt;p&gt;The stabilizer formalism allows us to generalize the ideas above in order to come up with new quantum codes and study their properties. The main idea is to change our perspective from states (or codewords) to operators, similarly to the way we defined classical linear codes using parity-check matrices. But generalizing parity-check operators in the quantum setting requires a bit of care. Let’s see how it works.&lt;/p&gt;

&lt;p&gt;First, let me outline the general idea of this section.
As we saw with Shor’s code, errors in a quantum code can be detected by measuring a certain set of operators, generalizing the parity checks of classical codes.
Those operators are called &lt;strong&gt;stabilizers&lt;/strong&gt; and have a certain number of properties: they have eigenvalues &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt;, they all commute (the order of measurement doesn’t matter), etc.
Moreover, any codeword is a common &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; eigenstate of all the stabilizers, i.e. measuring any stabilizer on an error-free state gives the value &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; and does not disturb the state.&lt;/p&gt;

&lt;p&gt;The goal is to go backward: given any set of stabilizers, does it define a code?
As found out by Daniel Gottesman, the answer is yes, and this simple fact has been foundational for quantum error correction, allowing us to find codes by searching for stabilizers with good properties.
Let us now introduce the formalism behind this brilliant idea.&lt;/p&gt;

&lt;p&gt;The n-qubit &lt;strong&gt;Pauli group&lt;/strong&gt; &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{P}_n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{P}_n&lt;/script&gt; is the set of all Pauli operators on &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; qubits, with the usual matrix multiplication as the group operation, that is:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
\mathcal{P}_n=\{ \omega P_1 \otimes \ldots \otimes P_n : P_i \in \{I,X,Y,Z \}, \omega \in \{1,-1,i,-i\}\}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\mathcal{P}_n=\{ \omega P_1 \otimes \ldots \otimes P_n : P_i \in \{I,X,Y,Z \}, \omega \in \{1,-1,i,-i\}\}
\end{aligned}&lt;/script&gt;

&lt;p&gt;Note that the phase factor is included in order for this set to be close under the group operation.&lt;/p&gt;

&lt;p&gt;We can now define a &lt;strong&gt;stabilizer group&lt;/strong&gt; as an abelian subgroup of the Pauli group that does not contain &lt;code class=&quot;MathJax_Preview&quot;&gt;-I&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-I&lt;/script&gt;.
Let’s slowly break that down.
First, the stabilizer group is a subgroup of the Pauli group, meaning that every element is a Pauli operator, and has in consequence eigenvalues &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt;. It is a group, meaning that the product of any two stabilizers is also a stabilizer.
This group is abelian, meaning that any two stabilizers commute, and can therefore be measured in any order.
Finally, we don’t want &lt;code class=&quot;MathJax_Preview&quot;&gt;-I&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-I&lt;/script&gt; to be included, or equivalently we don’t want any two operators &lt;code class=&quot;MathJax_Preview&quot;&gt;S&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;-S&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-S&lt;/script&gt; to be in the same stabilizer group, as it will make sense shortly.&lt;/p&gt;

&lt;p&gt;We can now define a &lt;strong&gt;stabilizer code&lt;/strong&gt; as the common &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; eigenspace of all the operators in a stabilizer group.
That is, given a stabilizer group &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{S}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}&lt;/script&gt;, we define the codespace of a stabilizer code as:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
\mathcal{C}=\{ \vert \psi \rangle : S\vert \psi \rangle = \vert \psi \rangle, \forall S \in \mathcal{S} \}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\mathcal{C}=\{ \vert \psi \rangle : S\vert \psi \rangle = \vert \psi \rangle, \forall S \in \mathcal{S} \}
\end{aligned}&lt;/script&gt;

&lt;p&gt;This set is well-defined, since by definition all the stabilizers commute, and have therefore some common eigenstates.
Moreover, each stabilizer has at least one &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; eigenvalue (remember that &lt;code class=&quot;MathJax_Preview&quot;&gt;-I&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-I&lt;/script&gt; is not included in the stabilizer group), so there is a common &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; eigenstate of all the stabilizers. Finally, you can notice that &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{C}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt; forms a vector space, and therefore defines a valid code.&lt;/p&gt;

&lt;p&gt;This one-to-one correspondence between codespaces defined as above and stabilizer groups is the foundation of quantum error correction: instead of the thinking of codes in the state picture (as a vector space of states), we can now think of them in the operator picture (as a stabilizer group). For instance, Shor’s code can either be defined in the state picture, as the codespace &lt;code class=&quot;MathJax_Preview&quot;&gt;\{a \vert 0 \rangle_{L_2} + b \vert 1 \rangle_{L_2} \}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\{a \vert 0 \rangle_{L_2} + b \vert 1 \rangle_{L_2} \}&lt;/script&gt; (with &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 0 \rangle_{L_2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 0 \rangle_{L_2}&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 1 \rangle_{L_2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 1 \rangle_{L_2}&lt;/script&gt; defined in the previous section), or in the operator picture, as the codespace stabilized by &lt;code class=&quot;MathJax_Preview&quot;&gt;\langle Z_i Z_j, \overline{X_i} \overline{X_j} \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\langle Z_i Z_j, \overline{X_i} \overline{X_j} \rangle&lt;/script&gt; with &lt;code class=&quot;MathJax_Preview&quot;&gt;i,j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i,j&lt;/script&gt; neighboring physical/logical qubits.&lt;/p&gt;

&lt;p&gt;Last but not least, how can we detect and correct errors with a stabilizer code? As we saw with the repetition code and Shor’s code, the idea is to simply measure all the stabilizers, resulting in what is called the &lt;strong&gt;syndrome&lt;/strong&gt;. If no error has occurred, the syndrome should consist of &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; for all the stabilizers. If a Pauli error &lt;code class=&quot;MathJax_Preview&quot;&gt;E&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt; has occurred, there are two possibilities for each stabilizer &lt;code class=&quot;MathJax_Preview&quot;&gt;S&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;: either it commutes or it anticommutes with it. If it commutes, we will measure &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt;:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
SE \vert \psi \rangle = ES \vert \psi \rangle = E \vert \psi \rangle
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
SE \vert \psi \rangle = ES \vert \psi \rangle = E \vert \psi \rangle
\end{aligned}&lt;/script&gt;

&lt;p&gt;If it anticommutes, we will measure &lt;code class=&quot;MathJax_Preview&quot;&gt;-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt;:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
SE \vert \psi \rangle = - ES \vert \psi \rangle = - E \vert \psi \rangle
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
SE \vert \psi \rangle = - ES \vert \psi \rangle = - E \vert \psi \rangle
\end{aligned}&lt;/script&gt;

&lt;p&gt;To detect &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; errors, we therefore need stabilizers with some &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; or &lt;code class=&quot;MathJax_Preview&quot;&gt;Y&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; operators, while to detect &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors, we need stabilizers with some &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; or &lt;code class=&quot;MathJax_Preview&quot;&gt;Y&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; operators.&lt;/p&gt;

&lt;p&gt;By analyzing the syndrome, it is often possible to correct errors as well. However, one specificity of quantum codes is that we often don’t need to find the exact qubits on which the errors have occurred. What we need is to find a correction operator &lt;code class=&quot;MathJax_Preview&quot;&gt;C&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; that restores our state, that is:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
C E \vert \psi \rangle = \vert \psi \rangle
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
C E \vert \psi \rangle = \vert \psi \rangle
\end{aligned}&lt;/script&gt;

&lt;p&gt;In other words, we want &lt;code class=&quot;MathJax_Preview&quot;&gt;C E&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C E&lt;/script&gt; to be a stabilizer. For instance, in Shor’s code, we saw that if the error &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_4&lt;/script&gt; occurs, we can correct it by using either &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_4&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_5&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_5&lt;/script&gt; or &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_6&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_6&lt;/script&gt; as our correction operator. This was due to &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_4 Z_4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_4 Z_4&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_4 Z_5&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_4 Z_5&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_4 Z_6&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_4 Z_6&lt;/script&gt; being stabilizers. Finding the best correction operator is the essence of the &lt;strong&gt;decoding problem&lt;/strong&gt;, which we will see in more details in the next post.&lt;/p&gt;

&lt;p&gt;So what have we done so far? We have shown that given a stabilizer group, that is a set of commuting Pauli operators that does not contain &lt;code class=&quot;MathJax_Preview&quot;&gt;-I&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-I&lt;/script&gt;, we can construct a quantum code by considering the common +1 eigenspace of all the stabilizers.
When errors occur in this code, moving the state outside of the codespace, they can be detected (and sometimes corrected) by measuring all the stabilizers and checking if some measurements are equal to &lt;code class=&quot;MathJax_Preview&quot;&gt;-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;However, we haven’t yet given any method to construct interesting stabilizer groups. The next section introduces one of the most important family of stabilizer codes, the CSS codes, which will help us to build a new example of quantum code: the quantum version of the Hamming code.&lt;/p&gt;

&lt;h2 id=&quot;quantum-codes-from-classical-codes-the-css-construction&quot;&gt;Quantum codes from classical codes: the CSS construction&lt;/h2&gt;

&lt;p&gt;So, how can we construct stabilizer codes? One method is to start from two classical codes: one that will take care of &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; errors and one that will take care of &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors. As we saw in the previous section, to correct &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; errors, we can use stabilizers made of &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; operators, and to correct &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors, we can use stabilizers made of &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; operators.&lt;/p&gt;

&lt;p&gt;The idea is therefore the following: let’s pick two classical codes that we will call &lt;code class=&quot;MathJax_Preview&quot;&gt;C_X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C_X&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;C_Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C_Z&lt;/script&gt;. For each parity check of &lt;code class=&quot;MathJax_Preview&quot;&gt;C_X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C_X&lt;/script&gt;, supported on bits &lt;code class=&quot;MathJax_Preview&quot;&gt;b_1,\ldots,b_k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;b_1,\ldots,b_k&lt;/script&gt;, add the stabilizer &lt;code class=&quot;MathJax_Preview&quot;&gt;X_{b_1} \ldots X_{b_k}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X_{b_1} \ldots X_{b_k}&lt;/script&gt; to the stabilizer group. Similarly, for each parity check of &lt;code class=&quot;MathJax_Preview&quot;&gt;C_Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C_Z&lt;/script&gt;, supported on bits &lt;code class=&quot;MathJax_Preview&quot;&gt;b'_1,\ldots,b'_k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;b'_1,\ldots,b'_k&lt;/script&gt;, add the stabilizer &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_{b'_1} \ldots Z_{b'_k}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_{b'_1} \ldots Z_{b'_k}&lt;/script&gt; to the stabilizer group.
For the resulting quantum code to be valid, remember that all the stabilizers should commute. While stabilizers of the same Pauli type necessarily commute, it is not obvious that all the &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; stabilizers commute with all the &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; stabilizers. For this to be the case, each &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; stabilizer should intersect on an even number of qubits with all the &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; stabilizers (see &lt;a href=&quot;2023-01-21-stabilizer-formalism-1/#appendix-handling-pauli-operators-with-ease&quot;&gt;Appendix&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;If this is the case, the resulting quantum code is valid and form what is called a Calderbank-Shor-Steane (CSS) code. More precisely, a &lt;strong&gt;CSS code&lt;/strong&gt; is a stabilizer code that can be generated by a set of pure &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and pure &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; stabilizers. For instance, both codes that we have encountered before, Shor’s code and the quantum repetition code, are examples of CSS codes. And so are most of the codes that you will encounter in the literature, making CSS codes one of the most important family of codes.&lt;/p&gt;

&lt;p&gt;However, if you try the construction above with some random codes &lt;code class=&quot;MathJax_Preview&quot;&gt;C_X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C_X&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;C_Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C_Z&lt;/script&gt; taken from the classical literature, you will find that it is very difficult to pass the commutation criterion. Therefore, more involved methods are needed to construct quantum codes, such as topological constructions or hypergraph products. However, there is on example where our procedure works extremely well: our good old Hamming code!&lt;/p&gt;

&lt;h2 id=&quot;steane-code-the-quantum-version-of-the-hamming-code&quot;&gt;Steane code, the quantum version of the Hamming code&lt;/h2&gt;

&lt;p&gt;It’s finally time to illustrate the stabilizer formalism with a concrete example!
If the two previous sections were feeling a bit abstract, this section should hopefully clarify things.&lt;/p&gt;

&lt;p&gt;So let’s apply the CSS construction to the Hamming code that we introduced in the &lt;a href=&quot;/blog/2022-05-21-classical-error-correction/&quot;&gt;previous blog post&lt;/a&gt;. As a reminder, the Hamming code is a &lt;code class=&quot;MathJax_Preview&quot;&gt;[7,4,3]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[7,4,3]&lt;/script&gt;-code defined by the following three parity check equations&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    x_1 + x_2 + x_3 + x_4 = 0 \\
    x_2 + x_3 + x_5 + x_6 = 0 \\
    x_3 + x_4 + x_6 + x_7 = 0
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    x_1 + x_2 + x_3 + x_4 = 0 \\
    x_2 + x_3 + x_5 + x_6 = 0 \\
    x_3 + x_4 + x_6 + x_7 = 0
\end{aligned}&lt;/script&gt;

&lt;p&gt;Let’s now apply the CSS construction with &lt;code class=&quot;MathJax_Preview&quot;&gt;C_X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C_X&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;C_Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C_Z&lt;/script&gt; two Hamming codes. To &lt;code class=&quot;MathJax_Preview&quot;&gt;C_X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C_X&lt;/script&gt;, we associate the group &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{S}_X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}_X&lt;/script&gt; on 7 qubits defined as:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \mathcal{S}_X = \langle X_1 X_2 X_3 X_4, X_2 X_3 X_5 X_6, X_3 X_4 X_6 X_7 \rangle
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    \mathcal{S}_X = \langle X_1 X_2 X_3 X_4, X_2 X_3 X_5 X_6, X_3 X_4 X_6 X_7 \rangle
\end{aligned}&lt;/script&gt;

&lt;p&gt;and to &lt;code class=&quot;MathJax_Preview&quot;&gt;C_Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C_Z&lt;/script&gt;, the group &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{S}_Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}_Z&lt;/script&gt; defined as:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \mathcal{S}_Z = \langle Z_1 Z_2 Z_3 Z_4, Z_2 Z_3 Z_5 Z_6, Z_3 Z_4 Z_6 Z_7 \rangle
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    \mathcal{S}_Z = \langle Z_1 Z_2 Z_3 Z_4, Z_2 Z_3 Z_5 Z_6, Z_3 Z_4 Z_6 Z_7 \rangle
\end{aligned}&lt;/script&gt;

&lt;p&gt;It is often convenient to visualize stabilizer codes using some graphical representations. Here is how to visualize the stabilizers defined above:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot; class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/img/blog/stabilizer-formalism-1/steane-code-lattice.png&quot; height=&quot;300&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;In this figure, each vertex (numbered from 1 to 7) represents a qubit, and each coloured face (often called &lt;strong&gt;plaquette&lt;/strong&gt; in the literature) supports an &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and a &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; stabilizer. The different plaquette stabilizers are shown explicitly here:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/stabilizer-formalism-1/steane-code-stabilizers.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From this representation, it is easy to see that each plaquette stabilizer intersects with every other plaquette stabilizers on exactly two qubits, which is an even number. As we discussed earlier, it means that elements of &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{S}_X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}_X&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{S}_Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}_Z&lt;/script&gt; commute, and we can form a valid code by combining the generators of the two groups. The resulting code is called the &lt;strong&gt;Steane code&lt;/strong&gt;, and is an example of &lt;strong&gt;color code&lt;/strong&gt; (a very interesting family of codes which would deserve their own blog post).&lt;/p&gt;

&lt;p&gt;The Steane code is often considered a promising candidate for near-term quantum error correction and is indeed one of the first codes to have been implemented on a real device (by different teams of ion trappers) &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. This is due to its many nice properties: its small size (it only requires &lt;code class=&quot;MathJax_Preview&quot;&gt;7&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;7&lt;/script&gt; physical qubits), its 2D locality (it can be built on a 2D lattice without requiring long-range connections to measure the stabilizers), and the presence of many transversal logical gates (a topic for another time). Furthermore, it only requires the measurement of weight-4 stabilizers, as opposed to Shor’s code which requires measuring weight-6 stabilizers. Since errors can happen during the measurement of stabilizers, a good rule of thumb to get well-performing quantum codes is to always try to minimize the weight of its stabilizer generators.&lt;/p&gt;

&lt;p&gt;We will study the characteristics of the Steane code in the next post, showing that it is a &lt;code class=&quot;MathJax_Preview&quot;&gt;[[7,1,3]]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[[7,1,3]]&lt;/script&gt; quantum code. Meanwhile, we can already look at what happens in the presence of single-qubit errors. Since &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors are detected in the same way (using either &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; or &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; stabilizers on the plaquettes), we can consider the effect of &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors only, without loss of generality. Below is the observed syndrome for a single &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; error on qubits &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; to &lt;code class=&quot;MathJax_Preview&quot;&gt;3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt;:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot; class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/img/blog/stabilizer-formalism-1/steane-code-errors.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;In this figure, the purple vertices correspond to &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors, and the highlighted plaquettes to stabilizer measurements of &lt;code class=&quot;MathJax_Preview&quot;&gt;-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt;.
To obtain this this result, note that a single-qubit &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; error will always anticommute with the &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; plaquette it touches (you can easily show this using the property of the appendix).&lt;/p&gt;

&lt;p&gt;You can continue this exercise with the remaining single-qubit errors, and you will see that they all lead to a different syndrome.
Therefore, the Steane code can correct any single-qubit error.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post, we have introduced the most important tool to build and analyze quantum codes: the stabilizer formalism. Starting from a stabilizer group (set of commuting Pauli operators), we found that we can construct a codespace by considering the common &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; eigenspace of its elements. We defined the family of CSS codes, whose stabilizer generators can be split into pure &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and pure &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; elements. We studied a few examples of stabilizer codes: the &lt;code class=&quot;MathJax_Preview&quot;&gt;[[3,1,1]]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[[3,1,1]]&lt;/script&gt; repetition code, the &lt;code class=&quot;MathJax_Preview&quot;&gt;[[9,1,3]]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[[9,1,3]]&lt;/script&gt; Shor code, and the &lt;code class=&quot;MathJax_Preview&quot;&gt;[[7,1,3]]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[[7,1,3]]&lt;/script&gt; Steane code.&lt;/p&gt;

&lt;p&gt;In the next post, we will go further in our study of stabilizer codes: we will learn how to find the logical operators, the distance and the number of encoded qubits of a code. We will see how the stabilizer formalism can be expressed in a matrix form, by generalizing parity-check matrices to the quantum realm. This matrix formulation will help us to define the decoding problem for quantum codes. We will finish our theoretical study of stabilizer codes by introducing the Clifford group and stabilizer states, concepts which will become very handy when learning about logical gates and the simulation of error-correcting codes. The Steane code will continue to serve as our main example throughout the next post.&lt;/p&gt;

&lt;h2 id=&quot;appendix-useful-tricks-to-manipulate-pauli-operators&quot;&gt;Appendix: useful tricks to manipulate Pauli operators&lt;/h2&gt;

&lt;p&gt;For the discussion that follows, let’s define a Pauli operator as an operator of the form &lt;code class=&quot;MathJax_Preview&quot;&gt;P_1 \otimes \ldots \otimes P_n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P_1 \otimes \ldots \otimes P_n&lt;/script&gt; with &lt;code class=&quot;MathJax_Preview&quot;&gt;P_i \in \{I, X, Y, Z\}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P_i \in \{I, X, Y, Z\}&lt;/script&gt; for all &lt;code class=&quot;MathJax_Preview&quot;&gt;i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;. As a reminder, here is the definitions of the Pauli matrices &lt;code class=&quot;MathJax_Preview&quot;&gt;X,Y,Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X,Y,Z&lt;/script&gt;:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    X = \left(
        \begin{matrix}
            0 &amp;amp; 1 \\ 1 &amp;amp; 0
        \end{matrix}
    \right), \;
    Y = \left(
        \begin{matrix}
            0 &amp;amp; -i \\ i &amp;amp; 0
        \end{matrix}
    \right), \;
    Z = \left(
        \begin{matrix}
            1 &amp;amp; 0 \\ 0 &amp;amp; -1
        \end{matrix}
    \right) \;
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    X = \left(
        \begin{matrix}
            0 &amp; 1 \\ 1 &amp; 0
        \end{matrix}
    \right), \;
    Y = \left(
        \begin{matrix}
            0 &amp; -i \\ i &amp; 0
        \end{matrix}
    \right), \;
    Z = \left(
        \begin{matrix}
            1 &amp; 0 \\ 0 &amp; -1
        \end{matrix}
    \right) \;
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We say that two Pauli operators &lt;code class=&quot;MathJax_Preview&quot;&gt;P&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;P'&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P'&lt;/script&gt; commute if &lt;code class=&quot;MathJax_Preview&quot;&gt;P P' = P'P&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P P' = P'P&lt;/script&gt;, and anticommute if &lt;code class=&quot;MathJax_Preview&quot;&gt;P P' = -P' P&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P P' = -P' P&lt;/script&gt;. The goal of this section is to prove the following extremely useful fact about Pauli operators:&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Property&lt;/strong&gt;: Two Pauli operators &lt;code class=&quot;MathJax_Preview&quot;&gt;P&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;P'&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P'&lt;/script&gt; commute if they intersect on an even number of terms with a different Pauli element. Otherwise, they anticommute.&lt;/p&gt;

&lt;p&gt;For instance, &lt;code class=&quot;MathJax_Preview&quot;&gt;X_1 Y_2 Z_3 X_4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X_1 Y_2 Z_3 X_4&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;X_1 Z_4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X_1 Z_4&lt;/script&gt; anticommute: they intersect on qubits &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; (with the same Pauli) and &lt;code class=&quot;MathJax_Preview&quot;&gt;4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;4&lt;/script&gt; (with a different Pauli), so only on one qubit with a different Pauli. On the other hand, &lt;code class=&quot;MathJax_Preview&quot;&gt;X_1 X_2 X_3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X_1 X_2 X_3&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;Z_2 Z_3 Z_4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_2 Z_3 Z_4&lt;/script&gt; commute as they intersect on two terms (qubits &lt;code class=&quot;MathJax_Preview&quot;&gt;2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt;) with a different Pauli.&lt;/p&gt;

&lt;p&gt;This property is used all the time in quantum error correction: to check that the stabilizers of a code commute, to see how errors affect the stabilizer measurements, etc. So it’s worth getting comfortable with it early in your QEC journey.&lt;/p&gt;

&lt;p&gt;So let’s show this fact. Let &lt;code class=&quot;MathJax_Preview&quot;&gt;P=P_1 \ldots P_n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P=P_1 \ldots P_n&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;P'=P'_1 \ldots P'_n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P'=P'_1 \ldots P'_n&lt;/script&gt; two Pauli operators, with each &lt;code class=&quot;MathJax_Preview&quot;&gt;P_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P_i&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;P'_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P'_i&lt;/script&gt; acting on qubit &lt;code class=&quot;MathJax_Preview&quot;&gt;i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;. Our objective is to go from &lt;code class=&quot;MathJax_Preview&quot;&gt;PP'=P_1 \ldots P_n P'_1 \ldots P'_n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;PP'=P_1 \ldots P_n P'_1 \ldots P'_n&lt;/script&gt; to &lt;code class=&quot;MathJax_Preview&quot;&gt;P'P=P'_1 \ldots P'_n P_1 \ldots P_n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P'P=P'_1 \ldots P'_n P_1 \ldots P_n&lt;/script&gt;.
For that, we will move each term &lt;code class=&quot;MathJax_Preview&quot;&gt;P'_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P'_i&lt;/script&gt; to the top. Since any &lt;code class=&quot;MathJax_Preview&quot;&gt;P'_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P'_i&lt;/script&gt; commute with all the terms &lt;code class=&quot;MathJax_Preview&quot;&gt;P_j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P_j&lt;/script&gt; with &lt;code class=&quot;MathJax_Preview&quot;&gt;i \neq j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i \neq j&lt;/script&gt; (they act on different qubits), we can move it next to &lt;code class=&quot;MathJax_Preview&quot;&gt;P_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P_i&lt;/script&gt;:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
PP'=P_1 \ldots P_i P'_i P_{i+1} \ldots P_n P'_1 \ldots P'_{i-1} P'_{i+1} \ldots P_n
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
PP'=P_1 \ldots P_i P'_i P_{i+1} \ldots P_n P'_1 \ldots P'_{i-1} P'_{i+1} \ldots P_n
\end{aligned}&lt;/script&gt;

&lt;p&gt;Now, if &lt;code class=&quot;MathJax_Preview&quot;&gt;P&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;P'&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P'&lt;/script&gt; don’t intersect on qubit &lt;code class=&quot;MathJax_Preview&quot;&gt;i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;, that is either &lt;code class=&quot;MathJax_Preview&quot;&gt;P_i=I&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P_i=I&lt;/script&gt; or &lt;code class=&quot;MathJax_Preview&quot;&gt;P'_i = I&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P'_i = I&lt;/script&gt;, we will have &lt;code class=&quot;MathJax_Preview&quot;&gt;P_i P'_i = P'_i P_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P_i P'_i = P'_i P_i&lt;/script&gt;. Same result if &lt;code class=&quot;MathJax_Preview&quot;&gt;P_i = P'_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P_i = P'_i&lt;/script&gt;. In those two cases, we can move &lt;code class=&quot;MathJax_Preview&quot;&gt;P'_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P'_i&lt;/script&gt; to the top without introducing any minus sign:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
PP'=P'_i P_1 \ldots P_n P'_1 \ldots P'_{i-1} P'_{i+1} \ldots P_n
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
PP'=P'_i P_1 \ldots P_n P'_1 \ldots P'_{i-1} P'_{i+1} \ldots P_n
\end{aligned}&lt;/script&gt;

&lt;p&gt;On the other hand, if &lt;code class=&quot;MathJax_Preview&quot;&gt;P_i \neq P'_i \neq I&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P_i \neq P'_i \neq I&lt;/script&gt;, we will have &lt;code class=&quot;MathJax_Preview&quot;&gt;P_i P'_i = - P'_i P_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P_i P'_i = - P'_i P_i&lt;/script&gt; (remember that &lt;code class=&quot;MathJax_Preview&quot;&gt;XZ+ZX=XY+YX=YZ+ZY=0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;XZ+ZX=XY+YX=YZ+ZY=0&lt;/script&gt;). This means that moving &lt;code class=&quot;MathJax_Preview&quot;&gt;P'_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P'_i&lt;/script&gt; to the top introduces a minus sign:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
PP'=-P'_i P_1 \ldots P_n P'_1 \ldots P'_{i-1} P'_{i+1} \ldots P_n
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
PP'=-P'_i P_1 \ldots P_n P'_1 \ldots P'_{i-1} P'_{i+1} \ldots P_n
\end{aligned}&lt;/script&gt;

&lt;p&gt;Therefore, each intersection with a different Pauli element introduces a minus sign, and the overall sign will be &lt;code class=&quot;MathJax_Preview&quot;&gt;+1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; if and only if there is an even number of such intersections, which proves our propositions.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Note that I’m using a different notation from the previous post, giving variables a more generalizable name. If that confuses you, here is the exact mapping: &lt;code class=&quot;MathJax_Preview&quot;&gt;z_1 \rightarrow x_1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_1 \rightarrow x_1&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;z_2 \rightarrow x_5&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_2 \rightarrow x_5&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;z_3 \rightarrow x_7&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_3 \rightarrow x_7&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;a \rightarrow x_2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a \rightarrow x_2&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;b \rightarrow x_3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;b \rightarrow x_3&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;c \rightarrow x_6&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;c \rightarrow x_6&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;d \rightarrow x_4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d \rightarrow x_4&lt;/script&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;The &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 0 \rangle_{L}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 0 \rangle_{L}&lt;/script&gt; state of the Steane code was first implemented by an Austrian team, in &lt;a href=&quot;https://arxiv.org/abs/1403.5426&quot;&gt;Nigg et al., 2014&lt;/a&gt;. Actual error correction using stabilizer measurements was then done by Honeywell in &lt;a href=&quot;https://arxiv.org/abs/2107.07505&quot;&gt;Ryan-Anderson et al., 2021&lt;/a&gt;. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Arthur Pesah</name><email>arthur.pesah@gmail.com</email></author><category term="blog" /><category term="quantum-computing" /><summary type="html">Now that you know all you need to know about classical error correction, the time has finally come to learn how to correct those damn errors that keep sabotaging your quantum computer! The key tool, introduced by Daniel Gottesman in his landmark 1997 PhD thesis is the stabilizer formalism. The same way most classical codes fall into the linear code category, almost all the quantum codes you will encounter can be classified as stabilizer codes. And for a good reason: stabilizer codes are simply the quantum generalization of linear codes! Your beloved parity checks will turn into stabilizers, a set of commuting measurements controlling the parity of your qubits in different bases. Parity-check matrices and Tanner graphs will get slightly bigger and more constrained. But apart from that, if you’re more or less comfortable with the notions discussed in the last post, going quantum shouldn’t give you too much trouble.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arthurpesah.me/assets/img/blog/stabilizer-formalism-1/thumbnail.png" /><media:content medium="image" url="https://arthurpesah.me/assets/img/blog/stabilizer-formalism-1/thumbnail.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">All you need to know about classical error correction</title><link href="https://arthurpesah.me/blog/2022-05-21-classical-error-correction/" rel="alternate" type="text/html" title="All you need to know about classical error correction" /><published>2022-05-21T00:00:00+02:00</published><updated>2022-05-21T00:00:00+02:00</updated><id>https://arthurpesah.me/blog/classical-error-correction</id><content type="html" xml:base="https://arthurpesah.me/blog/2022-05-21-classical-error-correction/">&lt;p&gt;When learning about quantum error correction (QEC) for the first time, I tried to jump directly into the core of the subject, going from the stabilizer formalism to topological codes and decoders, but completely missing the classical origin of those notions. The reason is that many introductions to the subject do a great job presenting all those concepts in a self-contained way, without assuming any knowledge in error correction.
So why bother learning classical error correction at all? Because if you dig deeper, you will find classical error correction concepts sprinkled all over QEC. Important classical notions such as parity checks, linear codes, Tanner graphs, belief propagation, low-density parity-check (LDPC) codes and many more, have natural generalizations in the quantum world and have been widely used in the development of QEC. Learning about classical error correction a few months into my QEC journey was completely illuminating: many ideas that I only understood formally suddenly made sense intuitively, and I was able to understand the content of many more papers. For this reason, I’d like this second article on quantum error correction to actually be about classical error correction. You will learn all you need to start off your QEC journey on the right foot!&lt;/p&gt;

&lt;p&gt;So what exactly are we gonna study today? The central notion in this post is that of &lt;strong&gt;linear code&lt;/strong&gt;. Linear codes form one of the most important families of error-correcting codes. It includes for instance Hamming codes (some of the earliest codes, invented in 1950), Reed-Solomon codes (used in CDs and QR codes), Turbo codes (used in 3G/4G communication) and LDPC codes (used in 5G communication). While we won’t try to cover the whole zoo of linear codes here, we will introduce some crucial tools to understand them. In particular, the goal of this post is to give you a good grasp of the parity-check matrix, and its graphical representation, the Tanner graph. As we will see when going quantum, the stabilizer formalism (the dominant paradigm to construct quantum codes) can be understood as a direct generalization of linear coding theory, and the quantum parity-check matrix happens to be an essential tool to simulate quantum codes in practice. This post will also be the occasion to introduce some more general coding terminology (encoding, decoding, distance, &lt;code class=&quot;MathJax_Preview&quot;&gt;[n,k,d]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[n,k,d]&lt;/script&gt;-code, codewords, etc.), which will be handy when delving into QEC.&lt;/p&gt;

&lt;p&gt;As a final motivating factor before starting, it happens that this field contains some of the most beautiful ideas in computer science. To get a sense of this beauty, I recommend watching the &lt;a href=&quot;https://youtu.be/X8jsijhllIA&quot;&gt;3Blue1Brown videos on the Hamming code&lt;/a&gt; as a complement to this post. It will give you a very visual picture of some of the techniques introduced here. However, it’s not a prerequisite, so feel free to continue reading this post and watch the video at a later time. On that note, let’s start!&lt;/p&gt;

&lt;h2 id=&quot;general-setting&quot;&gt;General setting&lt;/h2&gt;

&lt;p&gt;Let us consider the following setting: we would like to send a &lt;code class=&quot;MathJax_Preview&quot;&gt;k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;-bit message &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{x}&lt;/script&gt; across a noisy channel, for instance between your phone and a satellite. If we choose to send &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{x}&lt;/script&gt; directly without any pre-processing, a different result &lt;code class=&quot;MathJax_Preview&quot;&gt;\widetilde{\bm{x}}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\widetilde{\bm{x}}&lt;/script&gt; will arrive with a certain number of errors. Here, we will assume that all errors are bit-flip errors, meaning that they can turn a &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; into &lt;code class=&quot;MathJax_Preview&quot;&gt;0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; or a &lt;code class=&quot;MathJax_Preview&quot;&gt;0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; into &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; (potentially with a different probability).&lt;/p&gt;

&lt;p&gt;To protect the message against bit-flip errors, we can choose to add redundancy to it. For instance, let’s send all the bits three times: each &lt;code class=&quot;MathJax_Preview&quot;&gt;0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; becomes &lt;code class=&quot;MathJax_Preview&quot;&gt;000&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;000&lt;/script&gt; and each &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; becomes &lt;code class=&quot;MathJax_Preview&quot;&gt;111&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;111&lt;/script&gt;. For example, if we want to send the message &lt;code class=&quot;MathJax_Preview&quot;&gt;10100&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;10100&lt;/script&gt;, we should send instead&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    111,000,111,000,000
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    111,000,111,000,000
\end{aligned}&lt;/script&gt;

&lt;p&gt;Now, imagine some errors have occurred, and the satellite receives the following message instead:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    101,000,111,000,100
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    101,000,111,000,100
\end{aligned}&lt;/script&gt;

&lt;p&gt;Assuming that at most one error has occurred on each triplet, the original message can be decoded by taking a majority vote: &lt;code class=&quot;MathJax_Preview&quot;&gt;101&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;101&lt;/script&gt; is decoded as &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;100&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;100&lt;/script&gt; is decoded as &lt;code class=&quot;MathJax_Preview&quot;&gt;0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;. The message &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}=\bm{x} \bm{x} \bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}=\bm{x} \bm{x} \bm{x}&lt;/script&gt; that we send across the channel is called an &lt;strong&gt;encoding&lt;/strong&gt; of &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{x}&lt;/script&gt;, while trying to infer the original message &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{x}&lt;/script&gt; from the noisy encoding &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{\tilde{y}}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{\tilde{y}}&lt;/script&gt; is called &lt;strong&gt;decoding&lt;/strong&gt;. A particular encoding is often called a &lt;strong&gt;code&lt;/strong&gt; as well, and the example we have seen is called the &lt;strong&gt;3-repetition code&lt;/strong&gt;. More generally, an error-correction process can be summarized with the following diagram:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/classical-error-correction/error-correction-diagram.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s now introduce some important jargon. In general, an encoder &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{E}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{E}&lt;/script&gt; divides a message into chunks of &lt;code class=&quot;MathJax_Preview&quot;&gt;k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; bits, and encodes them into &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; bits. We write &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{E}(\bm{x})=\bm{y}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{E}(\bm{x})=\bm{y}&lt;/script&gt;, where &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{x}&lt;/script&gt; is a &lt;code class=&quot;MathJax_Preview&quot;&gt;k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;-bit message and &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}&lt;/script&gt; is an &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-bit encoding.
For example, in the 3-repetition code, we are encoding one bit at a time into three bits, so &lt;code class=&quot;MathJax_Preview&quot;&gt;k=1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k=1&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;n=3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n=3&lt;/script&gt;, and &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{E}(x)=xxx&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{E}(x)=xxx&lt;/script&gt; for each bit &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;codeword&lt;/strong&gt; is an element of the image of &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{E}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{E}&lt;/script&gt;. We denote the set of all codewords as &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{C}=\text{Im}(\mathcal{E})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}=\text{Im}(\mathcal{E})&lt;/script&gt;. For instance, in the &lt;strong&gt;3-repetition code&lt;/strong&gt;, we have two codewords: &lt;code class=&quot;MathJax_Preview&quot;&gt;000&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;000&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;111&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;111&lt;/script&gt;, respectively given by &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{E}(0)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{E}(0)&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{E}(1)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{E}(1)&lt;/script&gt;. In this example, we see that if one or two errors occur on a given codeword, the resulting bit-string won’t be a codeword anymore, making the error &lt;strong&gt;detectable&lt;/strong&gt;. For instance, if you see &lt;code class=&quot;MathJax_Preview&quot;&gt;110&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;110&lt;/script&gt; in your message, you know that an error has occurred, since &lt;code class=&quot;MathJax_Preview&quot;&gt;110&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;110&lt;/script&gt; is not a codeword. On the other hands, the error will only be &lt;strong&gt;correctable&lt;/strong&gt; by our majority vote procedure if at most one error occurs. Finally, if three bit-flips occur—going from &lt;code class=&quot;MathJax_Preview&quot;&gt;000&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;000&lt;/script&gt; to &lt;code class=&quot;MathJax_Preview&quot;&gt;111&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;111&lt;/script&gt; or the reverse—, the errors will be &lt;strong&gt;undetectable&lt;/strong&gt;, resulting in what we call a &lt;strong&gt;logical error&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This argument can be generalized using the notion of &lt;strong&gt;distance&lt;/strong&gt;. The distance &lt;code class=&quot;MathJax_Preview&quot;&gt;d&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; of a code is the minimum number of bit-flips required to pass from one codeword to another, or in other words, the minimum number of errors that would be undetectable with our code. To define the distance more formally, we need to introduce two important notions. The &lt;strong&gt;Hamming weight&lt;/strong&gt; &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert\bm{x}\vert&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert\bm{x}\vert&lt;/script&gt; of a binary vector &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{x}&lt;/script&gt; is the number of &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; of this vector. The &lt;strong&gt;Hamming distance&lt;/strong&gt; &lt;code class=&quot;MathJax_Preview&quot;&gt;D_H(\bm{x},\bm{y})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;D_H(\bm{x},\bm{y})&lt;/script&gt; between two binary vectors &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{x}&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}&lt;/script&gt; is the number of components that differ between &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{x}&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}&lt;/script&gt;, i.e.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    D_H(\bm{x},\bm{y})=\vert \bm{x} - \bm{y} \vert
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    D_H(\bm{x},\bm{y})=\vert \bm{x} - \bm{y} \vert
\end{aligned}&lt;/script&gt;

&lt;p&gt;We can then define the distance as the minimum Hamming distance between two vectors:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    d=\min_{\bm{x},\bm{y} \in \mathcal{C}} D_H(\bm{x},\bm{y})
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    d=\min_{\bm{x},\bm{y} \in \mathcal{C}} D_H(\bm{x},\bm{y})
\end{aligned}&lt;/script&gt;

&lt;p&gt;For instance, the distance of the 3-repetition code is 3 (we need to flip all 3 bits to have an undetectable error).
A code that encodes &lt;code class=&quot;MathJax_Preview&quot;&gt;k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; bits with &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; bits and has a distance &lt;code class=&quot;MathJax_Preview&quot;&gt;d&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; is called an &lt;code class=&quot;MathJax_Preview&quot;&gt;[n,k,d]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[n,k,d]&lt;/script&gt;&lt;strong&gt;-code&lt;/strong&gt;. The 3-repetition code is an example of &lt;code class=&quot;MathJax_Preview&quot;&gt;[3,1,3]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[3,1,3]&lt;/script&gt;-code.&lt;/p&gt;

&lt;p&gt;Finally, the &lt;strong&gt;rate&lt;/strong&gt; of an &lt;code class=&quot;MathJax_Preview&quot;&gt;[n, k, d]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[n, k, d]&lt;/script&gt;-code is defined as &lt;code class=&quot;MathJax_Preview&quot;&gt;R=k/n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;R=k/n&lt;/script&gt; (so &lt;code class=&quot;MathJax_Preview&quot;&gt;R=1/3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;R=1/3&lt;/script&gt; in our case).&lt;/p&gt;

&lt;p&gt;The difficulty in designing good error-correcting codes lies in the trade-off between rate and distance. Indeed, we would ideally like both quantities to be high: a high rate means that there is a low overhead in the encoding process (we only need a few redundancy bits), and a high distance means that we can correct many errors. So can we maximize both quantities? Unfortunately, many bounds have been established on this trade-off, telling us that high-rate codes must have low distance, and high-distance codes must have low rate. The easiest bound to understand is the &lt;strong&gt;Singleton bound&lt;/strong&gt;, which states that for any linear &lt;code class=&quot;MathJax_Preview&quot;&gt;[n, k, d]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[n, k, d]&lt;/script&gt;-code (we will see the definition of a linear code shortly), we have&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    k \leq n - d + 1
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    k \leq n - d + 1
\end{aligned}&lt;/script&gt;

&lt;p&gt;or in terms of rate&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    R \leq 1 - \frac{d - 1}{n}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    R \leq 1 - \frac{d - 1}{n}
\end{aligned}&lt;/script&gt;

&lt;p&gt;This shows that we cannot arbitrarily increase the rate without decreasing the distance. We will see an interesting illustration of this trade-off in this post by introducing two codes with opposite characteristics: the Hamming code and the simplex code. While the first one is an &lt;code class=&quot;MathJax_Preview&quot;&gt;[n, n - \log(n+1), 3]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[n, n - \log(n+1), 3]&lt;/script&gt;-code (high rate, low distance), the second one is an &lt;code class=&quot;MathJax_Preview&quot;&gt;[n, \log(n), n/2]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[n, \log(n), n/2]&lt;/script&gt;-code (low rate, high distance).&lt;/p&gt;

&lt;p&gt;We have introduced a lot of notations and jargon in this section, which can be overwhelming when seen for the first time. That’s what happens when you decide to learn a new field! But don’t get discouraged, those notations will keep appearing all the time during your (quantum) error correction learning trip, so they will soon be very familiar to you. In the meantime, I encourage you to do the following three (short) exercises to consolidate what you’ve just learned (the solutions are at the end of the post).&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 1&lt;/strong&gt;: We define the &lt;code class=&quot;MathJax_Preview&quot;&gt;\ell&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\ell&lt;/script&gt;&lt;strong&gt;-repetition code&lt;/strong&gt; as the repetition of each bit of the message &lt;code class=&quot;MathJax_Preview&quot;&gt;\ell&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\ell&lt;/script&gt; times. Work out the encoding function &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{E}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{E}&lt;/script&gt; and the different codewords, as well the parameters &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;d&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; of the code. How many errors per codeword are detectable? Correctable? What is the rate of the code, and its limit when &lt;code class=&quot;MathJax_Preview&quot;&gt;\ell \rightarrow \infty&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\ell \rightarrow \infty&lt;/script&gt;?&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 2&lt;/strong&gt;: Show that an &lt;code class=&quot;MathJax_Preview&quot;&gt;[n,k,d]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[n,k,d]&lt;/script&gt;-code can &lt;strong&gt;detect&lt;/strong&gt; up to &lt;code class=&quot;MathJax_Preview&quot;&gt;d-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d-1&lt;/script&gt; errors and &lt;strong&gt;correct&lt;/strong&gt; up to &lt;code class=&quot;MathJax_Preview&quot;&gt;\frac{d-1}{2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{d-1}{2}&lt;/script&gt; errors.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 3&lt;/strong&gt;: Show that for an &lt;code class=&quot;MathJax_Preview&quot;&gt;[n,k,d]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[n,k,d]&lt;/script&gt;-code defined by the set of codewords &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{C}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt;, we have &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert\mathcal{C}\vert=2^k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert\mathcal{C}\vert=2^k&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;parity-checks-and-hamming-codes&quot;&gt;Parity-checks and Hamming codes&lt;/h2&gt;

&lt;p&gt;In 1950, Richard Hamming discovered a more intelligent way to introduce redundancy in a message than having to repeat it several times. As a warm-up to understand his method, let’s consider the following code, called &lt;strong&gt;simple parity-check code&lt;/strong&gt;, that encodes &lt;code class=&quot;MathJax_Preview&quot;&gt;k=3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k=3&lt;/script&gt; bits into &lt;code class=&quot;MathJax_Preview&quot;&gt;n=4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n=4&lt;/script&gt; bits&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \mathcal{E}(abc) = a b c z
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    \mathcal{E}(abc) = a b c z
\end{aligned}&lt;/script&gt;

&lt;p&gt;where &lt;code class=&quot;MathJax_Preview&quot;&gt;z = a + b + c \; (\text{mod} \; 2)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z = a + b + c \; (\text{mod} \; 2)&lt;/script&gt;. We call &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; a &lt;strong&gt;parity-check bit&lt;/strong&gt;, as it indicates whether there is an even or an odd number of &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;s in the sum (&lt;code class=&quot;MathJax_Preview&quot;&gt;z=0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z=0&lt;/script&gt; for even and &lt;code class=&quot;MathJax_Preview&quot;&gt;z=1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z=1&lt;/script&gt; for odd). As an exercise, try to write the different codewords corresponding to this encoding map!&lt;/p&gt;

&lt;p&gt;Now, we can show that any single error can be detected by this code. Indeed, if one of the bits &lt;code class=&quot;MathJax_Preview&quot;&gt;a, b, c&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a, b, c&lt;/script&gt; gets flipped, the parity of the three bits will be reversed, and we won’t have &lt;code class=&quot;MathJax_Preview&quot;&gt;z=a + b + c \; (\text{mod} \; 2)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z=a + b + c \; (\text{mod} \; 2)&lt;/script&gt; anymore, indicating that an error have occurred. Similarly, if &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; gets flipped, it won’t correspond to the parity of &lt;code class=&quot;MathJax_Preview&quot;&gt;a,b,c&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a,b,c&lt;/script&gt; anymore and we will detect an error.&lt;/p&gt;

&lt;p&gt;However, there is no way to know &lt;em&gt;where&lt;/em&gt; the error has occurred using this code, or in other words, errors are not correctable. The genius of Hamming was to find a way to use parity checks to actually know the position of the error!&lt;/p&gt;

&lt;p&gt;To illustrate his method, let us consider a 4-bit message &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{x}=abcd&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{x}=abcd&lt;/script&gt;, as well as the three variables &lt;code class=&quot;MathJax_Preview&quot;&gt;z_1, z_2, z_3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_1, z_2, z_3&lt;/script&gt; given by&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    z_1 &amp;amp;= a + b + d \\
    z_2 &amp;amp;= a + c + d \\
    z_3 &amp;amp;= b + c + d \\
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    z_1 &amp;= a + b + d \\
    z_2 &amp;= a + c + d \\
    z_3 &amp;= b + c + d \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the sum is taken modulo 2 (we will consider all the sums of bits to be modulo 2 from now on, without explicitely writing &lt;code class=&quot;MathJax_Preview&quot;&gt;\mod 2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mod 2&lt;/script&gt;). Those variables indicate the parity of different chunks of our message, as illustrated here:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/classical-error-correction/hamming-code.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this diagram, each circle represents a parity-check bit &lt;code class=&quot;MathJax_Preview&quot;&gt;z_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_i&lt;/script&gt;, and each intersection represents a message bit &lt;code class=&quot;MathJax_Preview&quot;&gt;a,b,c,d&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a,b,c,d&lt;/script&gt;. By construction, each parity-check bit only involves the three message bits contained in its corresponding circle.&lt;/p&gt;

&lt;p&gt;If we now send the 7-bit message &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}=\mathcal{E}(abcd)=abcdz_1z_2z_3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}=\mathcal{E}(abcd)=abcdz_1z_2z_3&lt;/script&gt;, we can show that any single-bit error will be correctable. Indeed, let’s see what happens if an error occurs on bit &lt;code class=&quot;MathJax_Preview&quot;&gt;a&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;. In this case, we can read in the diagram above that both &lt;code class=&quot;MathJax_Preview&quot;&gt;z_1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_1&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;z_2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_2&lt;/script&gt; parity checks will be violated, while &lt;code class=&quot;MathJax_Preview&quot;&gt;z_3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_3&lt;/script&gt; will remain equal to &lt;code class=&quot;MathJax_Preview&quot;&gt;b+c+d&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;b+c+d&lt;/script&gt;. This can only happen if &lt;code class=&quot;MathJax_Preview&quot;&gt;a&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; is flipped, which allows us to correct the error. A similar reasoning can be performed for the other bits. This code, called the &lt;code class=&quot;MathJax_Preview&quot;&gt;[7,4]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[7,4]&lt;/script&gt;-&lt;strong&gt;Hamming code&lt;/strong&gt;, is a  &lt;code class=&quot;MathJax_Preview&quot;&gt;[7,4,3]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[7,4,3]&lt;/script&gt;-code with a rate &lt;code class=&quot;MathJax_Preview&quot;&gt;R=4/7 \approx 0.57&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;R=4/7 \approx 0.57&lt;/script&gt;, which is already better than the rate &lt;code class=&quot;MathJax_Preview&quot;&gt;R \approx 0.33&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;R \approx 0.33&lt;/script&gt; of the &lt;code class=&quot;MathJax_Preview&quot;&gt;3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt;-repetition code, for the same distance.&lt;/p&gt;

&lt;p&gt;The construction presented here can be generalized, leading to a whole family of Hamming codes defined for any &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; of the form &lt;code class=&quot;MathJax_Preview&quot;&gt;n=2^r-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n=2^r-1&lt;/script&gt;. We won’t go into the details of this construction here, but if you’re interested, I encourage you to watch &lt;a href=&quot;(https://youtu.be/X8jsijhllIA)&quot;&gt;this 3Blue1Brown video&lt;/a&gt; on the topic. The main takeaway from the general Hamming code construction is that we only need a logarithmic number of parity checks to correct all single-bit errors! More precisely, Hamming codes are &lt;code class=&quot;MathJax_Preview&quot;&gt;[2^r-1, 2^r-r-1, 3]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[2^r-1, 2^r-r-1, 3]&lt;/script&gt;-codes, with a rate &lt;code class=&quot;MathJax_Preview&quot;&gt;R=\frac{2^r-r-1}{2^r-1}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;R=\frac{2^r-r-1}{2^r-1}&lt;/script&gt; that converges to &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; when &lt;code class=&quot;MathJax_Preview&quot;&gt;r \rightarrow \infty&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;r \rightarrow \infty&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;However, Hamming codes have a low distance that doesn’t increase with the codeword length, and they would therefore be impractical in very noisy systems. So we need a more general framework that would allow us to find new codes with better characteristics. That framework is the one of linear codes.&lt;/p&gt;

&lt;h2 id=&quot;linear-codes&quot;&gt;Linear codes&lt;/h2&gt;

&lt;p&gt;This idea of transmitting both the message and some parity-check bits can be generalized with the notion of linear code. A linear code consists in using a matrix &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}&lt;/script&gt;—called &lt;strong&gt;generator matrix&lt;/strong&gt;—as our code, i.e.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \bm{y} = \bm{G} \bm{x}.
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    \bm{y} = \bm{G} \bm{x}.
\end{aligned}&lt;/script&gt;

&lt;p&gt;If our message &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{x}&lt;/script&gt; has length &lt;code class=&quot;MathJax_Preview&quot;&gt;k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; and is complemented by &lt;code class=&quot;MathJax_Preview&quot;&gt;m&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; parity checks, such that &lt;code class=&quot;MathJax_Preview&quot;&gt;n=k+m&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n=k+m&lt;/script&gt; is the size of &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}&lt;/script&gt;, we can write &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}&lt;/script&gt; as&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \bm{G}=\left(\begin{matrix} I_k \\ \hline \bm{A}  \end{matrix} \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    \bm{G}=\left(\begin{matrix} I_k \\ \hline \bm{A}  \end{matrix} \right)
\end{aligned}&lt;/script&gt;

&lt;p&gt;with &lt;code class=&quot;MathJax_Preview&quot;&gt;I_k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;I_k&lt;/script&gt; the &lt;code class=&quot;MathJax_Preview&quot;&gt;k \times k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k \times k&lt;/script&gt; identity matrix (used to reproduce the message in the code) and &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{A}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{A}&lt;/script&gt; an &lt;code class=&quot;MathJax_Preview&quot;&gt;m \times k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m \times k&lt;/script&gt; matrix that performs the parity checks. In this notation, all the matrix operations are performed modulo 2. For instance, the generator matrix of the Hamming code can be written&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \bm{G} = \left( \begin{matrix}
    1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; 1 &amp;amp; 0&amp;amp; 0 \\
    0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\
    \hline
    1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 \\
    1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 \\
    0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1
    \end{matrix} \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    \bm{G} = \left( \begin{matrix}
    1 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 1 &amp; 0&amp; 0 \\
    0 &amp; 0 &amp; 1 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 1 \\
    \hline
    1 &amp; 1 &amp; 0 &amp; 1 \\
    1 &amp; 0 &amp; 1 &amp; 1 \\
    0 &amp; 1 &amp; 1 &amp; 1
    \end{matrix} \right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;since&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \bm{G} \bm{x} =
    \left(
        \begin{matrix}
            1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\
            0 &amp;amp; 1 &amp;amp; 0&amp;amp; 0 \\
            0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\
            0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\
            \hline
            1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 \\
            1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 \\
            0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1
        \end{matrix}
    \right)
    \left(
        \begin{matrix}
            a \\
            b \\
            c \\
            d
        \end{matrix}
    \right)
    =
    \left(
        \begin{matrix}
            a \\
            b \\
            c \\
            d \\
            a + b + d \\
            a + c + d \\
            b + c + d
        \end{matrix}
    \right)
    =
    \left(
        \begin{matrix}
            a \\
            b \\
            c \\
            d \\
            z_1 \\
            z_2 \\
            z_3
        \end{matrix}
    \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    \bm{G} \bm{x} =
    \left(
        \begin{matrix}
            1 &amp; 0 &amp; 0 &amp; 0 \\
            0 &amp; 1 &amp; 0&amp; 0 \\
            0 &amp; 0 &amp; 1 &amp; 0 \\
            0 &amp; 0 &amp; 0 &amp; 1 \\
            \hline
            1 &amp; 1 &amp; 0 &amp; 1 \\
            1 &amp; 0 &amp; 1 &amp; 1 \\
            0 &amp; 1 &amp; 1 &amp; 1
        \end{matrix}
    \right)
    \left(
        \begin{matrix}
            a \\
            b \\
            c \\
            d
        \end{matrix}
    \right)
    =
    \left(
        \begin{matrix}
            a \\
            b \\
            c \\
            d \\
            a + b + d \\
            a + c + d \\
            b + c + d
        \end{matrix}
    \right)
    =
    \left(
        \begin{matrix}
            a \\
            b \\
            c \\
            d \\
            z_1 \\
            z_2 \\
            z_3
        \end{matrix}
    \right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Two important remarks about generator matrices:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Elementary operations on the rows and columns of &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}&lt;/script&gt; don’t change the code. Indeed, the code is defined as the image of &lt;code class=&quot;MathJax_Preview&quot;&gt;G&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;, which is invariant under similarity transformations. Using Gaussian reduction, it is therefore always possible to transform &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}&lt;/script&gt; to have the form &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}=\left(\begin{matrix} I_k \\ \hline \bm{A}  \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}=\left(\begin{matrix} I_k \\ \hline \bm{A}  \end{matrix} \right)&lt;/script&gt;. In other words, any linear code can be seen as a message supplemented with parity checks!&lt;/li&gt;
  &lt;li&gt;The codewords of a code described by &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}&lt;/script&gt; can be found by taking all the linear combinations of the columns of &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}&lt;/script&gt; (the vector &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{x}&lt;/script&gt; in the definition indicates which columns you select or not). Therefore, to find all the codewords, just calculate all the &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}&lt;/script&gt; of the form &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}=a_1 \bm{c_1} + ... a_k \bm{c_k}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}=a_1 \bm{c_1} + ... a_k \bm{c_k}&lt;/script&gt; where &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{c_i}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{c_i}&lt;/script&gt; is the &lt;code class=&quot;MathJax_Preview&quot;&gt;i^{\th}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i^{\th}&lt;/script&gt; column of &lt;code class=&quot;MathJax_Preview&quot;&gt;G&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;a_1,...a_k \in \{0,1\}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a_1,...a_k \in \{0,1\}&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An equivalent picture to describe linear codes is through the &lt;strong&gt;parity-check matrix&lt;/strong&gt;, defined as an &lt;code class=&quot;MathJax_Preview&quot;&gt;m \times n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m \times n&lt;/script&gt; matrix &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}&lt;/script&gt; such that&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \bm{H} \bm{y} = 0
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    \bm{H} \bm{y} = 0
\end{aligned}&lt;/script&gt;

&lt;p&gt;if and only if &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}&lt;/script&gt; is a codeword. In other words, the space of codewords can be defined as the kernel of &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}&lt;/script&gt;. The intuition is that linear codes can always be defined as a set of codewords obeying a certain system of linear equations, defined by the parity checks. For instance, the codewords of the Hamming code obey the following system:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    a + b + d + z_1 = 0 \\
    a + b + c + z_2 = 0 \\
    b + c + d + z_3 = 0
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    a + b + d + z_1 = 0 \\
    a + b + c + z_2 = 0 \\
    b + c + d + z_3 = 0
\end{aligned}&lt;/script&gt;

&lt;p&gt;(since &lt;code class=&quot;MathJax_Preview&quot;&gt;a+b+d=z_1 \Leftrightarrow a+b+d+z_1=0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a+b+d=z_1 \Leftrightarrow a+b+d+z_1=0&lt;/script&gt; when working modulo &lt;code class=&quot;MathJax_Preview&quot;&gt;2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2&lt;/script&gt;)&lt;/p&gt;

&lt;p&gt;Therefore, the parity-check matrix of the Hamming code can be written&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \bm{H} =
    \left(
        \begin{array}{cccc|ccc}
            1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\
            1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\
            0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\
        \end{array}
    \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    \bm{H} =
    \left(
        \begin{array}{cccc|ccc}
            1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\
            1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\
            0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\
        \end{array}
    \right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;For a generator matrix of the form &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}=\left(\begin{matrix} I_k \\ \hline \bm{A}  \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}=\left(\begin{matrix} I_k \\ \hline \bm{A}  \end{matrix} \right)&lt;/script&gt;, the corresponding parity-check matrix can be written&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \bm{H}=\left(\begin{matrix} \bm{A} &amp;amp; I_m  \end{matrix} \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    \bm{H}=\left(\begin{matrix} \bm{A} &amp; I_m  \end{matrix} \right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Indeed, if &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}&lt;/script&gt; is a codeword, it can be written &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}=\bm{G}\bm{x}=\left(\begin{matrix} \bm{x} \\ \hline \bm{A} \bm{x} \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}=\bm{G}\bm{x}=\left(\begin{matrix} \bm{x} \\ \hline \bm{A} \bm{x} \end{matrix} \right)&lt;/script&gt;. Applying &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}&lt;/script&gt;, we get&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \bm{H} \bm{y} = \bm{A}\bm{x} + \bm{A}\bm{x} = 2\bm{A}\bm{x} = \bm{0}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    \bm{H} \bm{y} = \bm{A}\bm{x} + \bm{A}\bm{x} = 2\bm{A}\bm{x} = \bm{0}
\end{aligned}&lt;/script&gt;

&lt;p&gt;since all operations are performed modulo 2, and &lt;code class=&quot;MathJax_Preview&quot;&gt;2=0 \mod 2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2=0 \mod 2&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Similarly to how all generator matrices can be chosen to have the form &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}=\left(\begin{matrix} I_k \\ \hline \bm{A}  \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}=\left(\begin{matrix} I_k \\ \hline \bm{A}  \end{matrix} \right)&lt;/script&gt;, we can always apply a Gaussian reduction on &lt;code class=&quot;MathJax_Preview&quot;&gt;H&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; such that is has the form &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}=\left(\begin{matrix} \bm{A} &amp;amp; I_m  \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\bm{H}=\left(\begin{matrix} \bm{A} &amp; I_m  \end{matrix} \right) %]]&gt;&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;At that point of the post, you might be wondering why we should use the parity-check matrix, when the generator matrix seems much more natural. There are many reasons to prefer the parity-check matrix over the generator matrix. The simplest one is that it gives a convenient method to detect and correct errors. Indeed, if &lt;code class=&quot;MathJax_Preview&quot;&gt;\widetilde{\bm{y}}=\bm{y} + \bm{e}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\widetilde{\bm{y}}=\bm{y} + \bm{e}&lt;/script&gt; is the received message disturbed by an error vector &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{e}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{e}&lt;/script&gt;, applying the parity-check matrix to &lt;code class=&quot;MathJax_Preview&quot;&gt;\widetilde{\bm{y}}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\widetilde{\bm{y}}&lt;/script&gt; gives&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \bm{H} \widetilde{\bm{y}} &amp;amp;= \bm{H} \left( \bm{y} + \bm{e} \right) \\
    &amp;amp;= \bm{H} \bm{e}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    \bm{H} \widetilde{\bm{y}} &amp;= \bm{H} \left( \bm{y} + \bm{e} \right) \\
    &amp;= \bm{H} \bm{e}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The new vector &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{s} = \bm{H} \bm{e}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{s} = \bm{H} \bm{e}&lt;/script&gt; has dimension &lt;code class=&quot;MathJax_Preview&quot;&gt;m&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; and is called the &lt;strong&gt;syndrome&lt;/strong&gt;. Each component &lt;code class=&quot;MathJax_Preview&quot;&gt;s_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; of the syndrome is equal to &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; if the parity-check equation &lt;code class=&quot;MathJax_Preview&quot;&gt;i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is violated. Decoding a message then consists in finding the most probable error &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{e}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{e}&lt;/script&gt; that has yielded to the syndrome &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{s}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{s}&lt;/script&gt;. Let’s illustrate this syndrome decoding technique with the &lt;code class=&quot;MathJax_Preview&quot;&gt;[7,4]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[7,4]&lt;/script&gt;-Hamming code. The components of the syndrome are given by the following system of equations:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    a + b + d + z_1 = s_1 \\
    a + b + c + z_2 = s_2 \\
    b + c + d + z_3 = s_3
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    a + b + d + z_1 = s_1 \\
    a + b + c + z_2 = s_2 \\
    b + c + d + z_3 = s_3
\end{aligned}&lt;/script&gt;

&lt;p&gt;When &lt;code class=&quot;MathJax_Preview&quot;&gt;s_i=1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;s_i=1&lt;/script&gt;, it therefore means that the parity-check bit &lt;code class=&quot;MathJax_Preview&quot;&gt;z_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_i&lt;/script&gt; is violated (it doesn’t correspond to the parity of its block anymore).
The following table shows the bit we choose to correct for each of the 8 possible syndromes (we can obtain it by looking at the Venn diagram of the Hamming code):&lt;/p&gt;

&lt;table class=&quot;stretch-table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Syndrome&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;000&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;001&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;010&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;011&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;100&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;101&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;110&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;111&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Correction&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;\emptyset&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\emptyset&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;z_1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_1&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;z_2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_2&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;c&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;z_3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_3&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;b&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;a&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;d&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A visual way to construct the parity-check matrix is through the &lt;strong&gt;Tanner graph&lt;/strong&gt; of the code. The Tanner graph is a bipartite graph containing two types of nodes, the data nodes (one for each bit of the codeword) and the check nodes (one for each bit of the syndrome). A check node &lt;code class=&quot;MathJax_Preview&quot;&gt;i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and a data node &lt;code class=&quot;MathJax_Preview&quot;&gt;j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; are connected if the syndrome bit &lt;code class=&quot;MathJax_Preview&quot;&gt;s_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; depends on &lt;code class=&quot;MathJax_Preview&quot;&gt;x_j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x_j&lt;/script&gt;. The figure below represents the Tanner graph of the Hamming code. The parity-check matrix is then simply the adjacency matrix of the Tanner graph, i.e. it has a &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; at row &lt;code class=&quot;MathJax_Preview&quot;&gt;i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and column &lt;code class=&quot;MathJax_Preview&quot;&gt;j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; if the check node &lt;code class=&quot;MathJax_Preview&quot;&gt;i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is connected to the data node &lt;code class=&quot;MathJax_Preview&quot;&gt;j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;, and &lt;code class=&quot;MathJax_Preview&quot;&gt;0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; otherwise.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/classical-error-correction/tanner-graph.png&quot; alt=&quot;&quot; /&gt;
Tanner graph of the Hamming code. The top nodes represent the codeword bits and the bottom nodes the syndrome bits, with an edge whenever a codeword bit is involved in the definition of a syndrome bit.&lt;/p&gt;

&lt;p&gt;In quantum error correction, the syndrome can be measured without disturbing the state (through the so-called stabilizer measurements), which makes the theory of linear codes easily transferable to the quantum domain. While quantum codewords can be complicated superpositions in the Hilbert space, errors are simple vectors of size &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; (the number of qubits), making the calculation of the syndrome &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{s}=\bm{H} \bm{e}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{s}=\bm{H} \bm{e}&lt;/script&gt; straightforward using the the parity-check matrix, and it is indeed used extensively in simulations.&lt;/p&gt;

&lt;p&gt;In the next section, we will study the problem of decoding linear codes in general, i.e. correcting the errors using the syndrome information. But before that, you can try to solve the following exercises to make sure you understand the basics of linear codes.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 4&lt;/strong&gt;: Find the generator and the parity-check matrix of the code defined by &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{E}(abc) = a b c z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{E}(abc) = a b c z&lt;/script&gt; where &lt;code class=&quot;MathJax_Preview&quot;&gt;z=a+b+c&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z=a+b+c&lt;/script&gt;, and draw its Tanner graph.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 5&lt;/strong&gt;: Find the generator and the parity-check matrix of the 3-repetition code, and draw its Tanner graph.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 6&lt;/strong&gt;: Show that the set of codewords is a vector space, i.e. if two vectors &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y_1}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y_1}&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y_2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y_2}&lt;/script&gt; are codewords, &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y_1} + \bm{y_2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y_1} + \bm{y_2}&lt;/script&gt; is also a codeword. This property is sometimes taken as the definition of linear codes.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 7&lt;/strong&gt;: &lt;strong&gt;(a)&lt;/strong&gt; Show that if &lt;code class=&quot;MathJax_Preview&quot;&gt;V&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; is a vector space over binary numbers, then &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert V\vert  = 2^{\dim(V)}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert V\vert  = 2^{\dim(V)}&lt;/script&gt;, where &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert V\vert&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert V\vert&lt;/script&gt; is the the number of elements in the vector space. &lt;code class=&quot;MathJax_Preview&quot;&gt;\newline&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\newline&lt;/script&gt;
&lt;strong&gt;(b)&lt;/strong&gt; Deduce that the parity-check matrix &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}&lt;/script&gt; of an &lt;code class=&quot;MathJax_Preview&quot;&gt;[n,k,d]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[n,k,d]&lt;/script&gt;-code obeys the relation &lt;code class=&quot;MathJax_Preview&quot;&gt;\text{rank}(H)=n-k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{rank}(H)=n-k&lt;/script&gt;.&lt;code class=&quot;MathJax_Preview&quot;&gt;\newline&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\newline&lt;/script&gt;
&lt;strong&gt;(c)&lt;/strong&gt; Deduce that if a code has &lt;code class=&quot;MathJax_Preview&quot;&gt;m&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; independent parity checks, we have &lt;code class=&quot;MathJax_Preview&quot;&gt;m=n-k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m=n-k&lt;/script&gt;. This relation is often used to find the parameter &lt;code class=&quot;MathJax_Preview&quot;&gt;k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; of a code given the parity checks.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 8&lt;/strong&gt;:
&lt;strong&gt;(a)&lt;/strong&gt; Show that the distance of a linear code is the minimum Hamming weight of all the non-zero codewords, i.e. &lt;code class=&quot;MathJax_Preview&quot;&gt;d=\min_{\bm{y}\in \mathcal{C}, \bm{y} \neq \bm{0}} \vert \bm{y} \vert&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d=\min_{\bm{y}\in \mathcal{C}, \bm{y} \neq \bm{0}} \vert \bm{y} \vert&lt;/script&gt;, where &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{C}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt; is the space of codewords and &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert\cdot\vert&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert\cdot\vert&lt;/script&gt; denotes the Hamming weight (&lt;em&gt;Hint: first prove that the Hamming distance between two codewords is translation-invariant, i.e. &lt;code class=&quot;MathJax_Preview&quot;&gt;D_H(\bm{x},\bm{y})=D_H(\bm{x}+\bm{z},\bm{y}+\bm{z})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;D_H(\bm{x},\bm{y})=D_H(\bm{x}+\bm{z},\bm{y}+\bm{z})&lt;/script&gt;&lt;/em&gt;)&lt;code class=&quot;MathJax_Preview&quot;&gt;\newline&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\newline&lt;/script&gt;
&lt;strong&gt;(b)&lt;/strong&gt; Use this property to compute the distance of the Hamming code.&lt;/p&gt;

&lt;h2 id=&quot;decoding-linear-codes&quot;&gt;Decoding linear codes&lt;/h2&gt;

&lt;p&gt;Designing a code with good characteristics, such as a high rate and a high distance, is not enough to make it practical: you need to show how to decode it efficiently. As discussed before, decoding consists in finding the original message given its noisy encoded version. We then define an &lt;strong&gt;efficient decoder&lt;/strong&gt; as an algorithm able to accomplish this task in polynomial time, i.e. with a time complexity that grows polynomially with the size &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; of the code.&lt;/p&gt;

&lt;p&gt;To see why decoding can be a difficult problem, let’s consider the general task of decoding a linear code, when errors follow a certain distribution &lt;code class=&quot;MathJax_Preview&quot;&gt;P(\bm{e})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\bm{e})&lt;/script&gt;. As we’ve seen in the previous section, decoding a linear code can be reduced to finding the most likely error given a received syndrome.
Let &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}&lt;/script&gt; denote the parity-check matrix of our code and &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{s}=\bm{H}\bm{e}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{s}=\bm{H}\bm{e}&lt;/script&gt; the received syndrome . The goal of an ideal decoder would be to find the vector &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{e}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{e}&lt;/script&gt; that maximizes the probability &lt;code class=&quot;MathJax_Preview&quot;&gt;P(\bm{e} \vert \bm{s})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\bm{e} \vert \bm{s})&lt;/script&gt;. Using Bayes rule, we can write this probability as:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    P(\bm{e} \vert \bm{s}) = \frac{P(\bm{s} \vert \bm{e}) P(\bm{e})}{P(\bm{s})}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    P(\bm{e} \vert \bm{s}) = \frac{P(\bm{s} \vert \bm{e}) P(\bm{e})}{P(\bm{s})}
\end{aligned}&lt;/script&gt;

&lt;p&gt;Since &lt;code class=&quot;MathJax_Preview&quot;&gt;P(\bm{s})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\bm{s})&lt;/script&gt; doesn’t depend explicitely on &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{e}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{e}&lt;/script&gt;, we can ignore it when solving the maximization problem over &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{e}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{e}&lt;/script&gt;. Next, we notice that any valid predicted error will have to obey &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}\bm{e}=\bm{s}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}\bm{e}=\bm{s}&lt;/script&gt;, so &lt;code class=&quot;MathJax_Preview&quot;&gt;P(\bm{s} \vert \bm{e})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\bm{s} \vert \bm{e})&lt;/script&gt; is either &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; or &lt;code class=&quot;MathJax_Preview&quot;&gt;0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;, depending on whether this equation is satisfied or not. In other words:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    P(\bm{s} \vert \bm{e}) = \bm{1}_{\{\bm{H}\bm{e}=\bm{s}\}}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    P(\bm{s} \vert \bm{e}) = \bm{1}_{\{\bm{H}\bm{e}=\bm{s}\}}
\end{aligned}&lt;/script&gt;

&lt;p&gt;Therefore, we can rewrite our optimization problem as:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \max_{\bm{e}\in \{0,1\}^n} P(\bm{e}) \; \text{ s.t. } \; \bm{H}\bm{e}=\bm{s}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    \max_{\bm{e}\in \{0,1\}^n} P(\bm{e}) \; \text{ s.t. } \; \bm{H}\bm{e}=\bm{s}
\end{aligned}&lt;/script&gt;

&lt;p&gt;An important special case is when errors are independent and identically distributed, such that&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    P(\bm{e}) = \prod_{i=1}^n P(e_i)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    P(\bm{e}) = \prod_{i=1}^n P(e_i)
\end{aligned}&lt;/script&gt;

&lt;p&gt;Writing &lt;code class=&quot;MathJax_Preview&quot;&gt;P(e_i=1)=p&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P(e_i=1)=p&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;P(e_i=0) = (1-p)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P(e_i=0) = (1-p)&lt;/script&gt;, and denoting &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \bm{e} \vert&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \bm{e} \vert&lt;/script&gt; the Hamming weight of &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{e}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{e}&lt;/script&gt; (i.e. the number of 1 in &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{e}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{e}&lt;/script&gt;), we can rewrite the equation above as&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    P(\bm{e}) = p^{\vert \bm{e} \vert} (1-p)^{n-\vert \bm{e} \vert}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    P(\bm{e}) = p^{\vert \bm{e} \vert} (1-p)^{n-\vert \bm{e} \vert}
\end{aligned}&lt;/script&gt;

&lt;p&gt;This expression only depends on the weight of &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{e}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{e}&lt;/script&gt;, and if the probability of error &lt;code class=&quot;MathJax_Preview&quot;&gt;p &amp;lt; 0.5&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
p &lt; 0.5 %]]&gt;&lt;/script&gt;, it increases when lowering the weight. In other words, our optimization problem reduces to finding the error of minimum weight that satisfies &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}\bm{e}=\bm{s}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}\bm{e}=\bm{s}&lt;/script&gt;:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \min_{\bm{e}\in \{0,1\}^n} \vert \bm{e} \vert \; \text{ s.t. } \; \bm{H}\bm{e}=\bm{s}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    \min_{\bm{e}\in \{0,1\}^n} \vert \bm{e} \vert \; \text{ s.t. } \; \bm{H}\bm{e}=\bm{s}
\end{aligned}&lt;/script&gt;

&lt;p&gt;Any decoder that explicitely solves this optimization problem is called a &lt;strong&gt;Maximum A Posteriori (MAP) decoder&lt;/strong&gt;, as we are maximizing the posterior distribution &lt;code class=&quot;MathJax_Preview&quot;&gt;P(\bm{e} \vert \bm{s})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\bm{e} \vert \bm{s})&lt;/script&gt;, and is considered to be an ideal decoder.&lt;/p&gt;

&lt;p&gt;So how do we solve the MAP decoding problem? A naive idea would be to simply search through all error vectors and find one that has minimum weight and obeys the constraint. Since there are &lt;code class=&quot;MathJax_Preview&quot;&gt;2^n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2^n&lt;/script&gt; possible error vectors, the time complexity of this algorithm would scale exponentially with &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; and definitely not be efficient. So can we do better? Unfortunately, the answer is no in general: this constrained optimization problem can be shown to be NP-complete&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, meaning that, most likely, no polynomial-time algorithm will ever solve it.&lt;/p&gt;

&lt;p&gt;What we do from there really depends on the code that is being decoded. Some parity-check matrices have a particular structure that allows the construction of polynomial-time algorithms that solve the MAP decoding problem. It’s for instance the case with Hamming codes and repetition codes. More generally, certain heuristics can be used as approximations to MAP decoding, and lead to high performance in practice. The main example of high-performance heuristic is the &lt;strong&gt;belief propagation algorithm&lt;/strong&gt;, a linear-time iterative algorithm that exploits the fact that the probability &lt;code class=&quot;MathJax_Preview&quot;&gt;P(\bm{e} \vert \bm{s})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\bm{e} \vert \bm{s})&lt;/script&gt; can often be factorized over a graph (in our case the Tanner graph)&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. This algorithm is used extensively by the classical error-correction community, and has recently started to become popular in the quantum community as well, so I will try to dedicate a blog post to it.&lt;/p&gt;

&lt;p&gt;Before ending this post, there is one last important notion I want you to know, as it is frequently used in quantum error correction: code duality.&lt;/p&gt;

&lt;h2 id=&quot;code-duality&quot;&gt;Code duality&lt;/h2&gt;

&lt;p&gt;Let’s consider a code &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{C}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt; with generator matrix &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}&lt;/script&gt; and parity-check matrix &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}&lt;/script&gt;. We define the &lt;strong&gt;dual code&lt;/strong&gt; &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{C}^\perp&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}^\perp&lt;/script&gt; as the code with generator matrix &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}^\perp=\bm{H}^T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}^\perp=\bm{H}^T&lt;/script&gt; and parity-check matrix &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}^\perp=\bm{G}^T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}^\perp=\bm{G}^T&lt;/script&gt;. It means that the codewords in &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{C}^\perp&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}^\perp&lt;/script&gt; now span the rows of &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}&lt;/script&gt;, and are orthogonal to all the codewords of &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{C}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt;. Duality is an extremely useful notion in coding theory, as it allows to construct new codes from known ones. It is also widely used in quantum error correction, where many constructions make use of the dual code. Let’s try to understand this notion more precisely by looking at some examples.&lt;/p&gt;

&lt;p&gt;First, what is the dual of the 3-repetition code? If you’ve attempted Exercise 5, you know that the repetition code is associated to the following matrices:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \bm{G} =
    \left(
        \begin{matrix}
            1 \\
            1 \\
            1 \\
        \end{matrix}
    \right)
    , \; \;
    \bm{H} =
    \left(
        \begin{matrix}
            1 &amp;amp; 1 &amp;amp; 0 \\
            1 &amp;amp; 0 &amp;amp; 1
        \end{matrix}
    \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    \bm{G} =
    \left(
        \begin{matrix}
            1 \\
            1 \\
            1 \\
        \end{matrix}
    \right)
    , \; \;
    \bm{H} =
    \left(
        \begin{matrix}
            1 &amp; 1 &amp; 0 \\
            1 &amp; 0 &amp; 1
        \end{matrix}
    \right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore, the dual of the 3-repetition code corresponds to&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \bm{G}^\perp =
    \left(
        \begin{matrix}
            1 &amp;amp; 1 \\
            1 &amp;amp; 0 \\
            0 &amp;amp; 1
        \end{matrix}
    \right)
    , \; \;
    \bm{H}^\perp =
    \left(
        \begin{matrix}
            1 &amp;amp; 1 &amp;amp; 1\\
        \end{matrix}
    \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    \bm{G}^\perp =
    \left(
        \begin{matrix}
            1 &amp; 1 \\
            1 &amp; 0 \\
            0 &amp; 1
        \end{matrix}
    \right)
    , \; \;
    \bm{H}^\perp =
    \left(
        \begin{matrix}
            1 &amp; 1 &amp; 1\\
        \end{matrix}
    \right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;From this new parity-check matrix, we can deduce that the codewords of &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{C}^\perp&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}^\perp&lt;/script&gt; are all the vectors
&lt;code class=&quot;MathJax_Preview&quot;&gt;\left(
    \begin{matrix}
        a \\
        b \\
        c
    \end{matrix}
\right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\left(
    \begin{matrix}
        a \\
        b \\
        c
    \end{matrix}
\right)&lt;/script&gt;
such that
&lt;code class=&quot;MathJax_Preview&quot;&gt;\left(
    \begin{matrix}
        1 &amp;amp; 1 &amp;amp; 1
    \end{matrix}
\right)
\left(
    \begin{matrix}
        a \\
        b \\
        c
    \end{matrix}
\right)
= a+b+c=0 = 0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\left(
    \begin{matrix}
        1 &amp; 1 &amp; 1
    \end{matrix}
\right)
\left(
    \begin{matrix}
        a \\
        b \\
        c
    \end{matrix}
\right)
= a+b+c=0 = 0 %]]&gt;&lt;/script&gt;. In other words, the first two bits can be seen as a message and the last bit as a parity check. That’s the &lt;code class=&quot;MathJax_Preview&quot;&gt;3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt;-bit version of the simple parity-check code that we’ve studied in the second section and in Exercise 4! It has four codewords, that we can read from the span of the columns of &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}^\perp&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}^\perp&lt;/script&gt;:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \mathcal{C}^\perp = \left\{
        \left(
        \begin{matrix}
            0 \\
            0 \\
            0
        \end{matrix}
        \right),
        \left(
        \begin{matrix}
            1 \\
            1 \\
            0
        \end{matrix}
        \right),
        \left(
        \begin{matrix}
            1 \\
            0 \\
            1
        \end{matrix}
        \right),
        \left(
        \begin{matrix}
            0 \\
            1 \\
            1
        \end{matrix}
        \right)
    \right\}
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    \mathcal{C}^\perp = \left\{
        \left(
        \begin{matrix}
            0 \\
            0 \\
            0
        \end{matrix}
        \right),
        \left(
        \begin{matrix}
            1 \\
            1 \\
            0
        \end{matrix}
        \right),
        \left(
        \begin{matrix}
            1 \\
            0 \\
            1
        \end{matrix}
        \right),
        \left(
        \begin{matrix}
            0 \\
            1 \\
            1
        \end{matrix}
        \right)
    \right\}
\end{aligned}&lt;/script&gt;

&lt;p&gt;A second, even simpler example, is the 2-repetition code, whose matrices are
&lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}=
\left(
    \begin{matrix}
        1 \\
        1 \\
    \end{matrix}
\right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}=
\left(
    \begin{matrix}
        1 \\
        1 \\
    \end{matrix}
\right)&lt;/script&gt;
and
&lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}=
\left(
    \begin{matrix}
        1 &amp;amp; 1
    \end{matrix}
\right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\bm{H}=
\left(
    \begin{matrix}
        1 &amp; 1
    \end{matrix}
\right) %]]&gt;&lt;/script&gt;.
Indeed, this code is the simplest example of a &lt;strong&gt;self-dual code&lt;/strong&gt;: its dual is equal to the original code. A more interesting example of self-dual code is given in Exercise 9.&lt;/p&gt;

&lt;p&gt;Finally, what about our good old &lt;code class=&quot;MathJax_Preview&quot;&gt;[7,4]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[7,4]&lt;/script&gt;-Hamming code? The generator of its dual is given by&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \bm{G}^\perp =
    \left(
        \begin{matrix}
            1 &amp;amp; 1 &amp;amp; 0 \\
            1 &amp;amp; 0 &amp;amp; 1 \\
            0 &amp;amp; 1 &amp;amp; 1 \\
            1 &amp;amp; 1 &amp;amp; 1 \\
            1 &amp;amp; 0 &amp;amp; 0 \\
            0 &amp;amp; 1 &amp;amp; 0 \\
            0 &amp;amp; 0 &amp;amp; 1 \\
        \end{matrix}
    \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    \bm{G}^\perp =
    \left(
        \begin{matrix}
            1 &amp; 1 &amp; 0 \\
            1 &amp; 0 &amp; 1 \\
            0 &amp; 1 &amp; 1 \\
            1 &amp; 1 &amp; 1 \\
            1 &amp; 0 &amp; 0 \\
            0 &amp; 1 &amp; 0 \\
            0 &amp; 0 &amp; 1 \\
        \end{matrix}
    \right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;from which we can deduce the list of codewords (by calculating the span of the columns of &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}^\perp&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}^\perp&lt;/script&gt;):&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    \mathcal{C}^\perp =
    \left\{
        \left(
            \begin{matrix} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \end{matrix}
        \right),
        \left(
            \begin{matrix} 1 \\ 1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \end{matrix}
        \right),
        \left(
            \begin{matrix} 1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 1 \\ 0 \end{matrix}
        \right),
        \left(
            \begin{matrix} 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 1 \end{matrix}
        \right),
        \left(
            \begin{matrix} 0 \\ 1 \\ 1 \\ 0 \\ 1 \\ 1 \\ 0 \end{matrix}
        \right),
        \left(
            \begin{matrix} 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \end{matrix}
        \right),
        \left(
            \begin{matrix} 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1 \end{matrix}
        \right),
        \left(
            \begin{matrix} 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 1 \end{matrix}
        \right),
    \right\}

\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
    \mathcal{C}^\perp =
    \left\{
        \left(
            \begin{matrix} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \end{matrix}
        \right),
        \left(
            \begin{matrix} 1 \\ 1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \end{matrix}
        \right),
        \left(
            \begin{matrix} 1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 1 \\ 0 \end{matrix}
        \right),
        \left(
            \begin{matrix} 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 1 \end{matrix}
        \right),
        \left(
            \begin{matrix} 0 \\ 1 \\ 1 \\ 0 \\ 1 \\ 1 \\ 0 \end{matrix}
        \right),
        \left(
            \begin{matrix} 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \end{matrix}
        \right),
        \left(
            \begin{matrix} 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1 \end{matrix}
        \right),
        \left(
            \begin{matrix} 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 1 \end{matrix}
        \right),
    \right\}

\end{aligned}&lt;/script&gt;

&lt;p&gt;An interesting fact about those 8 codewords is that they all belong to the 16 codewords of the Hamming codes! You can check that by showing either that the three last components of each codewords correspond to the parity checks &lt;code class=&quot;MathJax_Preview&quot;&gt;z_1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_1&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;z_2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_2&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;z_3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_3&lt;/script&gt; of the Hamming code, or that all the vectors are orthogonal to each others (meaning that that &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}\bm{y}=\bm{0}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}\bm{y}=\bm{0}&lt;/script&gt; for all of them). For this reason, we say the the &lt;code class=&quot;MathJax_Preview&quot;&gt;[7,4]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[7,4]&lt;/script&gt;-Hamming code is &lt;strong&gt;self-orthogonal&lt;/strong&gt;, meaning that its dual is included in the original code. Are self-orthogonal codes interesting, given that they just seem to be diminished version of known codes? It happens that they’re interesting if the included codewords all have a large Hamming weight, meaning that the distance will be higher than the original code (as shown in Exercise 8)! In our case, all the codewords have weight 4. The distance of the dual code is therefore 4, instead of 3 for the &lt;code class=&quot;MathJax_Preview&quot;&gt;[7,4]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[7,4]&lt;/script&gt;-Hamming code.&lt;/p&gt;

&lt;p&gt;The resulting code is therefore a &lt;code class=&quot;MathJax_Preview&quot;&gt;[7,3,4]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[7,3,4]&lt;/script&gt;-code, called a &lt;strong&gt;simplex code&lt;/strong&gt;. The general family of simplex codes, defined as dual of Hamming codes, are &lt;code class=&quot;MathJax_Preview&quot;&gt;[2^r-1, r, 2^{r-1}]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[2^r-1, r, 2^{r-1}]&lt;/script&gt;-codes, meaning that they have a very large distance but a very low rate. That’s one of the advantages of the duality construction, it allows us to find new codes with different properties!&lt;/p&gt;

&lt;p&gt;Let’s now take a step back and see what we can say about the general characteristics of dual codes:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The codeword length is the same for a code and its dual: &lt;code class=&quot;MathJax_Preview&quot;&gt;n^\perp=n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n^\perp=n&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;The message length is given by &lt;code class=&quot;MathJax_Preview&quot;&gt;k^\perp=n-k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k^\perp=n-k&lt;/script&gt;. Indeed, taking &lt;code class=&quot;MathJax_Preview&quot;&gt;H&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; to be full-rank and of dimension &lt;code class=&quot;MathJax_Preview&quot;&gt;m \times n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m \times n&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;G^\perp=H^T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;G^\perp=H^T&lt;/script&gt; is also full rank and has dimension &lt;code class=&quot;MathJax_Preview&quot;&gt;n \times m&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n \times m&lt;/script&gt;. By Exercise 7, &lt;code class=&quot;MathJax_Preview&quot;&gt;m=n-k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m=n-k&lt;/script&gt; (since H is full-rank), and &lt;code class=&quot;MathJax_Preview&quot;&gt;G^\perp&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;G^\perp&lt;/script&gt; therefore has &lt;code class=&quot;MathJax_Preview&quot;&gt;n-k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n-k&lt;/script&gt; (independent) columns, meaning that it has &lt;code class=&quot;MathJax_Preview&quot;&gt;n-k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n-k&lt;/script&gt; message bits.&lt;/li&gt;
  &lt;li&gt;As a consequence of 2, self-dual codes have rate &lt;code class=&quot;MathJax_Preview&quot;&gt;R=\frac{1}{2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;R=\frac{1}{2}&lt;/script&gt;, since &lt;code class=&quot;MathJax_Preview&quot;&gt;k=n-k \Rightarrow k=\frac{n}{2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k=n-k \Rightarrow k=\frac{n}{2}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;We have the following inequality on distances: &lt;code class=&quot;MathJax_Preview&quot;&gt;d+d^\perp -2 \leq n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d+d^\perp -2 \leq n&lt;/script&gt; (you can show it by adding together the Singleton bounds for a code and its dual). It can be interpreted as a tradeoff between the two distances: increasing the distance of a code often results in a decrease of its dual distance.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hence, the dual construction can often allow us to find codes with opposite characteristics, just like the Hamming and the simplex codes. Moreover, showing that a code is self-dual, or simply deriving the dual of a code, is often a powerful tool in coding theory to prove certain theorems.&lt;/p&gt;

&lt;p&gt;Before concluding this post, here is one last exercise to consolidate your knowledge of duality.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 9&lt;/strong&gt;: We can extend the Hamming code by adding a last parity-check bit, that checks the total parity of the message, i.e. &lt;code class=&quot;MathJax_Preview&quot;&gt;z_4=a+b+c+d&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_4=a+b+c+d&lt;/script&gt;. We call this code the &lt;strong&gt;extended Hamming code&lt;/strong&gt;. &lt;code class=&quot;MathJax_Preview&quot;&gt;\newline&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\newline&lt;/script&gt;
&lt;strong&gt;(a)&lt;/strong&gt; Write the parity-check matrix of the extended Hamming code. &lt;code class=&quot;MathJax_Preview&quot;&gt;\newline&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\newline&lt;/script&gt;
&lt;strong&gt;(b)&lt;/strong&gt; Show that the extended Hamming code is a &lt;code class=&quot;MathJax_Preview&quot;&gt;[8,4,4]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[8,4,4]&lt;/script&gt;-code. It means that it can now detect (but not correct) all the &lt;code class=&quot;MathJax_Preview&quot;&gt;2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2&lt;/script&gt;-bit errors.&lt;code class=&quot;MathJax_Preview&quot;&gt;\newline&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\newline&lt;/script&gt;
&lt;strong&gt;(c)&lt;/strong&gt; Show that this code is self-dual. It is the smallest non-trivial self-dual code, after the &lt;code class=&quot;MathJax_Preview&quot;&gt;2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2&lt;/script&gt;-repetition code introduced earlier.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So what have we learned in this post? We have defined error-correcting codes and shown that they are characterized by three important parameters: the number of bits &lt;code class=&quot;MathJax_Preview&quot;&gt;k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; of the message we want to send, the number of bits &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; of the encoding, and the distance &lt;code class=&quot;MathJax_Preview&quot;&gt;d&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; of the code. We have introduced linear codes, an important family of codes characterized by a generator and a parity-check matrix. We have seen that decoding can be performed by applying the parity-check matrix to the received message (getting what we called the syndrome), but that finding efficient algorithms to solve this problem can be challenging in general. Finally, we have shown that new codes can be obtained from known ones using duality.&lt;/p&gt;

&lt;p&gt;Now is time to confess that I have lied in the title: there is so much more to know about classical error correction, we’ve only barely scratched the surface!
Soon after Richard Hamming invented linear codes in the early 1950s, David Muller, Irving Reed and Gustave Solomon discovered a more algebraic way to come up with new linear codes, based on generator polynomials instead of generator matrices. This framework led to the invention of the most important codes of the 20th century, such as Reed-Muller codes, convolutional codes, Turbo codes, etc. More recently, LDPC codes, which use graph theory methods to obtain Tanner graphs with good properties, have gained popularity due to improvements of belief propagation decoders, and have for instance been used in the 5G protocol. Apart from coming up with new codes, classical error correction is also concerned with establishing bonds on code performance, often using information theory and probabilities. And decoding is a central notion in the field that we have only barely touch.&lt;/p&gt;

&lt;p&gt;However, what we have learned in this post will allow us to start quantum error correction from solid foundations. In the next post, we will introduce the main framework of quantum error correction: the stabilizer formalism. We will see how stabilizers are a direct generalization of parity checks when we have both &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; Pauli errors instead of just bit-flips. We will introduce the Shor code, a generalization of the repetition code, and the Steane code, a generalization of the &lt;code class=&quot;MathJax_Preview&quot;&gt;[7,4]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[7,4]&lt;/script&gt;-Hamming code. We will see how we can write a quantum version of the parity-check matrix, and how decoding works in this context. Moving forward in our quantum error correction journey, new classical error correction techniques will be needed, and we will introduce them in due time. But for the moment, you should have all you need to start quantum error correction on good feet!&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;Popular science videos to build some intuition:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=X8jsijhllIA&quot;&gt;How to send a self-correcting message&lt;/a&gt;, by 3Blue1Brown&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=-15nx57tbfc&amp;amp;list=PLp_s0welk1_cQkK6GxYsfE_SFQTRjXuB&quot;&gt;Error correcting codes&lt;/a&gt;, by Computerphile&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lecture series to actually delve into the subject:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtube.com/watch?v=vfjN7MmSB6g&amp;amp;list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr&quot;&gt;Algebraic coding theory&lt;/a&gt;, by Mary Wootters&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=eixCGqdlGxQ&amp;amp;list=PLJHszsWbB6hqkOyFCQOAlQtfzC1G9sf2&quot;&gt;Error Correcting codes&lt;/a&gt;, by Eigenchris&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Textbook from which I learned most of the content of this post:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.inference.org.uk/itprnn/book.pdf&quot;&gt;Information Theory, Inference, and Learning Algorithms&lt;/a&gt;, by David McKay&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Acknowledgment&lt;/strong&gt;: Big thanks to George Umbrarescu and Avinash Mocherla for their feedback on this blog post!&lt;/p&gt;

&lt;h2 id=&quot;solution-to-the-exercises&quot;&gt;Solution to the exercises&lt;/h2&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 1&lt;/strong&gt;: The encoding function of the &lt;code class=&quot;MathJax_Preview&quot;&gt;\ell&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\ell&lt;/script&gt;-repetition code is given by &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{E}(x) = x...x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{E}(x) = x...x&lt;/script&gt; where &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is repeated &lt;code class=&quot;MathJax_Preview&quot;&gt;\ell&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\ell&lt;/script&gt; times.
There are two codewords, made of all &lt;code class=&quot;MathJax_Preview&quot;&gt;0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; or all &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;: &lt;code class=&quot;MathJax_Preview&quot;&gt;0...0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;0...0&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;1...1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1...1&lt;/script&gt;. It is an &lt;code class=&quot;MathJax_Preview&quot;&gt;[\ell,1,\ell]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[\ell,1,\ell]&lt;/script&gt;-code, since we are encoding &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; bit into &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; bits, and the distance (number of bit-flips to go from one codeword to another) is &lt;code class=&quot;MathJax_Preview&quot;&gt;n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;. It means that &lt;code class=&quot;MathJax_Preview&quot;&gt;\ell-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\ell-1&lt;/script&gt; errors would be detectable, and &lt;code class=&quot;MathJax_Preview&quot;&gt;(\ell-1)/2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;(\ell-1)/2&lt;/script&gt; would be correctable (using majority vote). The rate of the code is &lt;code class=&quot;MathJax_Preview&quot;&gt;R=\frac{k}{n}=\frac{1}{\ell}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;R=\frac{k}{n}=\frac{1}{\ell}&lt;/script&gt;, which asymptotically goes to &lt;code class=&quot;MathJax_Preview&quot;&gt;0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; when &lt;code class=&quot;MathJax_Preview&quot;&gt;\ell\rightarrow \infty&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\ell\rightarrow \infty&lt;/script&gt;, making those codes impractical as they require a high number of redundant bits to encode a single bit.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 2&lt;/strong&gt;: The distance &lt;code class=&quot;MathJax_Preview&quot;&gt;d&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; is the minimum number of errors that can make the message switch from one codeword to another. Therefore, any error of weight &lt;code class=&quot;MathJax_Preview&quot;&gt;d-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d-1&lt;/script&gt; or lower will make the message leave the space of codewords, and will therefore be detectable. To correct an error, we need to find the closest codeword to the received message (in terms of Hamming distance). If the error weight is lower than &lt;code class=&quot;MathJax_Preview&quot;&gt;\frac{d-1}{2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{d-1}{2}&lt;/script&gt;, we will be &lt;code class=&quot;MathJax_Preview&quot;&gt;\frac{d-1}{2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{d-1}{2}&lt;/script&gt; errors apart from the correct codeword, but &lt;code class=&quot;MathJax_Preview&quot;&gt;\frac{d+1}{2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{d+1}{2}&lt;/script&gt; errors apart from the next closest codeword. We will therefore output the correct codeword. On the other hand, if the errors has weight &lt;code class=&quot;MathJax_Preview&quot;&gt;d/2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d/2&lt;/script&gt; or larger, we might be closer to another codeword and output the wrong correction.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 3&lt;/strong&gt;: By definition, we’re encoding a message of length &lt;code class=&quot;MathJax_Preview&quot;&gt;k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; using &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \mathcal{C} \vert&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \mathcal{C} \vert&lt;/script&gt; codewords. Since there are &lt;code class=&quot;MathJax_Preview&quot;&gt;2^k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2^k&lt;/script&gt; binary messages of lengths of &lt;code class=&quot;MathJax_Preview&quot;&gt;k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, we need &lt;code class=&quot;MathJax_Preview&quot;&gt;2^k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2^k&lt;/script&gt; codewords to encode them all.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 4&lt;/strong&gt;: Using the form &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}=\left(\begin{matrix} I_k \\ \hline \bm{A}  \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G}=\left(\begin{matrix} I_k \\ \hline \bm{A}  \end{matrix} \right)&lt;/script&gt; of the generator matrix, we find the following generator matrix for the simple parity-check code:
&lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G}=
\left(
    \begin{matrix}
    1 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; 1 &amp;amp; 0 \\
    0 &amp;amp; 0 &amp;amp; 1 \\\hline
    1 &amp;amp; 1 &amp;amp; 1
    \end{matrix}
\right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\bm{G}=
\left(
    \begin{matrix}
    1 &amp; 0 &amp; 0 \\
    0 &amp; 1 &amp; 0 \\
    0 &amp; 0 &amp; 1 \\\hline
    1 &amp; 1 &amp; 1
    \end{matrix}
\right) %]]&gt;&lt;/script&gt;. &lt;code class=&quot;MathJax_Preview&quot;&gt;\newline&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\newline&lt;/script&gt;
Similarly, using &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}=\left(\begin{matrix} \bm{A} &amp;amp; I_m \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\bm{H}=\left(\begin{matrix} \bm{A} &amp; I_m \end{matrix} \right) %]]&gt;&lt;/script&gt;, we find &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}=\left(\begin{matrix} 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\bm{H}=\left(\begin{matrix} 1 &amp; 1 &amp; 1 &amp; 1 \end{matrix} \right) %]]&gt;&lt;/script&gt;. The Tanner graph can be constructed using four message nodes, and one check node connected to all the message nodes.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 5&lt;/strong&gt;: The repetition code has two codewords, &lt;code class=&quot;MathJax_Preview&quot;&gt;\left( \begin{matrix} 0 \\ 0 \\ 0 \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\left( \begin{matrix} 0 \\ 0 \\ 0 \end{matrix} \right)&lt;/script&gt;
and &lt;code class=&quot;MathJax_Preview&quot;&gt;\left( \begin{matrix} 1 \\ 1 \\ 1 \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\left( \begin{matrix} 1 \\ 1 \\ 1 \end{matrix} \right)&lt;/script&gt;. It is therefore generated by &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{G} = \left( \begin{matrix} 1 \\ 1 \\ 1 \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{G} = \left( \begin{matrix} 1 \\ 1 \\ 1 \end{matrix} \right)&lt;/script&gt;. The first component can be interpreted as the identity matrix in dimension &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;, and we can define &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{A}=\left( \begin{matrix} 1 \\ 1 \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{A}=\left( \begin{matrix} 1 \\ 1 \end{matrix} \right)&lt;/script&gt;. From there, we get the parity-check matrix &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}= \left( \begin{matrix} 1 &amp;amp; 1 &amp;amp; 0 \\ 1 &amp;amp; 0 &amp;amp; 1 \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\bm{H}= \left( \begin{matrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{matrix} \right) %]]&gt;&lt;/script&gt;. It can be interpreted as checking that the parity of each pair of bits is even. The corresponding Tanner graph is drawn below.&lt;/p&gt;

&lt;p class=&quot;figure message&quot;&gt;&lt;img src=&quot;/assets/img/blog/classical-error-correction/repetition-code-tanner-graph.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 6&lt;/strong&gt;: We can show this using either the parity-check or the generator picture. In the generator picture, it comes down to showing that &lt;code class=&quot;MathJax_Preview&quot;&gt;\text{Im}(\bm{G})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{Im}(\bm{G})&lt;/script&gt; is a vector space. You might already know this fact from linear algebra, but if not, take two codewords &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y_1}, \bm{y_2} \in \text{Im}(\bm{G})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y_1}, \bm{y_2} \in \text{Im}(\bm{G})&lt;/script&gt;. By definition, they can be written as &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y_1}=\bm{G} \bm{x_1}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y_1}=\bm{G} \bm{x_1}&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y_2}=\bm{G} \bm{x_2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y_2}=\bm{G} \bm{x_2}&lt;/script&gt;. Therefore, &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y_1} + \bm{y_2} = \bm{G} \bm{x_1} + \bm{G} \bm{x_2} = \bm{G}(\bm{x_1} + \bm{x_2}) \in \text{Im}(\bm{G})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y_1} + \bm{y_2} = \bm{G} \bm{x_1} + \bm{G} \bm{x_2} = \bm{G}(\bm{x_1} + \bm{x_2}) \in \text{Im}(\bm{G})&lt;/script&gt;, and &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y_1} + \bm{y_2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y_1} + \bm{y_2}&lt;/script&gt; is a codeword.
In the parity-check picture, it comes down to showing that &lt;code class=&quot;MathJax_Preview&quot;&gt;\text{Ker}(\bm{H})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{Ker}(\bm{H})&lt;/script&gt; is a vector space, which you might also already know from linear algebra. It can be shown by taking two codewords &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y_1}, \bm{y_2} \in \text{Ker}(\bm{H})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y_1}, \bm{y_2} \in \text{Ker}(\bm{H})&lt;/script&gt;. Then &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H} (\bm{y_1} + \bm{y_2}) = \bm{H} \bm{y_1} + \bm{H} \bm{y_2} = \bm{0} + \bm{0} = \bm{0}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H} (\bm{y_1} + \bm{y_2}) = \bm{H} \bm{y_1} + \bm{H} \bm{y_2} = \bm{0} + \bm{0} = \bm{0}&lt;/script&gt;, so &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y_1}+\bm{y_2} \in \text{Ker}(\bm{H})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y_1}+\bm{y_2} \in \text{Ker}(\bm{H})&lt;/script&gt;.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 7&lt;/strong&gt;:
&lt;strong&gt;(a)&lt;/strong&gt; If &lt;code class=&quot;MathJax_Preview&quot;&gt;k=\dim V&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k=\dim V&lt;/script&gt;, we can find a basis &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}_1,...,\bm{y}_k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}_1,...,\bm{y}_k&lt;/script&gt; of &lt;code class=&quot;MathJax_Preview&quot;&gt;V&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; s.t. any element &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y} \in V&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y} \in V&lt;/script&gt; can be written &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}=a_1 \bm{y}_1 + ... + a_k \bm{y}_k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}=a_1 \bm{y}_1 + ... + a_k \bm{y}_k&lt;/script&gt; with &lt;code class=&quot;MathJax_Preview&quot;&gt;a_1,...,a_k \in \{0,1\}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a_1,...,a_k \in \{0,1\}&lt;/script&gt;. Since there are &lt;code class=&quot;MathJax_Preview&quot;&gt;2^k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2^k&lt;/script&gt; possible values of &lt;code class=&quot;MathJax_Preview&quot;&gt;a_1,...,a_k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a_1,...,a_k&lt;/script&gt;, we can deduce that there are &lt;code class=&quot;MathJax_Preview&quot;&gt;2^k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2^k&lt;/script&gt; elements in &lt;code class=&quot;MathJax_Preview&quot;&gt;V&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;. It can also be seen as a consequence of Exercise 3, where we showed that for any code, the number of codewords is &lt;code class=&quot;MathJax_Preview&quot;&gt;2^k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;2^k&lt;/script&gt;. &lt;code class=&quot;MathJax_Preview&quot;&gt;\newline&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\newline&lt;/script&gt;
&lt;strong&gt;(b)&lt;/strong&gt; From the &lt;a href=&quot;https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem&quot;&gt;rank-nullity theorem&lt;/a&gt;, we know that &lt;code class=&quot;MathJax_Preview&quot;&gt;\text{rank}(H) + \text{dim}(\text{Ker}(H)) = n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{rank}(H) + \text{dim}(\text{Ker}(H)) = n&lt;/script&gt;. Using (a), we get &lt;code class=&quot;MathJax_Preview&quot;&gt;\dim(\text{Ker}(H))=\log \vert \text{Ker}(H) \vert&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\dim(\text{Ker}(H))=\log \vert \text{Ker}(H) \vert&lt;/script&gt;. But we know from Exercise 3 that &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert\text{Ker}(H)\vert=2^k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert\text{Ker}(H)\vert=2^k&lt;/script&gt;, from which we can deduce that &lt;code class=&quot;MathJax_Preview&quot;&gt;\dim(\text{Ker}(H))=k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\dim(\text{Ker}(H))=k&lt;/script&gt;. Therefore, &lt;code class=&quot;MathJax_Preview&quot;&gt;\text{rank}(H) = n - k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{rank}(H) = n - k&lt;/script&gt;.&lt;code class=&quot;MathJax_Preview&quot;&gt;\newline&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\newline&lt;/script&gt;
&lt;strong&gt;(c)&lt;/strong&gt;: If there &lt;code class=&quot;MathJax_Preview&quot;&gt;m&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; independent parity checks, we can write &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}&lt;/script&gt; as a full-rank matrix where each row is one of the parity checks. Since &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{H}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{H}&lt;/script&gt; has dimension &lt;code class=&quot;MathJax_Preview&quot;&gt;m \times n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m \times n&lt;/script&gt;, and &lt;code class=&quot;MathJax_Preview&quot;&gt;m &amp;lt; n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
m &lt; n %]]&gt;&lt;/script&gt; (a fair assumption if we want &lt;code class=&quot;MathJax_Preview&quot;&gt;k &amp;gt; 0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k &gt; 0&lt;/script&gt;), we have &lt;code class=&quot;MathJax_Preview&quot;&gt;\text{rank}(\bm{H})=m&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{rank}(\bm{H})=m&lt;/script&gt;, and by the previous question, &lt;code class=&quot;MathJax_Preview&quot;&gt;m=n-k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m=n-k&lt;/script&gt;.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 8&lt;/strong&gt;: &lt;strong&gt;(a)&lt;/strong&gt; Let’s first show that the Hamming distance is translation-invariant: &lt;code class=&quot;MathJax_Preview&quot;&gt;D_H(\bm{x}+\bm{z}, \bm{y}+\bm{z})=\vert \bm{x} + \bm{z} - (\bm{y}+\bm{z})\vert=\vert \bm{x} - \bm{y} \vert=D_H(\bm{x},\bm{y})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;D_H(\bm{x}+\bm{z}, \bm{y}+\bm{z})=\vert \bm{x} + \bm{z} - (\bm{y}+\bm{z})\vert=\vert \bm{x} - \bm{y} \vert=D_H(\bm{x},\bm{y})&lt;/script&gt;. Therefore, it means that &lt;code class=&quot;MathJax_Preview&quot;&gt;D_H(\bm{x},\bm{y})=D_H(\bm{x}-\bm{x}, \bm{y}-\bm{x})=D_H(0,\bm{y}-\bm{x})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;D_H(\bm{x},\bm{y})=D_H(\bm{x}-\bm{x}, \bm{y}-\bm{x})=D_H(0,\bm{y}-\bm{x})&lt;/script&gt;. If &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{x}&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}&lt;/script&gt; are two codewords, &lt;code class=&quot;MathJax_Preview&quot;&gt;\bm{y}-\bm{x}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bm{y}-\bm{x}&lt;/script&gt; is also a codeword by linearity of our code (see Exercise 6 for a proof). Therefore, &lt;code class=&quot;MathJax_Preview&quot;&gt;d=\min_{\bm{x},\bm{y}\in \mathcal{C}, \bm{x} \neq \bm{y}} D_H(\bm{x},\bm{y})=\min_{\bm{x},\bm{y}\in \mathcal{C}, \bm{x} \neq \bm{y}} D_H(\bm{0},\bm{x}-\bm{y})=\min_{\bm{x}\in \mathcal{C}, \bm{x} \neq \bm{0}} D_H(\bm{0},\bm{x})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d=\min_{\bm{x},\bm{y}\in \mathcal{C}, \bm{x} \neq \bm{y}} D_H(\bm{x},\bm{y})=\min_{\bm{x},\bm{y}\in \mathcal{C}, \bm{x} \neq \bm{y}} D_H(\bm{0},\bm{x}-\bm{y})=\min_{\bm{x}\in \mathcal{C}, \bm{x} \neq \bm{0}} D_H(\bm{0},\bm{x})&lt;/script&gt;. &lt;code class=&quot;MathJax_Preview&quot;&gt;\newline&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\newline&lt;/script&gt;
&lt;strong&gt;(b)&lt;/strong&gt; Let’s show that the &lt;code class=&quot;MathJax_Preview&quot;&gt;[7,4]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;[7,4]&lt;/script&gt;-Hamming code has distance &lt;code class=&quot;MathJax_Preview&quot;&gt;3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt;. Any codeword of the Hamming code is of the form &lt;code class=&quot;MathJax_Preview&quot;&gt;(a, b, c, d, z_1, z_2, z_3)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;(a, b, c, d, z_1, z_2, z_3)&lt;/script&gt;. If &lt;code class=&quot;MathJax_Preview&quot;&gt;a=b=c=d=0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a=b=c=d=0&lt;/script&gt;, then we will have the zero codeword (which is not included in our optimization). If one of &lt;code class=&quot;MathJax_Preview&quot;&gt;a,b,c,d&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a,b,c,d&lt;/script&gt; is &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;, two of the parity checks will be &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;, giving a weight of &lt;code class=&quot;MathJax_Preview&quot;&gt;3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt; for those codewords. If two of &lt;code class=&quot;MathJax_Preview&quot;&gt;a,b,c,d&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a,b,c,d&lt;/script&gt; are &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;, then one parity-check will be &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;, giving also a weight of &lt;code class=&quot;MathJax_Preview&quot;&gt;3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt;. And if three or more variables are &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;, the weight will be at least three. Therefore, the minimum weight of a non-zero codeword is &lt;code class=&quot;MathJax_Preview&quot;&gt;3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt;, and we have &lt;code class=&quot;MathJax_Preview&quot;&gt;d=3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d=3&lt;/script&gt; by the previous question.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Exercise 9&lt;/strong&gt;: &lt;strong&gt;(a)&lt;/strong&gt; We obtain the parity-check matrix of the extended Hamming code by adding a new column (for our new parity-check bit) and appending the row &lt;code class=&quot;MathJax_Preview&quot;&gt;\left( \begin{matrix} 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \end{matrix} \right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\left( \begin{matrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \end{matrix} \right) %]]&gt;&lt;/script&gt;, giving
&lt;code class=&quot;MathJax_Preview&quot;&gt;\begin{aligned}
    \bm{H} =
    \left(
        \begin{matrix}
            1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
            1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
            0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\
            1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\
        \end{matrix}
    \right)
\end{aligned}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    \bm{H} =
    \left(
        \begin{matrix}
            1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
            1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
            0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
            1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
        \end{matrix}
    \right)
\end{aligned} %]]&gt;&lt;/script&gt; &lt;code class=&quot;MathJax_Preview&quot;&gt;\newline&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\newline&lt;/script&gt;
&lt;strong&gt;(b)&lt;/strong&gt; We now have &lt;code class=&quot;MathJax_Preview&quot;&gt;n=8&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;n=8&lt;/script&gt; since we have added a new check bit to the original Hamming code. The new parity-check matrix is full-rank, and therefore it has a number of columns &lt;code class=&quot;MathJax_Preview&quot;&gt;m=n-k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m=n-k&lt;/script&gt; (by Exercise 7). Since &lt;code class=&quot;MathJax_Preview&quot;&gt;m=4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;m=4&lt;/script&gt;, we can deduce that &lt;code class=&quot;MathJax_Preview&quot;&gt;k=4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k=4&lt;/script&gt;. To calculate the distance, we can find the lowest weight of its codewords (by Exercise 8a). A very similar reasoning to the proof of Exercise 8b can be used to prove that &lt;code class=&quot;MathJax_Preview&quot;&gt;d=4&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d=4&lt;/script&gt;. &lt;code class=&quot;MathJax_Preview&quot;&gt;\newline&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\newline&lt;/script&gt;
&lt;strong&gt;(c)&lt;/strong&gt; The non-identity part &lt;code class=&quot;MathJax_Preview&quot;&gt;A&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; of the parity-check matrix computed above is a symmetric matrix. Therefore, &lt;code class=&quot;MathJax_Preview&quot;&gt;A^T=A&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;A^T=A&lt;/script&gt; and the transpose of the parity-check matrix will be equal to the generator matrix, meaning that the code is self-dual.&lt;/p&gt;

&lt;!-- ---------------------------------------------------------------- --&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Berlekamp et al, &lt;a href=&quot;https://authors.library.caltech.edu/5607/1/BERieeetit78.pdf&quot;&gt;On the Inherent Intractability of Certain Coding Problems&lt;/a&gt;, 1978 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;See McKay, &lt;a href=&quot;https://www.inference.org.uk/itprnn/book.pdf&quot;&gt;Information Theory, Inference, and Learning Algorithms&lt;/a&gt;¸ for a great introduction to belief propagation algorithms for decoding &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Arthur Pesah</name><email>arthur.pesah@gmail.com</email></author><category term="blog" /><category term="quantum-computing" /><summary type="html">When learning about quantum error correction (QEC) for the first time, I tried to jump directly into the core of the subject, going from the stabilizer formalism to topological codes and decoders, but completely missing the classical origin of those notions. The reason is that many introductions to the subject do a great job presenting all those concepts in a self-contained way, without assuming any knowledge in error correction. So why bother learning classical error correction at all? Because if you dig deeper, you will find classical error correction concepts sprinkled all over QEC. Important classical notions such as parity checks, linear codes, Tanner graphs, belief propagation, low-density parity-check (LDPC) codes and many more, have natural generalizations in the quantum world and have been widely used in the development of QEC. Learning about classical error correction a few months into my QEC journey was completely illuminating: many ideas that I only understood formally suddenly made sense intuitively, and I was able to understand the content of many more papers. For this reason, I’d like this second article on quantum error correction to actually be about classical error correction. You will learn all you need to start off your QEC journey on the right foot!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arthurpesah.me/assets/img/blog/classical-error-correction/thumbnail.png" /><media:content medium="image" url="https://arthurpesah.me/assets/img/blog/classical-error-correction/thumbnail.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A bird’s-eye view of quantum error correction and fault tolerance</title><link href="https://arthurpesah.me/blog/2022-01-25-intro-qec-1/" rel="alternate" type="text/html" title="A bird’s-eye view of quantum error correction and fault tolerance" /><published>2022-01-25T00:00:00+01:00</published><updated>2022-01-25T00:00:00+01:00</updated><id>https://arthurpesah.me/blog/intro-qec-1</id><content type="html" xml:base="https://arthurpesah.me/blog/2022-01-25-intro-qec-1/">&lt;p&gt;This Summer marked the beginning of my thesis work, and with it, of my trip in the fascinating world of quantum error correction. I quickly found in this area the interdisciplinarity that I love: the field takes its roots in theoretical computer science (classical error correction), uses intuitions and techniques from theoretical physics (condensed matter, statistical physics, quantum field theory) and has deep connections to black hole research, statistical inference, algebraic topology and geometry, and many other areas of science and mathematics.&lt;/p&gt;

&lt;p&gt;As I am just starting my PhD adventure, I have taken the resolution to start blogging again. And since my first task as a new PhD student is to learn as much as I can about my field, why not sharing this learning journey with you?&lt;/p&gt;

&lt;p&gt;In this first article, we will dive together into the basics of quantum error correction (QEC). The goal is for a reader beginning in the field to get familiar with the big picture of fault-tolerance and error correction. No previous QEC background is required, but some familiarity with the basics of quantum computing (qubits, gates, measurements, etc.) is assumed.&lt;/p&gt;

&lt;h2 id=&quot;a-bit-of-history&quot;&gt;A bit of history&lt;/h2&gt;

&lt;p&gt;When the concept of a quantum computer was first proposed and formalized in the 1980s and 1990s, many physicists were skeptical that those devices would one day see the light of day. As an example, Serge Haroche and Jean-Michel Raimond—two eminent atomic physicists—famously said in a &lt;a href=&quot;https://physicstoday.scitation.org/doi/10.1063/1.881512&quot;&gt;1996 article in Physics Today&lt;/a&gt; that&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The large-scale quantum machine, though it may be the computer scientist’s dream, is the experimenter’s nightmare.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The reason for their skepticism was the inherent fragility of quantum states: any noise present in a quantum system, due for instance to unwanted interactions with the environment, can irreversibly modify your quantum state.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/intro-qec/qc-dream-nightmare.png&quot; alt=&quot;&quot; /&gt;
Article from &lt;a href=&quot;https://physicstoday.scitation.org/doi/10.1063/1.881512&quot;&gt;Physics Today&lt;/a&gt; by Serge Haroche and Jean-Michel Raimond, that expressed a general skepticism towards the idea of a large-scale quantum computer.&lt;/p&gt;

&lt;p&gt;Fortunately, the problem of computing with noise had been known for decades in the classical computing community. In the 1940s and 1950s, most computers were built from either mechanical relays or vacuum tubes, both of which were very prone to failure. It meant that random bits could flip in the middle of your calculations, giving you rubbish at the end. It’s exactly in this context that the Bell Labs mathematician Richard Hamming invented the first practical error-correcting code, out of frustration that his calculations on &lt;em&gt;Model 5&lt;/em&gt;—a relay computer only available to him on week-ends—were failing weeks after weeks&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Modern computers can still have errors from time to time (for instance &lt;a href=&quot;https://www.youtube.com/watch?v=AaZ_RSt0KP8&amp;amp;ab_channel=Veritasium&quot;&gt;due to cosmic rays&lt;/a&gt;!) and some critical systems therefore still require some sort of error correction. But more commonly, error-correcting codes are used everywhere in wireless communication, including satellite communication and the 4G/5G protocol.&lt;/p&gt;

&lt;p&gt;However, generalizing those ideas from classical to quantum bits was not an easy task. The invention of the first quantum error-correcting codes, and with them of the &lt;strong&gt;threshold theorem&lt;/strong&gt;, changed the game. This theorem states that there exist some families of quantum codes that can correct arbitrary errors by increasing the number of redundant qubits, as long as the noise level of the system is below a certain threshold. So, let’s say you found a code with a threshold of &lt;code class=&quot;MathJax_Preview&quot;&gt;1\%&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1\%&lt;/script&gt;. Then, if your noise is above &lt;code class=&quot;MathJax_Preview&quot;&gt;1\%&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1\%&lt;/script&gt;, adding more qubits to your code will yield more errors, while if it is below, adding more qubits will reduce the number of errors. 
The presence of a threshold below &lt;code class=&quot;MathJax_Preview&quot;&gt;50\%&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;50\%&lt;/script&gt; is a purely quantum phenomenon: classical codes are always improved by increasing the number of redundant bits. This important difference is due to the fact that there is only one type of classical errors, the bit-flips, while quantum errors can be caused by both bit-flips and phase-flips, as we will see in the next section.&lt;/p&gt;

&lt;p&gt;While the first quantum codes served as a useful proof of concept, their threshold was estimated to be around &lt;code class=&quot;MathJax_Preview&quot;&gt;10^{-6}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;10^{-6}&lt;/script&gt;, which was far below the experimental capabilities of the time &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. The introduction of the stabilizer formalism in 1998 revolutionized quantum error-correction and led to the invention of the surface code&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, which has a threshold over &lt;code class=&quot;MathJax_Preview&quot;&gt;1\%&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1\%&lt;/script&gt; in realistic settings&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. While subsequent years saw the development of many different quantum codes, the surface code has remained one of the most promising one, and I will probably dedicate several articles to it. But for now, let’s try to understand how quantum error correction works.&lt;/p&gt;

&lt;h2 id=&quot;what-is-quantum-error-correction&quot;&gt;What is quantum error correction?&lt;/h2&gt;

&lt;h3 id=&quot;modeling-quantum-noise&quot;&gt;Modeling quantum noise&lt;/h3&gt;

&lt;p&gt;Imagine that you are running a circuit on a real quantum computer. At each step of the circuit, you are expecting a certain state &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \psi \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \psi \rangle&lt;/script&gt; to come out of the device. However, if you are reading this in the 2020s, chances are that your device is noisy: instead of getting &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \psi\rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \psi\rangle&lt;/script&gt;, you will get a different state &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \widetilde{\psi} \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \widetilde{\psi} \rangle&lt;/script&gt;. This failure can be caused by multiple phenomena: unwanted interactions between qubits when applying a gate, unwanted interactions with the environment (causing decoherence), badly-controlled gates, errors in the measurement or state preparation process, etc. While the source of noise is diverse, it happens that a very general and simple noise model can be used to accurately model noise in a large range of situations: the &lt;strong&gt;Pauli error model&lt;/strong&gt;. It consists in assuming that a Pauli operator &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; or &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; is randomly applied after each gate or each clock cycle of the quantum computer. The generality of this model comes from the fact that any continuous error can be decomposed in the Pauli basis, and &lt;code class=&quot;MathJax_Preview&quot;&gt;Y=iXZ&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Y=iXZ&lt;/script&gt;. For instance, if your error is an unwanted rotation by an angle &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; in the Bloch sphere (let’s say in the &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; direction), we can write:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
R_x(\theta) = \cos\left(\frac{\theta}{2}\right) I - i \sin\left(\frac{\theta}{2}\right) X
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
R_x(\theta) = \cos\left(\frac{\theta}{2}\right) I - i \sin\left(\frac{\theta}{2}\right) X
\end{aligned}&lt;/script&gt;

&lt;p&gt;implying that&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
\vert \widetilde{\psi} \rangle = R_x(\theta) \vert \psi\rangle = \cos\left(\frac{\theta}{2}\right) \vert \psi\rangle - i \sin\left(\frac{\theta}{2}\right) X \vert \psi\rangle
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\vert \widetilde{\psi} \rangle = R_x(\theta) \vert \psi\rangle = \cos\left(\frac{\theta}{2}\right) \vert \psi\rangle - i \sin\left(\frac{\theta}{2}\right) X \vert \psi\rangle
\end{aligned}&lt;/script&gt;

&lt;p&gt;You will soon learn that quantum error correction is done by constantly applying non-destructive measurements to our state (the so-called stabilizer measurements), resulting in the above state being projected in either &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \psi\rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \psi\rangle&lt;/script&gt; with probability &lt;code class=&quot;MathJax_Preview&quot;&gt;\cos^2(\theta/2)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\cos^2(\theta/2)&lt;/script&gt;, or &lt;code class=&quot;MathJax_Preview&quot;&gt;X \vert \psi\rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X \vert \psi\rangle&lt;/script&gt; with probability &lt;code class=&quot;MathJax_Preview&quot;&gt;\sin^2(\theta/2)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\sin^2(\theta/2)&lt;/script&gt;. This process of decomposing any continuous error into &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; errors is called the &lt;strong&gt;digitization of errors&lt;/strong&gt;, and is key to quantum error correction, since analog errors, even on a classical computer, cannot easily be error-corrected.&lt;/p&gt;

&lt;p&gt;So we’ve reduced the infinite range of possible continuous errors to only two types of errors: &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt;. But what is the effect of those errors? Let’s consider a general single-qubit state, given by&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
\vert \psi \rangle = a \vert 0 \rangle + b \vert 1 \rangle
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\vert \psi \rangle = a \vert 0 \rangle + b \vert 1 \rangle
\end{aligned}&lt;/script&gt;

&lt;p&gt;Applying &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; to it results in the state&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
X \vert \psi \rangle = a \vert 1 \rangle + b \vert 0 \rangle
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
X \vert \psi \rangle = a \vert 1 \rangle + b \vert 0 \rangle
\end{aligned}&lt;/script&gt;

&lt;p&gt;In other words, we have applied a &lt;strong&gt;bit-flip&lt;/strong&gt; to our state. On the other hand, applying a &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; error&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
Z \vert \psi \rangle = a \vert 0 \rangle - b \vert 1 \rangle
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
Z \vert \psi \rangle = a \vert 0 \rangle - b \vert 1 \rangle
\end{aligned}&lt;/script&gt;

&lt;p&gt;results in a &lt;strong&gt;phase-flip&lt;/strong&gt;. That’s a first major difference between classical and quantum error-correction: instead of having just one type of error (bit-flips), we now have two types of error (bit-flips and phase-flips). This is a key fact that makes classical error correction techniques not directly applicable to the quantum setting. So, how can we correct quantum errors?&lt;/p&gt;

&lt;h3 id=&quot;encoding-and-decoding&quot;&gt;Encoding and decoding&lt;/h3&gt;

&lt;p&gt;The idea of quantum error correction is to &lt;strong&gt;encode&lt;/strong&gt; your state on a larger system, using redundant qubits. A simple example of encoding is the 3-repetition code, defined with the following dictionary:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
\vert 0 \rangle \longrightarrow \vert 000 \rangle_E \\
\vert 1 \rangle \longrightarrow \vert 111 \rangle_E
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\vert 0 \rangle \longrightarrow \vert 000 \rangle_E \\
\vert 1 \rangle \longrightarrow \vert 111 \rangle_E
\end{aligned}&lt;/script&gt;

&lt;p&gt;It means that a general qubit &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \psi \rangle = a \vert 0 \rangle + b \vert 1 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \psi \rangle = a \vert 0 \rangle + b \vert 1 \rangle&lt;/script&gt; will be encoded as &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \psi \rangle_E = a \vert 000 \rangle + b \vert 111 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \psi \rangle_E = a \vert 000 \rangle + b \vert 111 \rangle&lt;/script&gt;. We say that the code maps three &lt;strong&gt;physical qubits&lt;/strong&gt; into one &lt;strong&gt;logical qubit&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;When this encoded state goes through a noisy channel, errors can happen. But this time, we have some degree of protection. For instance, let’s say that a bit-flip occurred on the first qubit, leading to the state &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \widetilde{\psi} \rangle_E = a \vert 100 \rangle + b \vert 011 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \widetilde{\psi} \rangle_E = a \vert 100 \rangle + b \vert 011 \rangle&lt;/script&gt;. We can then detect and correct this error, in the so-called &lt;strong&gt;decoding&lt;/strong&gt; process: we perform measurements to the state and apply some correction operators depending on the measurement results. However, here comes another specificity of the quantum setting: we cannot simply measure the whole state (and then apply a majority vote or something), as it would result in a general collapse of the state. So we need to apply non-destructive measurements, also called &lt;strong&gt;stabilizer measurements&lt;/strong&gt;. An example of stabilizer measurement would be to measure the parity of each pair of qubits, i.e. whether the two qubits are equal or not. For instance, the following circuit can be used to measure the parity of qubits 1 and 2:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/intro-qec/stabilizer-measurement.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Indeed, if the first two qubits are in the state &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 00 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 00 \rangle&lt;/script&gt; or &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 11 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 11 \rangle&lt;/script&gt;, you can check that the measurement result in the &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; basis will be &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; (i.e. the ancilla qubit is in the &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 0 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 0 \rangle&lt;/script&gt; state), while if we are in either &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 01 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 01 \rangle&lt;/script&gt; or &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 10 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 10 \rangle&lt;/script&gt;, the measurement result will be &lt;code class=&quot;MathJax_Preview&quot;&gt;-1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt;. Note that by measuring the ancilla qubit, our state has not been destroyed.&lt;/p&gt;

&lt;p&gt;So, what happens if we apply those parity measurements to the state &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \widetilde{\psi} \rangle_E = a \vert 100 \rangle + b \vert 011 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \widetilde{\psi} \rangle_E = a \vert 100 \rangle + b \vert 011 \rangle&lt;/script&gt;? We will see that qubits 2 and 3 are equal, while qubit 1 is different from the two others. Assuming that we only had one &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; error, we can deduce that the only possibility is that the first qubit has been subjected to a bit-flip. Therefore, we apply a Pauli &lt;code class=&quot;MathJax_Preview&quot;&gt;\text{X}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{X}&lt;/script&gt; operator on the first qubit to recover our original state.&lt;/p&gt;

&lt;p&gt;The quantum error-correction process can be summarized by the following diagram:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
\vert \psi \rangle \xrightarrow{\text{encoding}} \vert \psi \rangle_E \xrightarrow{\text{noise}} \vert \widetilde{\psi} \rangle_E \xrightarrow{\text{decoding}} \vert \psi \rangle_E
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\vert \psi \rangle \xrightarrow{\text{encoding}} \vert \psi \rangle_E \xrightarrow{\text{noise}} \vert \widetilde{\psi} \rangle_E \xrightarrow{\text{decoding}} \vert \psi \rangle_E
\end{aligned}&lt;/script&gt;

&lt;p&gt;Note that our code has two major drawbacks: it cannot correctly decode more than one bit-flip errors, and cannot detect phase-flip errors at all. While the first problem could be alleviated by increasing the number of physical qubits, the second one is more fundamental and requires the design of more complex quantum codes. One of the most popular such code is the &lt;strong&gt;surface code&lt;/strong&gt; (also called &lt;strong&gt;toric code&lt;/strong&gt;), which belongs to the more general class of &lt;strong&gt;topological quantum codes&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;surface-code&quot;&gt;Surface code&lt;/h3&gt;

&lt;p&gt;Explaining exactly how the surface code work is a out-of-scope for this article, as it would require a proper introduction to the stabilizer formalism. However, given the importance this code has taken in the QEC world, I couldn’t resist giving you a big picture overview of its main characteristics, as a treat before I write a dedicated article on the topic! If you want to get a more in-depth explanation, feel free to look at the resources posted at the end of this article, such as &lt;a href=&quot;https://decodoku.medium.com/5-the-toric-code-part-1-caaa4b79afd8&quot;&gt;this series of blog post&lt;/a&gt; by James Wootton!&lt;/p&gt;

&lt;p&gt;So what is the surface code? It’s a code that encodes one logical qubit on a 2D grid of &lt;code class=&quot;MathJax_Preview&quot;&gt;L^2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;L^2&lt;/script&gt; physical qubits. For instance, the picture below represents one logical qubit of the surface code, encoded with 25 physical qubits (the white dots). The two colors represent the two types of measurements that are used to detect errors, made either of &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; operators (to detect phase-flips) or &lt;code class=&quot;MathJax_Preview&quot;&gt;Z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; operators (to detect bit-flips), but we won’t dive into those details now.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/intro-qec/surface-code.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To achieve noise levels that are low enough to run something like Shor’s algorithm might require grids containing between 1,000 and 10,000 physical qubits per logical qubit&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;! While it might sound like a huge overhead (and indeed, it is!), the surface code still has a few advantages, otherwise it wouldn’t be the most popular code in the market! First, contrary to the repetition code that we saw in the previous section, it can detect both bit-flips and phase-flips. Since doing so is one of the main difficulties in designing QEC codes and the reason why we cannot simply generalize classical codes, it is worth noting. A second advantage compared to other QEC codes is that it’s &lt;strong&gt;two-dimensional&lt;/strong&gt;. It means that in practice, we only need to arrange the qubits on a physical 2D chip with interaction between nearest neighbors, which makes the surface code particularly promising for superconducting chips like IBM’s and Google’s quantum computers. Finally, the threshold has been calculated to be around &lt;code class=&quot;MathJax_Preview&quot;&gt;\sim 1\%&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\sim 1\%&lt;/script&gt; (the exact number depends on the exact nature of the noise), which is one of the highest in the QEC zoo&lt;sup id=&quot;fnref:5:1&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;. As a reminder, the threshold is the noise level below which increasing the number of physical qubits (here the size of the grid) actually improves the number of errors that can be corrected. So to make the surface code work, it means that we just need our noise level to be below &lt;code class=&quot;MathJax_Preview&quot;&gt;1\%&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1\%&lt;/script&gt; (probably slightly lower in realistic settings), which is definitely within the range of what experimentalists can do!&lt;/p&gt;

&lt;p&gt;Note that many alternatives to the 2D surface code have been proposed, such as 3D and 4D surface codes, color codes, low-density parity-check codes (also known as LDPC codes), subsystem codes, etc. Choosing a code is often a trade-off between connectivity (e.g. LDPC codes often require long-range interaction, so are not easily implementable on 2D chips), ease of measurements (how many qubits are measured simultaneously to detect errors), performance with a given noise model, threshold, gate design, etc.&lt;/p&gt;

&lt;p&gt;Once the encoding and decoding procedures are established, there is still one last step before being able to do quantum computation error-free: designing logical gates that act on the code. It happens that this task is far from trivial in general, and a whole sub-field of QEC is dedicated to it. That’s the subject of &lt;strong&gt;fault-tolerant quantum computing&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;fault-tolerant-quantum-computing&quot;&gt;Fault-tolerant quantum computing&lt;/h2&gt;

&lt;h3 id=&quot;transversality-and-fault-tolerance&quot;&gt;Transversality and fault-tolerance&lt;/h3&gt;

&lt;p&gt;The main challenge in designing gates is to avoid the propagation of errors: if we’re not careful, multi-qubit gates can turn one-qubit errors into correlated multi-qubit ones. This is particularly disturbing when the gate couples several physical qubits representing a single logical one (what we call a &lt;strong&gt;block&lt;/strong&gt;). For instance, imagine you want to apply a logical Hadamard on the 3-repetition code considered earlier. This gate, that we can call &lt;code class=&quot;MathJax_Preview&quot;&gt;H_L&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H_L&lt;/script&gt;, should use three physical qubits to act on one logical qubit, i.e.:&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
H \vert 0 \rangle = \frac{1}{\sqrt{2}} \left( \vert 0 \rangle + \vert 1 \rangle \right) \; \xrightarrow{\text{encoding}} \; H_L \vert 000 \rangle = \frac{1}{\sqrt{2}} \left( \vert 000 \rangle + \vert 111 \rangle \right) \\
H \vert 1 \rangle = \frac{1}{\sqrt{2}} \left( \vert 0 \rangle - \vert 1 \rangle \right) \; \xrightarrow{\text{encoding}} \; H_L \vert 111 \rangle = \frac{1}{\sqrt{2}} \left( \vert 000 \rangle - \vert 111 \rangle \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
H \vert 0 \rangle = \frac{1}{\sqrt{2}} \left( \vert 0 \rangle + \vert 1 \rangle \right) \; \xrightarrow{\text{encoding}} \; H_L \vert 000 \rangle = \frac{1}{\sqrt{2}} \left( \vert 000 \rangle + \vert 111 \rangle \right) \\
H \vert 1 \rangle = \frac{1}{\sqrt{2}} \left( \vert 0 \rangle - \vert 1 \rangle \right) \; \xrightarrow{\text{encoding}} \; H_L \vert 111 \rangle = \frac{1}{\sqrt{2}} \left( \vert 000 \rangle - \vert 111 \rangle \right)
\end{aligned}&lt;/script&gt;

&lt;p&gt;To implement the unitary &lt;code class=&quot;MathJax_Preview&quot;&gt;H_L&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H_L&lt;/script&gt;, some entangling gates will be needed. For example, you can check that the following circuit gives the correct unitary on &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 000 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 000 \rangle&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert 111 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert 111 \rangle&lt;/script&gt;:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/intro-qec/logical-hadamard-small.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The issue here is the presence of physical CNOTs &lt;em&gt;within the block&lt;/em&gt;, as they can create correlated errors that won’t be easily correctable.&lt;/p&gt;

&lt;p&gt;On the other hand, if we wanted to implement a logical CNOT between two logical qubits &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \psi \rangle_1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \psi \rangle_1&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert \psi \rangle_2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert \psi \rangle_2&lt;/script&gt;, we would simply use the following circuit&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/intro-qec/cnot-small.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;which doesn’t contain any entangling gate within each block. Such gate is called &lt;strong&gt;transversal&lt;/strong&gt;, while the Hadamard gate we saw previously was &lt;strong&gt;non-transversal&lt;/strong&gt;. An encoded circuit composed only of transversal gates is &lt;strong&gt;fault-tolerant&lt;/strong&gt;, meaning that it doesn’t propagate errors further.&lt;/p&gt;

&lt;p&gt;So, do we know any code such that all the gates can be built transversally? Unfortunately, the answer is no, and we even know that such code cannot exist. That’s the object of a foundational theorem in the field, called the &lt;a href=&quot;https://arxiv.org/abs/0811.4262&quot;&gt;Eastin-Knill theorem&lt;/a&gt;, which states that for any QEC code, there is no universal gate set&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; made only of transversal gates. For instance, in the surface code, only &lt;strong&gt;Clifford gates&lt;/strong&gt; can be implemented transversally, i.e all the gates that can be constructed from &lt;code class=&quot;MathJax_Preview&quot;&gt;H&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;CNOT&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;CNOT&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;S&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; gates. It includes all Pauli gates, but not the &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; gate for example, which is required for universality.&lt;/p&gt;

&lt;h3 id=&quot;magic-state-distillation&quot;&gt;Magic state distillation&lt;/h3&gt;

&lt;p&gt;Many tricks have been proposed to “by-pass” this no-go theorem. One of the most common tricks is called &lt;strong&gt;magic state distillation&lt;/strong&gt;. To illustrate how it works, let’s say we want to implement a &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; gate fault-tolerantly. As a reminder, the &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; gate is a one-qubit gate defined by the matrix&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
    T = \left( \begin{matrix}
    1 &amp;amp; 0          \\
    0 &amp;amp; e^{i\pi/4} \\
    \end{matrix} \right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    T = \left( \begin{matrix}
    1 &amp; 0          \\
    0 &amp; e^{i\pi/4} \\
    \end{matrix} \right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and is one of the simplest examples of non-Clifford gate, meaning that it cannot be written out of just &lt;code class=&quot;MathJax_Preview&quot;&gt;H&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;S&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;CNOT&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;CNOT&lt;/script&gt;. On the other hand, appending it to the Clifford gate set leads to a universal gate set, i.e. any gate you can think of can be written out of just &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;H&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;CNOT&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;CNOT&lt;/script&gt; (we don’t need &lt;code class=&quot;MathJax_Preview&quot;&gt;S&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; anymore, since &lt;code class=&quot;MathJax_Preview&quot;&gt;S=T^2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;S=T^2&lt;/script&gt;). Since the surface code can already implement all Clifford gates fault-tolerantly, it means that we only need a procedure to implement &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; gates!&lt;/p&gt;

&lt;p&gt;The first idea of magic state distillation is to implement the state &lt;code class=&quot;MathJax_Preview&quot;&gt;T\vert+\rangle=\vert 0 \rangle + e^{i\pi/4} \vert 1 \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T\vert+\rangle=\vert 0 \rangle + e^{i\pi/4} \vert 1 \rangle&lt;/script&gt; non-fault-tolerantly on an ancilla qubit (meaning that errors can occur when creating the state) and to “inject” it in the qubits of our code, thereby creating a &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; gate on those qubits. This trick, called &lt;strong&gt;state injection&lt;/strong&gt; (or &lt;strong&gt;gate teleportation&lt;/strong&gt; in the measurement-based QC community) can be summarized by the following circuit:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/intro-qec/magic-state-distillation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where the &lt;code class=&quot;MathJax_Preview&quot;&gt;S&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; gate is applied when the measurement of the ancilla qubit leads &lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;. It’s a little exercise the check that the circuit above indeed leads to a &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; gate (and you can find the answer on &lt;a href=&quot;https://quantumcomputing.stackexchange.com/questions/13629/what-are-magic states&quot;&gt;Stack Exchange&lt;/a&gt;). The state &lt;code class=&quot;MathJax_Preview&quot;&gt;T\vert + \rangle&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T\vert + \rangle&lt;/script&gt; is called a &lt;strong&gt;magic state&lt;/strong&gt;, and as we saw, its preparation can involve some errors (it’s not fault-tolerant). The second idea of magic state distillation is therefore distillation. By preparing &lt;code class=&quot;MathJax_Preview&quot;&gt;N&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; copies of a noisy magic state, it’s possible to create an arbitrary clean &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; gate (similarly to how &lt;a href=&quot;https://en.wikipedia.org/wiki/Entanglement_distillation&quot;&gt;entanglement distillation&lt;/a&gt; is done in quantum communication). The full protocol can be shown to be fault-tolerant, but it often requires a very large overhead in both the number of qubits and number of gates&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;. This overhead led to the claim that obtaining a quantum advantage with quadratic speed-ups might be unrealistic in the first fault-tolerant quantum computers&lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;. Reducing this overhead or finding codes that don’t need magic state distillation is therefore a very important research problem.&lt;/p&gt;

&lt;h3 id=&quot;measurement-based-gates-lattice-surgery-and-twists&quot;&gt;Measurement-based gates: lattice surgery and twists&lt;/h3&gt;

&lt;p&gt;So far, we have presented transversal gates as the good guys of fault-tolerant quantum computing, as they can be implemented directly without propagating errors. However, transversal two-qubit gates, while fault-tolerant, are often far from practical. Take two logical qubits of the surface code for instance, and align them on a single 2D lattice.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/intro-qec/lattice-surgery.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A transversal two-qubit gate would consist in &lt;code class=&quot;MathJax_Preview&quot;&gt;L^2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;L^2&lt;/script&gt; physical gates between each pair of qubits &lt;code class=&quot;MathJax_Preview&quot;&gt;a&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;a'&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a'&lt;/script&gt;. However, as seen in the picture above, it would require some long-range interactions, since qubits &lt;code class=&quot;MathJax_Preview&quot;&gt;a&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;a'&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a'&lt;/script&gt; are always &lt;code class=&quot;MathJax_Preview&quot;&gt;L&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; qubits apart. A solution could consist in putting one surface code above the other (in 3D). However, 3D chips are hard to build in practice, and it happens that more convenient solutions exist, namely &lt;strong&gt;lattice surgery&lt;/strong&gt; and &lt;strong&gt;twist deformations&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In both methods, gates are applied by performing measurements and temporarily changing the code itself. In lattice surgery, those measurements allow to merge different surface codes into a single one, and split it again when needed. It can be shown that many gates, such as the CNOT gate, can be designed by performing such a series of merge and split operations. In twist deformations, measurements create holes in the lattice, and braiding operators around those holes allow to create some gates. All those operations are local, making both techniques amenable to real devices. I hope to describe those techniques in more details in future posts.&lt;/p&gt;

&lt;h3 id=&quot;measurement-errors&quot;&gt;Measurement errors&lt;/h3&gt;

&lt;p&gt;Finally, in fault-tolerant quantum computing, the measurement apparatus is also crucial. To detect errors, measurements are made all the time, and those can be very noisy. It means that our stabilizer measurements can lead to the wrong answer. 
To mitigate this effect, the basic technique simply consists in repeating the stabilizer measurements many times. However, some codes such as the 3D surface code have a property that make them amenable to &lt;strong&gt;single-shot quantum error correction&lt;/strong&gt;, as set of techniques that allow to correct errors with a single noisy measurement per stabilizer. More generally, any fault-tolerant scheme must also include a method to perform measurements without propagating errors in a non-fault-tolerant way.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this first post, we set eyes on the big picture of QEC and its different subfields. We discussed a lot of notions and introduced a lot of jargon, that we will review in detail in different posts, so don’t worry if a few things are unclear for the moment. Just retain that quantum error correction comprises two main challenges: designing an encoding process that can protect qubits against noise, and building actual circuits on the code without introducing more errors (the subject of fault-tolerance). In the meantime, feel free to use the following resources if you want to learn more about any of those topics.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Acknowledgment&lt;/strong&gt;: Big thanks to Shashvat Shukla and Michał Stęchły for their detailed feedback on this blog post!&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.11157&quot;&gt;Quantum Error Correction: An Introductory Guide&lt;/a&gt;, by Joschka Roffe&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sites.google.com/site/danbrowneucl/teaching/lectures-on-topological-codes-and-quantum-computation&quot;&gt;Lectures on Topological Codes and Quantum Computation&lt;/a&gt;, by Dan Browne&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ltJ1jXQeDl8&amp;amp;ab_channel=InstituteforQuantumComputing&quot;&gt;Video lectures on Quantum Error Correction and Fault Tolerance&lt;/a&gt;, by Daniel Gottesman&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mmrc.amss.cas.cn/tlb/201702/W020170224608149940643.pdf&quot;&gt;Quantum Computation and Quantum Information — Chapter 7&lt;/a&gt;, Nielsen &amp;amp; Chuang&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://decodoku.medium.com/1-what-is-quantum-error-correction-4ab6d97cb398&quot;&gt;What is Quantum Error Correction&lt;/a&gt;, series of blog posts by James Wootton&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Richard Hamming, &lt;a href=&quot;http://worrydream.com/refs/Hamming-TheArtOfDoingScienceAndEngineering.pdf&quot;&gt;The Art of Doing Science and Engineering&lt;/a&gt;, 1997 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;E. Knill, R. Laflamme, W. Zurek, &lt;a href=&quot;https://arxiv.org/abs/quant-ph/9610011&quot;&gt;Threshold Accuracy for Quantum Computation&lt;/a&gt;, 1996 &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Eric Dennis, Alexei Kitaev, Andrew Landahl, John Preskill, &lt;a href=&quot;https://arxiv.org/abs/quant-ph/0110143&quot;&gt;Topological quantum memory&lt;/a&gt;, 2001 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;David S. Wang, Austin G. Fowler, Lloyd C. L. Hollenberg, &lt;a href=&quot;https://arxiv.org/abs/1009.3686&quot;&gt;Quantum computing with nearest neighbor interactions and error rates over 1%&lt;/a&gt;, 2010 &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;Austin G. Fowler, Matteo Mariantoni, John M. Martinis,  Andrew N. Cleland, &lt;a href=&quot;https://arxiv.org/abs/1208.0928&quot;&gt;Surface codes: Towards practical large-scale quantum computation&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:5:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;Here, a universal gate set is defined as a set that can approximate any unitary up to a precision &lt;code class=&quot;MathJax_Preview&quot;&gt;\epsilon&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; with &lt;code class=&quot;MathJax_Preview&quot;&gt;O\left(\log\left( \frac{1}{\epsilon}\right)^c\right)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;O\left(\log\left( \frac{1}{\epsilon}\right)^c\right)&lt;/script&gt; gates. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;Sergey Bravyi, Jeongwan Haah, &lt;a href=&quot;https://arxiv.org/abs/1209.2426&quot;&gt;Magic-state distillation with low overhead&lt;/a&gt; &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;Ryan Babbush, Jarrod McClean, Michael Newman, Craig Gidney, Sergio Boixo, Hartmut Neven, &lt;a href=&quot;https://arxiv.org/abs/2011.04149&quot;&gt;Focus beyond quadratic speedups for error-corrected quantum advantage&lt;/a&gt; &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Arthur Pesah</name><email>arthur.pesah@gmail.com</email></author><category term="blog" /><category term="quantum-computing" /><summary type="html">This Summer marked the beginning of my thesis work, and with it, of my trip in the fascinating world of quantum error correction. I quickly found in this area the interdisciplinarity that I love: the field takes its roots in theoretical computer science (classical error correction), uses intuitions and techniques from theoretical physics (condensed matter, statistical physics, quantum field theory) and has deep connections to black hole research, statistical inference, algebraic topology and geometry, and many other areas of science and mathematics.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arthurpesah.me/assets/img/blog/intro-qec/thumbnail.png" /><media:content medium="image" url="https://arthurpesah.me/assets/img/blog/intro-qec/thumbnail.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Improve your Scientific Models with Meta-Learning and Likelihood-Free Inference</title><link href="https://arthurpesah.me/blog/2018-12-23-alfi/" rel="alternate" type="text/html" title="Improve your Scientific Models with Meta-Learning and Likelihood-Free Inference" /><published>2018-12-23T00:00:00+01:00</published><updated>2018-12-23T00:00:00+01:00</updated><id>https://arthurpesah.me/blog/alfi</id><content type="html" xml:base="https://arthurpesah.me/blog/2018-12-23-alfi/">&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Note&lt;/strong&gt;: This post was first published as a &lt;a href=&quot;https://towardsdatascience.com/improve-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&quot;&gt;Medium Article&lt;/a&gt; for Towards Data Science&lt;/p&gt;
&lt;p&gt;Introduction to likelihood-free inference and distillation of the paper &lt;a href=&quot;https://arxiv.org/abs/1811.12932&quot;&gt;Recurrent Machines for Likelihood-Free Inference&lt;/a&gt;, published at the &lt;a href=&quot;http://metalearning.ml&quot;&gt;NeurIPS 2018 Workshop on Meta-Learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Article jointly written by Arthur Pesah and Antoine Wehenkel&lt;/p&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;

&lt;p&gt;There are usually two ways of coming up with a new scientific theory:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Starting from first principles, deducing the consequent laws, and coming up with experimental predictions in order to verify the theory&lt;/li&gt;
  &lt;li&gt;Starting from experiments and inferring the simplest laws that explain your data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The role of statistics and machine learning in science is usually related to the second kind of inference, also called &lt;em&gt;induction&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Imagine for instance that you want to model the evolution of two populations (let’s say foxes and rabbits) in an environment. A simple model is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Lotka–Volterra_equations&quot;&gt;Lotka-Volterra differential equation&lt;/a&gt;: you consider the probability that an event such as “a fox eating a rabbit”, “a rabbit being born”, “a fox being born”, etc. happens in a small time interval, deduce a set of differential equations depending on those probabilities, and predict the evolution of the two animals by solving those equations. By comparing your prediction with the evolution of a real population, you can infer the best model parameters (probabilities) in this environment.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*t9Lv2LZ6EJiutaVzKoQ3lQ.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Modern theories require &lt;strong&gt;simulations&lt;/strong&gt; in order to be linked to observations. It can be either a simple differential equation solver as in the case of the Lotka-Volterra model, or a complex Monte-Carlo simulator as they use in particle physics for instance.&lt;/p&gt;

&lt;p&gt;By comparing the results of a simulation, i.e. the predictions of a model, with real data, it is then possible to know the correctness of your model and adjust it accordingly. If this process of going back and forth between the model and the experimental data is usually done manually, the question that any machine learning practitioner would ask is: can we do it automatically? Can we build a machine that takes a tweakable simulator and real data as input, and returns the version of the simulator that fits best some real data?&lt;/p&gt;

&lt;p&gt;That’s the object of our recent work&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, where we trained a neural network to come up with the best sequence of simulator tweaks in order to approximate experimental data, capitalizing on the recent advances in the fields of likelihood-free inference and meta-learning.&lt;/p&gt;

&lt;h1 id=&quot;likelihood-free-inference&quot;&gt;Likelihood-free inference&lt;/h1&gt;

&lt;p&gt;Let’s rephrase our problem in a more formal way. We can model a simulator (also called generative model) by a stochastic function that takes some parameters &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and returns samples &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; drawn from a certain distribution (the so-called &lt;em&gt;model&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;This formalism applies to any scientific theory that includes randomness, as it’s very often the case in modern science (particle collisions are governed by the law of quantum physics which are intrinsically random, biological processes or chemical reactions often occur in a noisy environment, etc.).&lt;/p&gt;

&lt;p&gt;Experimental data consist in a set of points living in the same space as the output of the simulator. The goal of inference is then to find the parameters &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; such that the simulator generate points as close as possible to the real data.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*328OmNFA4xBuj4xgLQdK9w.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Scientific fields using such simulators include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Population genetics. &lt;strong&gt;Model:&lt;/strong&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Coalescent_theory&quot;&gt;Coalescent theory&lt;/a&gt;. &lt;strong&gt;Observation:&lt;/strong&gt; the DNA of a current population. &lt;strong&gt;Parameters:&lt;/strong&gt; the DNA of the common ancestor.&lt;/li&gt;
  &lt;li&gt;High-energy particle physics. &lt;strong&gt;Model&lt;/strong&gt;: &lt;a href=&quot;https://en.wikipedia.org/wiki/Standard_Model&quot;&gt;Standard Model&lt;/a&gt; of particle physics. &lt;strong&gt;Observation:&lt;/strong&gt; output of the detector during a collision. &lt;strong&gt;Parameters:&lt;/strong&gt; coupling constants of the Standard Model (like the mass of the particles or the strength of the different forces).&lt;/li&gt;
  &lt;li&gt;Computational neuroscience. &lt;strong&gt;Model:&lt;/strong&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model&quot;&gt;Hodgkin-Huxley model&lt;/a&gt;. &lt;strong&gt;Observation:&lt;/strong&gt; evolution of the voltage of a neuron after activation. &lt;strong&gt;Parameters&lt;/strong&gt;: biophysical parameters of the neuron.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So how can we predict the parameters of a simulator given some real observations? Let’s consider the simple example of a Gaussian simulator, that takes a vector &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta=(\mu,\sigma)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta=(\mu,\sigma)&lt;/script&gt; as parameters and returns samples from the Gaussian distribution &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{N}(\mu,\sigma)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(\mu,\sigma)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The classical way to infer the parameters of such a simulator is called &lt;em&gt;Maximum Likelihood Estimation (MLE)&lt;/em&gt;. The likelihood is defined as the density of the real data under a parametric model. It means that if most data points are located in high density regions, the likelihood will be high. Hence, the best parameters of a model are often the ones that maximize the likelihood of real data. If you are unfamiliar with likelihood-based inference, you can read &lt;a href=&quot;https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1&quot;&gt;this excellent introduction to the subject&lt;/a&gt;.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*16t2IyuYfARkjea8mfUDTQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you have explicitly access to the underlying probability distribution of your simulator, as well as the gradient of this distribution, you can for instance perform a gradient descent in the parameters space, in order to maximize the likelihood and infer the best parameters of your model.&lt;/p&gt;

&lt;p&gt;However, many real-life simulators have an &lt;strong&gt;intractable likelihood&lt;/strong&gt;, which means that the explicit probability distribution is too hard to compute (either analytically or numerically) . We must therefore find new ways to infer the optimal parameters without using neither the likelihood function nor its gradient.&lt;/p&gt;

&lt;p&gt;To sum it up, we have a black-box stochastic simulator that takes parameters and generates samples from an unknown probability distribution, as well as real data that we are able to compare to the generated ones. Our goal is to find the parameters that lead the simulator to generate data as close as possible to the real ones. This setting is called &lt;strong&gt;likelihood-free inference&lt;/strong&gt;.&lt;/p&gt;

&lt;h1 id=&quot;how-can-we-likelihood-free-infer&quot;&gt;How can we likelihood-free infer?&lt;/h1&gt;

&lt;p&gt;Let’s try to come up progressively with a method to solve our problem. The first thing we can do is to start from a random parameter, and simulate the corresponding data:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*Dgh8KZYJ_aAYYI73jy8wUQ.png&quot; alt=&quot;&quot; /&gt;&lt;em&gt;Representation of the two spaces of interest. The true parameter (that we wish to infer) is the red point on the left. The real data correspond to the red cloud of points on the right. We start by choosing a random parameter θ (in gray on the left) and simulating the corresponding data points (in gray on the right)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;By doing so, we can see how far our generated data are from the real data, but we have no way to know where to move in the parameter-space. Instead, let’s simulate several parameters:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*a6mfrolFz-s9CicMOULgvA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;To do so, we consider a distribution in the parameter-space, called &lt;strong&gt;proposal distribution&lt;/strong&gt; and noted &lt;code class=&quot;MathJax_Preview&quot;&gt;q(\theta|\psi)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q(\theta|\psi)&lt;/script&gt;. If we choose &lt;code class=&quot;MathJax_Preview&quot;&gt;q&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; to be a Gaussian distribution, we will have &lt;code class=&quot;MathJax_Preview&quot;&gt;\psi=(\mu,\sigma)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\psi=(\mu,\sigma)&lt;/script&gt;. The first step consists in initializing &lt;code class=&quot;MathJax_Preview&quot;&gt;\psi&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt; randomly. In the figure above, we considered &lt;code class=&quot;MathJax_Preview&quot;&gt;\psi=\mu&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\psi=\mu&lt;/script&gt; for simplicity. Then, we can perform the following step until convergence:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sampling a few parameters from the proposal distribution: 4 parameters around &lt;code class=&quot;MathJax_Preview&quot;&gt;\psi&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt; in the figure.&lt;/li&gt;
  &lt;li&gt;Generating data from them: the 4 cloud of points on right.&lt;/li&gt;
  &lt;li&gt;Choosing a good direction to move &lt;code class=&quot;MathJax_Preview&quot;&gt;\psi&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The third step is the hard one. Intuitively, you would like to move &lt;code class=&quot;MathJax_Preview&quot;&gt;\psi&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt; towards the orange and green parameters, since the corresponding predictions (orange and green cloud of points) are the closest to the real data. A set of methods, called &lt;em&gt;natural evolution strategies&lt;/em&gt; &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, allow you to link the performance of each &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; (in terms of similarity between its predictions and the real data) to a direction in the parameter space. A recent paper &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; use for instance the similarity measure given by a Generative Adversarial Network (GAN) to find the best direction. Even though those algorithms perform well in the general case, one can wonder if for a given simulator, it is not possible to find a better algorithm that would exploit the particular properties of this simulator. That’s where meta-learning comes into play!&lt;/p&gt;

&lt;h1 id=&quot;meta-learning-for-optimization&quot;&gt;Meta-learning for optimization&lt;/h1&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*Wip2SwVt4aMqBPtE2Spffw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The idea behind meta-learning is to learn how to learn, and in our case to &lt;strong&gt;learn the optimization process&lt;/strong&gt;. The main idea, introduced in the paper &lt;a href=&quot;https://arxiv.org/abs/1606.04474&quot;&gt;Learning to learn gradient descent by gradient descent&lt;/a&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, is to use a recurrent neural network (RNN) to find the best descent direction at each iteration. Below is an example of sequence of points produced by a randomly initialized RNN:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*5Axutz0l_0TjxRupjEt-1Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each descent direction is random and the last point produced is far from the minimum. During training, it should learn to exploit the gradient information at each point in order to move toward the minimum, giving something like:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*r2Ww8UZAmL3cBRds_p35OQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So how to train it? Generate many functions whose you know the minimum, and ask the RNN to minimize the distance between the last point of the sequence and the real minimum.&lt;/p&gt;

&lt;h1 id=&quot;learning-to-learn-scientific-models&quot;&gt;Learning to learn scientific models&lt;/h1&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*qKC9tH01bkQ_giLs_yGJxg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the case of likelihood-free inference, the RNN should return a sequence of proposal parameters &lt;code class=&quot;MathJax_Preview&quot;&gt;\psi&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt;, given the real observations and the generated cloud of points at each step.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*La-daJwmoy6ksOpAeIrFkQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, same question as for learning to learn by gradient descent, how do we train the RNN? Here, the trick is to generate many random parameters θ and to pretend for each one that it is the “true parameter”. We can then simulate each θ generated and obtain a set of “real observations”. The RNN can then be trained by passing it those real observations, looking at its final proposal distribution, and comparing it to the true parameter (that we know because we have generated it).&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*7j8kOuY8p0MlwLXnDVzOEg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s go through an example to clarify all of that. In the figures below, the proposal distribution is represented in color (red = high density). At the beginning, the RNN is initialized randomly and we evaluate it on our first generated true parameter &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; (in red).&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*_PA7u4qOMkAnVEMm73XPiQ.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can then backpropagate the loss into the RNN. After repeating this process for 200 different “true parameters”, this is what is should look like:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*j6eVXNAOHY3rQxD2AqjKWQ.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that it has learnt to exploit the information of the observation space to move towards the good parameter.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;We evaluated our model on different toy simulators, some where we the likelihood is known and some where it is not.&lt;/p&gt;

&lt;h2 id=&quot;non-likelihood-free-problem-poisson-simulator&quot;&gt;Non-likelihood-free problem: Poisson Simulator&lt;/h2&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*204e7qk6Ml3dmQhwbjx7rw.png&quot; alt=&quot;&quot; /&gt;&lt;em&gt;Example of Poisson distributions for various parameters. Source: Wikipedia&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The first simulator takes a parameter λ and draw samples from a Poisson distribution &lt;code class=&quot;MathJax_Preview&quot;&gt;P(\lambda)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\lambda)&lt;/script&gt;. The goal of this example was to see if we obtain comparable performance as the maximum likelihood estimator. Here are the results:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*ALQZ7-7AD6RVjJqQpgXfMA.png&quot; alt=&quot;&quot; /&gt;&lt;em&gt;Comparison of ALFI (Automatic Likelihood-Free Inference, the name of our model), to a maximum likelihood estimator (MLE). Those box-plots represent the distribution of the mean-squared errors between the true parameters and the expected value of the final proposal distributions.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We can see that the performance are comparable, even though we didn’t give our model access to the likelihood.&lt;/p&gt;

&lt;h2 class=&quot;figure&quot; id=&quot;likelihood-free-problem-particle-physics-simulator&quot;&gt;Likelihood-free problem: Particle Physics Simulator&lt;/h2&gt;

&lt;p&gt;To evaluate our model in a true likelihood-free setting, we considered a simplified particle physics model that simulate the collision of an electron and a positron turning into a muon and an antimuon.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*_Zpy-kvc2wiMW-9KzowoLw.png&quot; alt=&quot;&quot; /&gt;&lt;em&gt;Feynman diagram of the simulated process&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The parameters of the simulator are the energy of the incoming particles and the Fermi constant, and the output is the angle between the two muons.&lt;/p&gt;

&lt;p&gt;To evaluate our method, we compared the real observations with the ones generated by the last parameter found. Here are the results:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*gr7tU9hWoGt8FRkPpZKppQ.png&quot; alt=&quot;&quot; /&gt;&lt;em&gt;Results of our method on a simple particle physics model. Comparison of our the real observations (angles of the produced particles) with the ones generated by our predicted parameter.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;We saw what likelihood-free inference is and how meta-learning can be used to solve it by learning the best sequence of simulator tweaks to fit a model to the reality.&lt;/p&gt;

&lt;p&gt;As most meta-learning models, a limitation is that it is hard to train. We had trouble scaling our method to more complex simulators, since meta-training requires a lot of simulator calls, which might be very slow in real-world settings. However, as the field of meta-learning makes progress, we hope that new methods will emerge to alleviate this problem and make it more scalable.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;A. Pesah, A. Wehenkel and G. Louppe, &lt;a href=&quot;https://arxiv.org/abs/1811.12932&quot;&gt;Recurrent Machines for Likelihood-Free Inference&lt;/a&gt; (2018), NeurIPS 2018 Workshop on Meta-Learning &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peter, J. Schmidhuber, &lt;a href=&quot;http://jmlr.org/papers/v15/wierstra14a.html&quot;&gt;Natural Evolution Strategies&lt;/a&gt; (2014), Journal of Machine Learning Research (JMLR). &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;G. Louppe, J. Hermans, K. Cranmer, &lt;a href=&quot;https://arxiv.org/abs/1707.07113&quot;&gt;Adversarial Variational Optimization of Non-Differentiable Simulator&lt;/a&gt; (2017), arXiv e-prints 1707.07113 &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, N. de Freitas, &lt;a href=&quot;https://arxiv.org/abs/1606.04474&quot;&gt;Learning to learn gradient descent by gradient descent&lt;/a&gt; (2016), NIPS 2016 &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Arthur Pesah</name><email>arthur.pesah@gmail.com</email></author><category term="blog" /><category term="machine-learning" /><summary type="html">Note: This post was first published as a Medium Article for Towards Data Science Introduction to likelihood-free inference and distillation of the paper Recurrent Machines for Likelihood-Free Inference, published at the NeurIPS 2018 Workshop on Meta-Learning.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arthurpesah.me/assets/img/blog/alfi/cms_coverl.jpg" /><media:content medium="image" url="https://arthurpesah.me/assets/img/blog/alfi/cms_coverl.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Recent Advances for a Better Understanding of Deep Learning</title><link href="https://arthurpesah.me/blog/2018-08-19-theory-deep-learning/" rel="alternate" type="text/html" title="Recent Advances for a Better Understanding of Deep Learning" /><published>2018-08-19T00:00:00+02:00</published><updated>2018-08-19T00:00:00+02:00</updated><id>https://arthurpesah.me/blog/theory-deep-learning</id><content type="html" xml:base="https://arthurpesah.me/blog/2018-08-19-theory-deep-learning/">&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Note&lt;/strong&gt;: This post was first published as a &lt;a href=&quot;https://towardsdatascience.com/recent-advances-for-a-better-understanding-of-deep-learning-part-i-5ce34d1cc914&quot;&gt;Medium Article&lt;/a&gt; for Towards Data Science*&lt;/p&gt;

&lt;blockquote class=&quot;lead&quot;&gt;
  &lt;p&gt;I would like to live in a world whose systems are built on &lt;strong&gt;rigorous, reliable, verifiable knowledge&lt;/strong&gt;, and not on alchemy. […] Simple experiments and simple theorems are the &lt;strong&gt;building blocks&lt;/strong&gt; that help understand complicated larger phenomena.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This call for a better &lt;strong&gt;understanding&lt;/strong&gt; of deep learning was the core of Ali Rahimi’s &lt;a href=&quot;http://www.argmin.net/2017/12/05/kitchen-sinks/&quot;&gt;Test-of-Time Award presentation&lt;/a&gt; at NIPS in December 2017. By comparing deep learning with alchemy, the goal of Ali was not to dismiss the entire field, but “&lt;a href=&quot;http://www.argmin.net/2017/12/11/alchemy-addendum/&quot;&gt;to open a conversation&lt;/a&gt;”. This goal &lt;a href=&quot;https://syncedreview.com/2017/12/12/lecun-vs-rahimi-has-machine-learning-become-alchemy/&quot;&gt;has definitely been achieved&lt;/a&gt; and people &lt;a href=&quot;https://twitter.com/RandomlyWalking/status/1017899452378550273&quot;&gt;are still debating&lt;/a&gt; whether our current practice of deep learning should be considered as alchemy, engineering or science.&lt;/p&gt;

&lt;p&gt;Seven months later, the machine learning community gathered again, this time in Stockholm for the International Conference on Machine Learning (ICML). With more than 5,000 participants and 629 papers published, it was one of the most important events regarding fundamental machine learning research. And &lt;strong&gt;deep learning theory&lt;/strong&gt; has become one of the biggest subjects of the conference.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/theory-deep-learning/trends.jpg&quot; alt=&quot;trends&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This renew interest was revealed on the first day, with one of the biggest rooms of the conference full of machine learning practitioners ready to listen to the tutorial &lt;a href=&quot;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&quot;&gt;Towards Theoretical Understanding of Deep Learning&lt;/a&gt; by Sanjeev Arora. In his talk, the Professor of computer science at Princeton summarized the current areas of deep learning theory research, by dividing them into four branches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Non Convex Optimization&lt;/strong&gt;: How can we understand the highly non-convex loss function associated with deep neural networks? Why does stochastic gradient descent even converge?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Overparametrization and Generalization&lt;/strong&gt;: In classical statistical theory, generalization depends on the number of parameters but not in deep learning. Why? Can we find another good measure of generalization?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Role of Depth&lt;/strong&gt;: How does depth help a neural network to converge? What is the link between depth and generalization?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Generative Models&lt;/strong&gt;: Why do Generative Adversarial Networks (GANs) work so well? What theoretical properties could we use to stabilize them or avoid mode collapse?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this series of articles, we will try to build intuition in those four areas based on the most recent papers, with a particular focus on ICML 2018.&lt;/p&gt;

&lt;p&gt;This first article will focus on the mysteries of non-convex optimization for deep networks.&lt;/p&gt;

&lt;h1 id=&quot;non-convex-optimization&quot;&gt;Non-Convex Optimization&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/theory-deep-learning/energy-landscape.png&quot; alt=&quot;energy-landscape&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I bet a lot of you have tried training a deep net of your own from scratch and walked away feeling bad about yourself because you couldn’t get it to perform. I don’t think it’s your fault. I think it’s gradient descent’s fault.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;stated Ali Rahimi with a provocative tone in his talk at NIPS. Stochastic Gradient Descent (SGD) is indeed the cornerstone of deep learning. It is supposed to find a solution of a highly non-convex optimization problem, and understanding when it works or not, and why, is one the most fundamental questions we would have to adress in a general theory of deep learning. More specifically, the study of non-convex optimization for deep neural networks can be divided into two questions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What does the loss function look like?&lt;/li&gt;
  &lt;li&gt;Why does SGD converge?&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;what-does-the-loss-function-look-like&quot;&gt;What does the loss function look like?&lt;/h1&gt;

&lt;p&gt;If I ask you to visualize a global minimum, it’s very likely that the first representation that will come to your mind will look something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/theory-deep-learning/minimum.png&quot; alt=&quot;minimum&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And it’s normal. In a 2D-world, it’s not rare to find problems, where around a global minimum, your function will be &lt;strong&gt;strictly&lt;/strong&gt; convex (which means that the two eigenvalues of the hessian matrix at this point will be both strictly positive). But in a world with billions of parameters, as it is the case in deep learning, what are the odds that none of the directions around a global minimum are flat? Or equivalently that the hessian contains not a single zero (or almost zero) eigenvalue?&lt;/p&gt;

&lt;p&gt;One of the first comment of Sanjeev Arora in his tutorial was that the number of possible directions that you can take on a loss function grows exponentially with the dimension.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/theory-deep-learning/curse-dimensionality.png&quot; alt=&quot;curse-dimensionality&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, intuitively, it seems likely that a global minimum will not be a point, but a &lt;strong&gt;connected manifold&lt;/strong&gt;. Which means that if you’ve reached a global minimum, you should be able to walk around on a flat path where all the points are also minima. This has been experimentally proven on large networks by a team at Heidelberg University, in their paper &lt;a href=&quot;https://icml.cc/Conferences/2018/Schedule?showEvent=2780&quot;&gt;Essentially No Barriers in Neural Network Energy Landscape&lt;/a&gt;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. They argue an even more general statement, namely that any two global minima can be connected through a flat path.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/theory-deep-learning/no-barrier.png&quot; alt=&quot;no-barrier&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was already known to be the case for a CNN on MNIST or an RNN on PTB&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, but this work extended that knowledge to much bigger networks (some DenseNets and ResNets) trained on more advanced datasets (CIFAR10 and CIFAR100). To find this path, they used a heuristic coming from molecular statistical mechanics, called AutoNEB. The idea is to create an initial path (for instance linear) between your two minima, and to place pivots on that path. You then iteratively modify the positions of the pivots, such that it minimizes the loss of each pivot and make sure the distances between pivots stay about the same (by modelling the space between pivots by springs).&lt;/p&gt;

&lt;p&gt;If they didn’t prove that result theoretically, they gave some intuitive explanations on why such path exists:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If we perturb a single parameter, say by adding a small constant, but leave the others free to adapt to this change to still minimise the loss, it may be argued that by adjusting somewhat, the myriad other parameters can “make up” for the change imposed on only one of them&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thus, the results of this paper can help us seeing minima in a different way, through the lens of overparametrization and high-dimensional spaces.&lt;/p&gt;

&lt;p&gt;More generally, when thinking about the loss function of neural network, you should always have in mind that the number of possible directions at a given point is huge. Another consequence of that is the fact that saddle points must be much more abundant than local minima: at a given (critical) point, among the billions of possible directions, it’s very likely to find one that goes down (if you’re not in a global minimum). This intuition was formalized rigorously and proved empirically in a paper published at NIPS 2014: &lt;a href=&quot;https://arxiv.org/abs/1406.2572&quot;&gt;Identifying and attacking the saddle point problem in high-dimensional non-convex optimization&lt;/a&gt;&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h1 id=&quot;why-does-sgd-converge-or-not&quot;&gt;Why does SGD converge (or not)?&lt;/h1&gt;

&lt;p&gt;The second important question in optimization of deep neural networks is related to the convergence properties of SGD. While this algorithm has long been seen as a faster but approximate version of gradient descent, we now have evidence that SGD actually converges to better, more general, minima&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. But can we formalize it and explain quantitatively the capacity of SGD to escape from local minima or saddle points?&lt;/p&gt;

&lt;h2 id=&quot;sgd-modifies-the-loss-function&quot;&gt;SGD modifies the loss function&lt;/h2&gt;

&lt;p&gt;The paper &lt;a href=&quot;https://arxiv.org/abs/1802.06175&quot;&gt;An Alternative View: When Does SGD Escape Local Minima?&lt;/a&gt;&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; showed that performing SGD is equivalent to doing regular gradient descent on a convolved (thus smoothed) loss function. With that point of view and under certain assumptions (shown by the authors to be often true in practice), they prove that SGD will manage to escape local minima and converge to a small region around a global minimum.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/theory-deep-learning/sgd-convolution.png&quot; alt=&quot;sgd-convolution&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sgd-is-governed-by-stochastic-differential-equations&quot;&gt;SGD is governed by stochastic differential equations&lt;/h2&gt;

&lt;p&gt;Another approach to SGD that has really changed my vision of this algorithm is continuous SGD. The idea was presented by Yoshua Bengio during his talk &lt;a href=&quot;http://www.iro.umontreal.ca/~bengioy/talks/ICMLW-nonconvex-14july2018.pptx.pdf&quot;&gt;On stochastic gradient descent, flatness and generalization&lt;/a&gt;, given at the ICML Workshop on Non-Convex Optimization. SGD does not move a point on a loss function, but a &lt;strong&gt;cloud of points&lt;/strong&gt;, or in other words, &lt;strong&gt;a distribution&lt;/strong&gt;.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/theory-deep-learning/bengio-01.png&quot; alt=&quot;bengio-01&quot; /&gt;
Slide extracted from the presentation On stochastic gradient descent, flatness and generalization, 
by Y. Bengio, at ICML 2018. He presented an alternative way to see SGD, 
where you replace points by distributions (clouds of points)&lt;/p&gt;

&lt;p&gt;The size of this cloud of point (i.e. the variance of the associated distribution) is proportional to the factor &lt;em&gt;learning_rate / batch_size&lt;/em&gt;. A proof of this is given in the amazing paper by Pratik Chaudhari and Stefano Soatto, &lt;a href=&quot;https://arxiv.org/pdf/1710.11029.pdf&quot;&gt;Stochastic gradient descent performs variational inference&lt;/a&gt;, converges to limit cycles for deep networks&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;, that they presented during the Workshop on Geometry in Machine Learning. This formula is quite intuitive: a low batch size means a very noisy gradient (because computed on a very small subset of the dataset), and a high learning rate means noisy steps.&lt;/p&gt;

&lt;p&gt;The consequence of seeing SGD as a distribution moving over time is that the equations governing the descent are now &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_partial_differential_equation&quot;&gt;stochastic partial differential equations&lt;/a&gt;. More precisely, under certain assumptions, [5] showed that the governing equation is actually a &lt;a href=&quot;https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation&quot;&gt;Fokker-Planck equation&lt;/a&gt;.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/theory-deep-learning/continuous-sgd.jpeg&quot; alt=&quot;continuous-sgd&quot; /&gt;
Slide extracted from the presentation High-dimensional Geometry and Dynamics of 
Stochastic Gradient Descent for Deep Networks, by P. Chaudhari and S. Soatto, at ICML 2018. 
They showed how to pass from a discrete system to a continuous one described
by the Fokker-Plank equation&lt;/p&gt;

&lt;p&gt;In statistical physics, this type of equations describes the evolution of particles exposed to a drag force (that drifts the distribution, i.e. moves its mean) and to random forces (that diffuse the distribution, i.e. increase its variance). In SGD, the drag force is modeled by the true gradient while the random forces correspond to noise inherent to the algorithm. As you can see in the slide above, the diffusion term is proportional to a temperature term T=1/β=learning_rate/(2*batch_size), which shows once again the importance of this ratio!&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/theory-deep-learning/FokkerPlanck.gif&quot; alt=&quot;FokkerPlanck&quot; /&gt;
Evolution of a distribution under the Fokker-Planck equation. 
It drifts on the left and diffuses with time. 
Source: Wikipedia&lt;/p&gt;

&lt;p&gt;Using this framework, Chaudhari and Soatto proved that our distribution will monotonically converge to a certain steady distribution (in the sense of the KL-divergence):&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/theory-deep-learning/theorem-5.png&quot; alt=&quot;theorem-5&quot; /&gt;
One of the main theorems of [5], proving monotonic convergence of the distribution to a steady state 
(in the sense of the KL divergence). The second equation shows that minimizing F is equivalent to 
minimizing a certain potential ϕ as well as maximizing the entropy of the distribution 
(trade-off controlled by the temperature 1/β)*&lt;/p&gt;

&lt;p&gt;There are several interesting points to comment in the theorem above:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The functional that is minimized by SGD can be rewritten as a sum of two terms (Eq. 11): the expectancy of a potential Φ, and the entropy of the distribution. The temperature 1/β controls the trade-off between those two terms.&lt;/li&gt;
  &lt;li&gt;The potential Φ depends only on the data and the architecture of the network (and not the optimization process). If it is equal to the loss function, SGD will converge to a global minimum. However, the paper shows that it’s rarely the case, and knowing how far Φ is from the loss function will tell you how likely your SGD will converge.&lt;/li&gt;
  &lt;li&gt;The entropy of the final distribution depends on the ratio &lt;em&gt;learning_rate/batch_size&lt;/em&gt; (the temperature). Intuitively, the entropy is related to the size of a distribution and having a high temperature often comes down to having a distribution with high variance, which usually means a flat minimum. Since flat minima are often considered to generalize better, it’s consistent with the empirical finding that high learning and low batch size often lead to better minima.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, seeing SGD as a distribution moving over time showed us that &lt;em&gt;learning_rate/batch_size&lt;/em&gt; is more meaningful than each hyperparameter separated regarding convergence and generalization. Moreover, it enabled the introduction of the potentiel of a network, related to convergence and that could give a good metric for architecture search.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The quest of finding a deep learning theory can be broken down into two parts: first, building intuitions on how and why it works, through toy models and experiments, then expressing those intuitions into a mathematical form that can help us explaining our current results and making new ones.&lt;/p&gt;

&lt;p&gt;In this first article, we tried to convey more intuition of both the high-dimensional loss function of neural networks and the interpretations of SGD, while showing that new kinds of formalism are being built in the objective of having a real mathematical theory of deep neural networks optimization.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Felix Draxler, Kambis Veschgini, Manfred Salmhofer, Fred Hamprecht. &lt;em&gt;Essentially No Barriers in Neural Network Energy Landscape&lt;/em&gt;, ICML 2018. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;C. Daniel Freeman, Joan Bruna. &lt;em&gt;Topology and Geometry of Half-Rectified Network Optimization&lt;/em&gt;, arXiv:1611.01540, 2016. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, Yoshua Bengio. &lt;em&gt;Identifying and attacking the saddle point problem in high-dimensional non-convex optimization&lt;/em&gt;, NIPS 2014 &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang. &lt;em&gt;On large-batch training for deep learning: Generalization gap and sharp minima&lt;/em&gt;, ICLR 2017. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Robert Kleinberg, Yuanzhi Li, Yang Yuan. &lt;em&gt;An Alternative View: When Does SGD Escape Local Minima?&lt;/em&gt;, ICML 2018 &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;Pratik Chaudhari, Stefano Soatto. &lt;em&gt;Stochastic gradient descent performs variational inference, converges to limit cycles for deep network&lt;/em&gt;, ICLR 2018 &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Arthur Pesah</name><email>arthur.pesah@gmail.com</email></author><category term="blog" /><category term="machine-learning" /><summary type="html">Note: This post was first published as a Medium Article for Towards Data Science*</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arthurpesah.me/assets/img/blog/theory-deep-learning/blackboard.jpg" /><media:content medium="image" url="https://arthurpesah.me/assets/img/blog/theory-deep-learning/blackboard.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Little Review of Domain Adaptation in 2017</title><link href="https://arthurpesah.me/blog/2018-01-03-domain-adaptation-2017/" rel="alternate" type="text/html" title="A Little Review of Domain Adaptation in 2017" /><published>2018-01-03T00:00:00+01:00</published><updated>2018-01-03T00:00:00+01:00</updated><id>https://arthurpesah.me/blog/domain-adaptation-2017</id><content type="html" xml:base="https://arthurpesah.me/blog/2018-01-03-domain-adaptation-2017/">&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;Note&lt;/strong&gt;: This post was first published as a Quora answer to the question &lt;a href=&quot;https://www.quora.com/What-are-the-most-significant-machine-learning-advances-in-2017/answer/Arthur-Pesah&quot;&gt;What are the most significant machine learning advances in 2017?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2017 has been an amazing year for domain adaptation: awesome image-to-image and language-to-language translations have been produced, adversarial methods for DA have made huge progress and very innovative algorithms have been proposed to tackle the giant problem of adapting two domains.&lt;/p&gt;

&lt;p&gt;By domain adaptation, I mean any algorithm trying to transfer two domains, usually called source and target (for instance paintings and real photos), into a common domain. To do so, one can chose either to translate one domain into the other (e.g. translate paintings to photos) or to find a common embedding between the two domains. When only the source domain has labels and the goal is to predict the labels of the target domain, it’s called unsupervised domain adaptation and that’s where the advances were the most incredible. There are many benchmarks to evaluate a DA algorithm, one of the most common being to predict the labels of SVHN (a dataset of digits built with house numbers) by using MNIST (the most common handwritten digits dataset) and its labels. In a year, the results have passed from 90% (with Domain Transfer Network (DTN)&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, which was already a great improvement on previous methods that turned around 82%, like DRCN&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;) to 99.2% (with self-ensembling DA&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;). Besides this quantitative analysis, the translations performed by some algorithms released this year are qualitatively amazing, particularly in visual DA and NLP.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/domain-adaptation/svhn2mnist-SBDA-GAN.png&quot; alt=&quot;&quot; /&gt;
Figure 1. Transfer of SVHN to MNIST by SBADA-GAN&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, May 2017. For testing a DA algorithm, one can try to predict the labels of SVHN by only using the labels of MNIST and the unsupervised translation between SVHN and MNIST.*&lt;/p&gt;

&lt;p&gt;Let’s try to summarize how awesome this year has been for domain adaptation.&lt;/p&gt;

&lt;h1 id=&quot;adversarial-domain-adaptation&quot;&gt;Adversarial Domain Adaptation&lt;/h1&gt;

&lt;p&gt;If 2015 saw the birth of adversarial domain adaptation (with DANN&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;) and 2016 the birth of GAN-based domain adaptation (with CoGAN&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; and DTN&lt;sup id=&quot;fnref:2:1&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; ), 2017 has seen huge improvements and amazing results with these methods. The idea behind adversarial DA is to train two neural networks: a discriminator that tries to separate the target domain from the transformed source domain, and a generator that tries to fool the discriminator to make the source domain look like the target one as much as possible. It’s basically a GAN but taking the source distribution as input instead of a uniform distribution (it is usually called a conditional GAN). I’ve realized a little animation to explain the concept more visually (you can find the code &lt;a href=&quot;https://github.com/artix41/transfer-learning-algorithms/tree/master/adda&quot;&gt;here&lt;/a&gt;):&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/domain-adaptation/gan-working.gif&quot; alt=&quot;&quot; /&gt;
Figure 2. GAN-based adversarial domain adaptation for two Gaussian domains. 
The discriminator (background) tries to separate the green distribution from the orange 
distribution, and the generator modifies the green distribution to fool the discriminator. 
You can find the code &lt;a href=&quot;https://github.com/artix41/transfer-learning-algorithms/tree/master/adda&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So, what were the “significant advances” in 2017?&lt;/p&gt;

&lt;h2 id=&quot;adda&quot;&gt;ADDA&lt;/h2&gt;

&lt;p&gt;First, in February, ADDA&lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; released a generalized theoretical framework for adversarial domain adaptation and achieved a 76.0% score with a simple GAN loss on SVHN → MNIST (which they thought to be the best score for an adversarial network on this task, but they had probably not heard of DTN at the time they submitted their article).&lt;/p&gt;

&lt;h2 id=&quot;cyclegan&quot;&gt;CycleGAN&lt;/h2&gt;

&lt;p&gt;A month later, the most important contribution of adversarial DA occurred: the invention of the cycle-consistency loss by &lt;strong&gt;CycleGAN&lt;/strong&gt;&lt;sup id=&quot;fnref:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;. This paper was a real revolution. Their idea was to train two conditional GANs, one transferring source to target, and the other target to source, and to consider a new loss, called cycle-consistency, which ensures that if you connect the two networks together it will produce an identity mapping (source → target → source). Their examples of transferring horses to zebra or painting to photos have become really famous and I consider it to be one of the coolest thing of this year! Contrary to other methods like pix2pix&lt;sup id=&quot;fnref:10&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;, they didn’t train their algorithm on pairs of images (like a photo of cat and the sketch of this same cat, for pix2pix), but only on the two distributions separated, which makes their results even more impressive.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/domain-adaptation/cycleGAN.png&quot; alt=&quot;&quot; /&gt;
Figure 3. Examples of image-to-image translations with CycleGAN&lt;/p&gt;

&lt;h2 id=&quot;discogan&quot;&gt;DiscoGAN&lt;/h2&gt;

&lt;p&gt;What’s fun is that a bunch of other papers discovered the cycle-consistency loss simultaneously, between March and May, sometimes giving it another name (like reconstruction loss). It’s for instance the case of &lt;strong&gt;DiscoGAN&lt;/strong&gt;&lt;sup id=&quot;fnref:11&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;, whose loss was a bit different (cross-entropy for the GAN loss instead of MSE for instance) but they also achieved incredible results, by managing to transfer both texture properties (like transforming blonde-haired to brown-haired people, women to men or people with glasses to people without glasses) and geometrical properties (chairs to cars and faces to cars).&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/domain-adaptation/discoGAN-01.png&quot; alt=&quot;discoGAN-01&quot; /&gt;
&lt;img src=&quot;/assets/img/blog/domain-adaptation/discoGAN-02.png&quot; alt=&quot;discoGAN-02&quot; /&gt;
Figure 4. Examples of image-to-image translations with DiscoGAN&lt;/p&gt;

&lt;h2 id=&quot;dualgan&quot;&gt;DualGAN&lt;/h2&gt;

&lt;p&gt;It’s also the case of &lt;strong&gt;DualGAN&lt;/strong&gt;&lt;sup id=&quot;fnref:12&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;, who used the cycle loss with a WGAN and other recent tricks on how to train GANs. They applied it on day ←→ night or sketch ←→ photos translations, and here are the results:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/domain-adaptation/dualgan.png&quot; alt=&quot;dualgan&quot; /&gt;
Figure 5. Examples of image-to-image translations with DualGAN&lt;/p&gt;

&lt;h2 id=&quot;sbada-gan&quot;&gt;SBADA-GAN&lt;/h2&gt;

&lt;p&gt;But those 3 papers didn’t consider any dataset with a task (like classification), so didn’t give any quantitative evaluation of their method. &lt;strong&gt;SBADA-GAN&lt;/strong&gt;&lt;sup id=&quot;fnref:4:1&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; did it by adding a classifier at the end of their network in order to predict the labels of both the source and the transformed target sample. During the training, pseudo-labels are assigned to the target samples and contribute to the classification loss. The score obtained for SVHN → MNIST is not very good (~76%, same as ADDA), but they achieved new state-of-the-arts on the opposite transformation (MNIST→SVHN) and on MNIST ←→ USPS (another handwritten-digits dataset very close to MNIST).&lt;/p&gt;

&lt;h2 id=&quot;gentoadapt&quot;&gt;GenToAdapt&lt;/h2&gt;

&lt;p&gt;Other kind of adversarial architectures have been tried this year with more success on digits benchmarks, like &lt;strong&gt;GenToAdapt&lt;/strong&gt;&lt;sup id=&quot;fnref:14&quot;&gt;&lt;a href=&quot;#fn:14&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; in April who made the first real state-of-the-art of the year in SVHN → MNIST, with a score of 92.4%. Their technique was basically to use a GAN to generate source images from both source and target samples, and to discriminate both real vs fake samples and the different classes of the source samples (like AC-GAN). The learned embedding is then used to train a third network, C, to directly predict the labels of the input samples. The figure below (from the original paper) is certainly clearer than my explanation:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/domain-adaptation/gentoadapt.png&quot; alt=&quot;gentoadapt&quot; /&gt;
Figure 6. The architecture of GenToAdapt&lt;/p&gt;

&lt;h2 id=&quot;unit&quot;&gt;UNIT&lt;/h2&gt;

&lt;p&gt;It’s also the case of &lt;strong&gt;UNIT&lt;/strong&gt;&lt;sup id=&quot;fnref:15&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;, an adversarial method proposed by Nvidia. Like in many Nvidia papers, they performed a large bunch of amazing experiments (image-to-image translation between different outside conditions on the road, between GTA and reality, between different breeds of dogs, etc.). They have also tested their algorithm on SVHN → MNIST, and obtained 90.53%, which is very close to DTN score, but they manage to transfer much higher-resolution images. Their technique is based on CoGAN&lt;sup id=&quot;fnref:6:1&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;, which consists in two GANs, one for generating the source domain and one for the target domain, with weight-sharing for some layers. Nvidia’s main contribution was to replace the generator by a VAE. They indeed show that the VAE loss is equivalent to the cycle-consistency constraint described in the previous papers.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/domain-adaptation/UNIT.png&quot; alt=&quot;UNIT&quot; /&gt;
Figure 7. Examples of image-to-image translations with UNIT&lt;/p&gt;

&lt;h2 id=&quot;stargan&quot;&gt;StarGAN&lt;/h2&gt;

&lt;p&gt;However, those architectures are only capable of transferring one source domain to one target domain at a time. But if you have multiple domains, there should be a way to train a network to perform transfers in all the domains. In November &lt;strong&gt;StarGAN&lt;/strong&gt;&lt;sup id=&quot;fnref:17&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; adapted CycleGAN to this so-called multi-source domain adaptation problem. Their results in transferring different hair colors or emotions for the same person were pretty amazing as you can see:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/domain-adaptation/StarGAN.png&quot; alt=&quot;StarGAN&quot; /&gt;
Figure 8. Example of multi-domain image translations with StarGAN&lt;/p&gt;

&lt;h2 id=&quot;word-translation-without-parallel-data&quot;&gt;Word Translation Without Parallel Data&lt;/h2&gt;

&lt;p&gt;It might seem from the examples above that the DA community is putting all its efforts into computer vision (CV). But one of the most impressive (and shared) DA paper of the year is in natural language processing (NLP) : &lt;strong&gt;Word Translation Without Parallel Data&lt;/strong&gt;&lt;sup id=&quot;fnref:18&quot;&gt;&lt;a href=&quot;#fn:18&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;. They basically used adversarial DA to find a common embedding between samples from two languages (source and target), and managed to perform very accurate translations without having trained on any example of translation! If you read the paper, you can notice that the expression “domain adaptation” haven’t been used once… Since most DA folks are into computer vision, it seems that the NLP guys who wrote this paper were not aware that their work entered into domain adaptation. So I think NLP would benefit a great deal by testing on their data all the brand new DA methods that the CV community has invented this year.&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/domain-adaptation/translation-without-pairs.png&quot; alt=&quot;translation-without-pairs&quot; /&gt;
Figure 9. Alignement of the embedding word spaces of the source (english) and the target (italian) domains.&lt;/p&gt;

&lt;h2 id=&quot;pix2pix-hd&quot;&gt;Pix2Pix HD&lt;/h2&gt;

&lt;p&gt;Finally, I have talked only about unpaired domain adaptation (where you don’t use any pair of corresponding source/target samples during the training), but paired DA has also known a little revolution with &lt;strong&gt;pix2pixHD&lt;/strong&gt;&lt;sup id=&quot;fnref:19&quot;&gt;&lt;a href=&quot;#fn:19&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;. It’s basically an improved version of pix2pix (a conditional GAN trained on pairs of images) with many tricks to make it scalable to bigger images. They trained their network to transform semantic maps into realistic photos of street scenes, as you can see on the animation below:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/domain-adaptation/pix2pixHD.gif&quot; alt=&quot;pix2pixHD&quot; /&gt;
Figure 10. Translation of a semantic map (map of labels) to a real street scene with pix2pixHD&lt;/p&gt;

&lt;h1 id=&quot;embedding-methods&quot;&gt;Embedding methods&lt;/h1&gt;

&lt;p&gt;Apart from adversarial DA, many other methods have been tried this year, some of them being very successful. That’s the case of two recent methods which try to find a common embedding between the source and target domains, leading at the end to a single neural network capable of classifying both source and target samples.&lt;/p&gt;

&lt;h2 id=&quot;associative-da&quot;&gt;Associative DA&lt;/h2&gt;

&lt;p&gt;The first one is &lt;strong&gt;Associative DA&lt;/strong&gt; &lt;code class=&quot;MathJax_Preview&quot;&gt;DA_{assoc}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;DA_{assoc}&lt;/script&gt;&lt;sup id=&quot;fnref:20&quot;&gt;&lt;a href=&quot;#fn:20&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; who achieved a score of &lt;strong&gt;97.6%&lt;/strong&gt; on SVHN→MNIST. In order to find the best embedding, they used the new trend of 2017… cycle-consistency loss! Yes, again, but this time without any GAN or other adversarial network: they just try to learn an embedding (last layer of a neural network) such that the probability of translating a source sample to a target sample (based on the distance between the two points in the embedding space), then converting back this target sample to another source sample will be high if the two source samples belong to the same class.&lt;/p&gt;

&lt;h2 id=&quot;self-ensembling-da&quot;&gt;Self-Ensembling DA&lt;/h2&gt;

&lt;p&gt;The second one is &lt;strong&gt;Self-Ensembling DA&lt;/strong&gt;&lt;sup id=&quot;fnref:3:1&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, who really destroyed our benchmark SVHN→MNIST with a score of &lt;strong&gt;99.2%&lt;/strong&gt; ! We’ll have to find other benchmarks next year! They did this exploit by adapting Mean Teacher − a method coming from semi-supervised learning that has achieved recent SOTA in this field − to domain adaptation. The idea is to have two networks, a student and a teacher, and to make the weights of the teacher a moving average of all the weights that the student got during training. Then, labeled source samples are used to train the student to be a good classifier, and unlabeled target samples to train the student to be like the teacher (with a consistency loss). You can find a more visual explanation &lt;a href=&quot;https://thecuriousaicompany.com/mean-teacher/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;optimal-transport&quot;&gt;Optimal Transport&lt;/h1&gt;

&lt;p&gt;Another kind of method has been developed this year: domain adaptation based on optimal transport. Optimal transport is a huge area of applied mathematics, consisting in finding the best transport plan from one distribution to another, by minimizing the total cost of transporting a source mass to a target point. For instance, if you consider two sets of points (with the same number of points each), source and target, and take as the cost function simply the euclidean distance, optimal transport asks you to associate every source point to a target points, so that the total distance is minimized. Here is the solution for two Gaussian domains:&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/domain-adaptation/simple-ot.png&quot; alt=&quot;simple-ot&quot; /&gt;
Figure 11. Best transport plan between two Gaussian domains. 
Each source point is transported to a target point, and the total distance is minimized. 
This graph has been produced with the library &lt;a href=&quot;https://github.com/rflamary/POT&quot;&gt;POT&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This &lt;a href=&quot;https://vincentherrmann.github.io/blog/wasserstein/&quot;&gt;blog article&lt;/a&gt; is an excellent introduction if you want to learn more about OT.&lt;/p&gt;

&lt;p&gt;If you start to understand a bit domain adaptation, I think you can now clearly see the link between OT and DA. The relation between those two fields had been theorized in 2016&lt;sup id=&quot;fnref:22&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt;, but a very interesting algorithm has come out in 2017: &lt;strong&gt;Joint Distribution Optimal Transportation (JDOT)&lt;/strong&gt;&lt;sup id=&quot;fnref:23&quot;&gt;&lt;a href=&quot;#fn:23&quot; class=&quot;footnote&quot;&gt;19&lt;/a&gt;&lt;/sup&gt;. Their method is an iterative process: at each iteration, pseudo-labels are given to every target points (at first using a classifier trained on the source samples). Then, the goal is to transport every source point to a target point, minimizing not only the total distance traveled, but also the number of change of label during the transport (between the label of the source point and the pseudo-label of the target point). I made a visual explanation here : &lt;a href=&quot;https://github.com/artix41/transfer-learning-algorithms/blob/master/jdot/README.md&quot;&gt;A Visual Explanation of JDOT Algorithm&lt;/a&gt;, summarized in this GIF (not sure if understandable without pausing at each step):&lt;/p&gt;

&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;/assets/img/blog/domain-adaptation/animation.gif&quot; alt=&quot;animation&quot; /&gt;
Figure 12. Animation showing the different steps of the JDOT algorithm. 
You can find all those images separated and associated to some explanations 
&lt;a href=&quot;https://github.com/artix41/transfer-learning-algorithms/blob/master/jdot/README.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;To sum it up, not only has 2017 destroyed some domain adaptation benchmarks, it has also produced the first high-quality translations from one domain to another (as you can see in all those pictures above). But we can still do much better on many more complicated benchmarks and adapt DA to other areas of machine learning (like reinforcement learning and NLP), so 2018 has all its chances to be as awesome as 2017, and I look forward to see what it gives!&lt;/p&gt;

&lt;p&gt;If you want to learn more about domain adaptation, I’m maintaining an updated list of great resources (papers, datasets, results, etc.) about DA and transfer learning on &lt;a href=&quot;https://github.com/artix41/awesome-transfer-learning&quot;&gt;this GitHub repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: the description of those papers only corresponds to my current understanding of them, so take it with a grain of salt and don’t hesitate to tell me if I am incorrect or imprecise in some of my explanations. Concerning the results I give, they are only the ones given in the original papers and a more rigorous methodology should be used in order to make a real comparison.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1611.02200.pdf&quot;&gt;Unsupervised Cross-domain Image Generation&lt;/a&gt; (2016) &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1607.03516.pdf&quot;&gt;Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation&lt;/a&gt; (2016) &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:2:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.05208.pdf&quot;&gt;Self-ensembling for domain adaptation&lt;/a&gt; (2017) &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:3:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1705.08824.pdf&quot;&gt;From source to target and back: symmetric bi-directional adaptive GAN&lt;/a&gt; (2017) &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:4:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1505.07818.pdf&quot;&gt;Domain-Adversarial Training of Neural Networks&lt;/a&gt; (2015) &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1606.07536.pdf&quot;&gt;Coupled Generative Adversarial Networks&lt;/a&gt; (2016) &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:6:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1702.05464.pdf&quot;&gt;Adaptative Discriminative Domain Adaptation&lt;/a&gt; (2017) &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.10593&quot;&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&lt;/a&gt; (2017) &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1611.07004.pdf&quot;&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;/a&gt; (2016) &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.05192.pdf&quot;&gt;Learning to Discover Cross-Domain Relations with Generative Adversarial Networks&lt;/a&gt; (2017) &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.02510.pdf&quot;&gt;DualGAN: Unsupervised Dual Learning for Image-to-Image Translation&lt;/a&gt; (2017) &lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:14&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.01705.pdf&quot;&gt;Generate To Adapt: Aligning Domains using Generative Adversarial Networks&lt;/a&gt; (2017) &lt;a href=&quot;#fnref:14&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:15&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.00848.pdf&quot;&gt;Unsupervised Image-to-Image Translation Networks&lt;/a&gt; (2017) &lt;a href=&quot;#fnref:15&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:17&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1711.09020.pdf&quot;&gt;StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation&lt;/a&gt; (2017) &lt;a href=&quot;#fnref:17&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:18&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;&quot;&gt;Word Translation without Parallel Data&lt;/a&gt; (2017) &lt;a href=&quot;#fnref:18&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:19&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1711.11585.pdf&quot;&gt;High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs&lt;/a&gt; &lt;a href=&quot;#fnref:19&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:20&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.00938.pdf&quot;&gt;Associative Domain Adaptation&lt;/a&gt; (2017) &lt;a href=&quot;#fnref:20&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:22&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1610.04420.pdf&quot;&gt;Theoretical Analysis of Domain Adaptation with Optimal Transport&lt;/a&gt; (2016) &lt;a href=&quot;#fnref:22&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:23&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1705.08848.pdf&quot;&gt;Joint distribution optimal transportation for domain adaptation&lt;/a&gt; (2017) &lt;a href=&quot;#fnref:23&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Arthur Pesah</name><email>arthur.pesah@gmail.com</email></author><category term="blog" /><category term="machine-learning" /><summary type="html">Note: This post was first published as a Quora answer to the question What are the most significant machine learning advances in 2017?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arthurpesah.me/assets/img/blog/domain-adaptation/teaser_high_res.jpg" /><media:content medium="image" url="https://arthurpesah.me/assets/img/blog/domain-adaptation/teaser_high_res.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>